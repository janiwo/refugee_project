{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate merging and related preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages for the following parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "#python libraries\n",
    "import stanza\n",
    "from stanza_batch import batch\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# self written modules\n",
    "import preprocessing\n",
    "import candidate_processing as cand_prep\n",
    "import candidate_extraction as cand_ex\n",
    "\n",
    "import candidate_extraction as cand_ex\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "seg = Segmenter() \n",
    "\n",
    "from ekphrasis.classes.tokenizer import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. We import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 92806 tweets!\n"
     ]
    }
   ],
   "source": [
    "#data_url = r\"CBS - Copenhagen Business School\\Kick-Ass Master Thesis - General\\Data\\moria-data/moria_no_duplicates.csv\"\n",
    "beirut_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_beirut.csv\" # for Beirut\n",
    "tigray_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_tigray.csv\" # for Tigray\n",
    "channel_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_channel.csv\" # for Channel\n",
    "moria_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_moria.csv\" # for Moria\n",
    "all_url = r\"Dropbox (CBS)/Master thesis data/df_tweets.csv\" # for all\n",
    "\n",
    "\n",
    "def read_event_df(data_url):\n",
    "    directory_path = os.getcwd() + \"/../../../\" + data_url \n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df(moria_url)\n",
    "#channel_df = read_event_df(channel_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. We instantiate stanza english language module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-06 09:35:57 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-04-06 09:35:57 INFO: Use device: cpu\n",
      "2021-04-06 09:35:57 INFO: Loading: tokenize\n",
      "2021-04-06 09:35:57 INFO: Loading: pos\n",
      "2021-04-06 09:35:57 INFO: Loading: lemma\n",
      "2021-04-06 09:35:57 INFO: Loading: depparse\n",
      "2021-04-06 09:35:58 INFO: Loading: sentiment\n",
      "2021-04-06 09:35:59 INFO: Loading: ner\n",
      "2021-04-06 09:36:00 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ needed when running first time ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "\n",
    "#stanza.download(\"en\")\n",
    "\n",
    "#stanza.install_corenlp()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# loading the pipeline\n",
    "en_nlp = stanza.Pipeline(\"en\", tokenize_pretokenized=True, ner_batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle_files(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 92806/92806 [00:39<00:00, 2375.71it/s]\n",
      "2021-04-06 09:36:39 INFO: Writing properties to tmp file: corenlp_server-6cd4306e2f9c4fbe.props\n",
      "2021-04-06 09:36:39 INFO: Starting server with command: java -Xmx8G -cp C:\\Users\\nikodemicek\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 600000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-6cd4306e2f9c4fbe.props -annotators tokenize,ssplit,pos,lemma,parse,coref,ner,depparse -preload -outputFormat serialized\n",
      "  0%|                                                                                        | 0/92806 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting coreference chains...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████████████████████████████▋                 | 70198/92806 [10:50:18<4:01:45,  1.56it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "tqdm.pandas()\n",
    "moria_tweets = event_df['text'].progress_apply(preprocessing.preprocess_tweets)\n",
    "moria_crf_list = cand_ex.extract_corefs(moria_tweets)\n",
    "del moria_tweets\n",
    "pickle_files('moria_crf_list',moria_crf_list)\n",
    "del moria_crf_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['no_candidate'],\n",
       " [],\n",
       " ['UNHCR'],\n",
       " ['immigrant camps'],\n",
       " [],\n",
       " ['Refugees'],\n",
       " ['Donald Trump'],\n",
       " ['we'],\n",
       " ['you', 'the illegals'],\n",
       " [],\n",
       " ['this man'],\n",
       " [],\n",
       " ['The same one antifa'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Trump supporters'],\n",
       " [],\n",
       " ['I', 'countries in gulf', 'Afgan , BD and other refugees'],\n",
       " [],\n",
       " ['you'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['I', 'you all', 'they all'],\n",
       " ['Nigeria'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Roman'],\n",
       " ['Which UK people'],\n",
       " [],\n",
       " ['the uk', 'I'],\n",
       " [],\n",
       " ['we', 'Racism'],\n",
       " [],\n",
       " ['THEY'],\n",
       " ['him'],\n",
       " ['Arson'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['I', 'Donald Trump'],\n",
       " [],\n",
       " ['it', 'Greek migrants'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Notts'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['more than 11,200 people'],\n",
       " [],\n",
       " ['more than 11,200 people'],\n",
       " [],\n",
       " ['They'],\n",
       " ['them all', 'We'],\n",
       " ['the crowded conditions'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Turkey'],\n",
       " ['Glasgow City Council', 'Petition'],\n",
       " [],\n",
       " [],\n",
       " ['us'],\n",
       " ['their'],\n",
       " ['a building on Lee St'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['the deaths of Americans'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['more than 11,200 people'],\n",
       " [],\n",
       " [],\n",
       " ['Greeks', 'I'],\n",
       " ['Glasgow'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['We', 'this'],\n",
       " [],\n",
       " ['refugees across greece who have had their status for more than a month',\n",
       "  'Claiming asylum'],\n",
       " ['more than 11,200 people'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['the Greek police'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['ALL of the main political parties'],\n",
       " [],\n",
       " ['people'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['We', '11,200 refugees'],\n",
       " [],\n",
       " ['Greece', 'refugees'],\n",
       " [],\n",
       " ['any migrants'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['India'],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " [],\n",
       " ['you'],\n",
       " ['Greece'],\n",
       " ['least twelve people', 'migrant boat'],\n",
       " ['the Greek government'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['greece'],\n",
       " ['RELOCATION OFF GREEK MAINLAND'],\n",
       " ['people'],\n",
       " [],\n",
       " [],\n",
       " ['I'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['President Trump'],\n",
       " [],\n",
       " ['NGOs'],\n",
       " ['RELOCATION OFF GREEK MAINLAND'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['refugees who have received a positive decision on their claims for asylum'],\n",
       " [],\n",
       " ['10,000 refugees'],\n",
       " [],\n",
       " ['LOST AND FOUND CAT by @DougKuntzPhotos'],\n",
       " [],\n",
       " [],\n",
       " ['the respective minister'],\n",
       " [],\n",
       " ['he', 'this'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['more than 11,200 people'],\n",
       " ['my'],\n",
       " ['This', 'your'],\n",
       " ['the employer'],\n",
       " [],\n",
       " [],\n",
       " ['Glasgow'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['I', 'a Somali immigrant'],\n",
       " [],\n",
       " ['You'],\n",
       " ['me'],\n",
       " ['Four people'],\n",
       " [],\n",
       " ['Greece', 'you', 'them'],\n",
       " ['Fr', 'Seraphim', 'I'],\n",
       " [],\n",
       " [],\n",
       " ['a 5 am fire at 791 E'],\n",
       " [],\n",
       " [],\n",
       " ['They'],\n",
       " ['it'],\n",
       " ['the thin man', 'the Greek immigrant'],\n",
       " [],\n",
       " [],\n",
       " ['us'],\n",
       " [],\n",
       " [],\n",
       " ['you', 'I', 'my beautiful country'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Greece'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Greece with its poor economy and pressure from refugees', 'we'],\n",
       " [],\n",
       " [],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " [],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " ['refugees'],\n",
       " [],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " [],\n",
       " [],\n",
       " ['illegal immigrants'],\n",
       " [],\n",
       " ['our', 'these Violent protestors'],\n",
       " ['you'],\n",
       " [],\n",
       " ['The Southwest'],\n",
       " ['the EU / Greece'],\n",
       " ['@TimesofSweden'],\n",
       " ['God', 'the devil', 'you'],\n",
       " [],\n",
       " ['We', 'refugees'],\n",
       " ['Abey hatau'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Greece', 'Our'],\n",
       " ['Their members'],\n",
       " [],\n",
       " [],\n",
       " ['this', 'We'],\n",
       " [],\n",
       " ['Today', 'refugees and solidarians'],\n",
       " [],\n",
       " ['They'],\n",
       " ['We'],\n",
       " [],\n",
       " [],\n",
       " ['Greece'],\n",
       " ['we'],\n",
       " [],\n",
       " ['CBP'],\n",
       " [],\n",
       " ['you', 'all kids', 'their'],\n",
       " ['you', 'Our population'],\n",
       " [],\n",
       " [],\n",
       " ['He'],\n",
       " ['someone'],\n",
       " [],\n",
       " ['He'],\n",
       " ['ISF'],\n",
       " [],\n",
       " [],\n",
       " ['greece 8,300 refugees'],\n",
       " [],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [\"my people ' s land\", \"my people ' s\"],\n",
       " [],\n",
       " ['me', 'you'],\n",
       " ['people'],\n",
       " [],\n",
       " [],\n",
       " ['a British - Senegalese refugee lawyer'],\n",
       " ['marginalized people', 'Trump'],\n",
       " [],\n",
       " [],\n",
       " ['Soros'],\n",
       " ['I', 'their', 'you'],\n",
       " ['We', '3 home fire responses'],\n",
       " [],\n",
       " [],\n",
       " ['our country', 'you'],\n",
       " ['you'],\n",
       " ['I'],\n",
       " [],\n",
       " [],\n",
       " ['The Red Cross'],\n",
       " ['You'],\n",
       " [],\n",
       " ['the US'],\n",
       " ['Florida', 'officers', 'GRADY JUDD'],\n",
       " ['I'],\n",
       " ['She', 'her ancestors'],\n",
       " ['our'],\n",
       " ['an insurrection , which deserves a pants on fire rating from @PolitiFact',\n",
       "  'Trump'],\n",
       " ['Some'],\n",
       " ['refugees', 'our'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['A family of seven'],\n",
       " ['So many people in Greece'],\n",
       " ['The president'],\n",
       " ['they', 'I'],\n",
       " [],\n",
       " [],\n",
       " ['Immigrants and fagots', 'some mini - Iran', 'me'],\n",
       " ['you'],\n",
       " ['I', 'a farm', 'An immigrant family', 'Locals'],\n",
       " ['them'],\n",
       " ['he', 'immigrants', 'many immigrants from Africa & other countries'],\n",
       " ['He'],\n",
       " ['my father', 'My'],\n",
       " [],\n",
       " ['His', 'those in power'],\n",
       " [],\n",
       " [],\n",
       " ['we'],\n",
       " [],\n",
       " ['The UK', 'the total idiots who believe all these migrants'],\n",
       " ['They'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Greece'],\n",
       " [],\n",
       " ['my', 'they', 'you'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Greek Orthodox , Muslims , Armenian Orthodox , Maronites Latin Catholics on our beautiful island',\n",
       "  'a Rabbi'],\n",
       " ['his anti Black', 'Trump'],\n",
       " [],\n",
       " ['She', 'Jouri'],\n",
       " [],\n",
       " ['the vandalized Orthodox Church'],\n",
       " ['migrants with refugee status for more than one month'],\n",
       " [],\n",
       " [],\n",
       " ['Greeks', 'The boat'],\n",
       " [],\n",
       " ['the Greek island of Lesvos', '229 illegal muslim migrants'],\n",
       " [],\n",
       " ['A humanitarian disaster'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['your', 'the world'],\n",
       " ['Three front line workers describing their experiences at the Moria Refugee Camp in Lesvos'],\n",
       " [],\n",
       " [],\n",
       " ['Its pity Modi'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['refugees UNHCR greece'],\n",
       " ['many refugees in Greece'],\n",
       " [],\n",
       " [],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " ['your'],\n",
       " ['UNHCR', 'many recognized refugees'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " [],\n",
       " [],\n",
       " ['greek migration minister Notis mitarakis'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['RREs new report'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['The Hellenic CG', 'us'],\n",
       " [],\n",
       " [],\n",
       " ['they'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['immigrants with sulfuric acid and the same chemical used in Nazi gas chambers'],\n",
       " ['you'],\n",
       " ['Greece -', 'The EU'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['refugees', 'these people'],\n",
       " ['we'],\n",
       " [],\n",
       " ['the Greek tent - like raft'],\n",
       " [],\n",
       " [],\n",
       " ['the @Refugees'],\n",
       " [],\n",
       " ['in the States'],\n",
       " [],\n",
       " [],\n",
       " ['Ethniscist / racists Greek people'],\n",
       " [],\n",
       " ['illegal immigrants'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Leros'],\n",
       " [],\n",
       " ['me'],\n",
       " ['Jerry Leiber and Mike Stoller'],\n",
       " ['its', 'you'],\n",
       " [],\n",
       " ['the EU'],\n",
       " ['Turkey'],\n",
       " ['CYAer Bliss Perry'],\n",
       " ['God', 'the devil'],\n",
       " [],\n",
       " ['refugees'],\n",
       " ['One firefighter'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['I'],\n",
       " [],\n",
       " ['you'],\n",
       " [],\n",
       " ['the covid _ 19 pandemic', 'them', 'you'],\n",
       " [],\n",
       " ['My'],\n",
       " [],\n",
       " ['8.300 refugees', 'EU'],\n",
       " ['them'],\n",
       " ['English', 'the African Americans', 'i'],\n",
       " ['unaccompanied migrant kids'],\n",
       " [],\n",
       " [],\n",
       " ['they'],\n",
       " ['the EU'],\n",
       " ['The ones at the baseball game'],\n",
       " ['America'],\n",
       " ['Youre'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['the EU', 'their'],\n",
       " [],\n",
       " ['they'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['you'],\n",
       " ['you', 'me'],\n",
       " ['Greece'],\n",
       " ['Iraq', 'he', 'I'],\n",
       " ['that reaction'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Joe', 'I'],\n",
       " ['you', 'all kids', 'their'],\n",
       " ['migration policy', 'Greece'],\n",
       " ['the clowns', 'they'],\n",
       " ['you', 'all kids', 'their'],\n",
       " [],\n",
       " ['They', 'the us constitution', 'You'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['your', 'i', 'ACAB'],\n",
       " [],\n",
       " [\"the migrants '\"],\n",
       " ['He'],\n",
       " [],\n",
       " ['YOU'],\n",
       " ['people staying there in the last few days'],\n",
       " ['France'],\n",
       " [],\n",
       " [],\n",
       " ['Greece'],\n",
       " ['you'],\n",
       " ['the illegals Erdogan forces', 'you'],\n",
       " ['I'],\n",
       " [],\n",
       " ['a red meat sauce', 'Coney chili'],\n",
       " ['The Greek immigrants'],\n",
       " ['Greece'],\n",
       " ['the wall'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['the Anishinaabe', 'us'],\n",
       " ['Nearly 300,000 displaced people'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['greek people'],\n",
       " [],\n",
       " ['I'],\n",
       " ['POLICE'],\n",
       " ['America', \"the country ' s million - plus Greek immigrants\"],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['asylum seekers'],\n",
       " ['UNHCR', 'them'],\n",
       " ['greece', 'he'],\n",
       " ['those looters', 'I'],\n",
       " [],\n",
       " ['fire'],\n",
       " ['all illegal migrants'],\n",
       " ['more than 100 people'],\n",
       " [],\n",
       " ['the migrants', 'Especially Karnataka people and other southern states'],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " ['Greek border guards', 'A big crisis facing the Islam today'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['The owners'],\n",
       " [],\n",
       " ['Aight'],\n",
       " [],\n",
       " ['Pakistan', 'the guest refugees'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['God', 'you', 'the devil'],\n",
       " [],\n",
       " ['Greece', 'Thousands of migrants'],\n",
       " [],\n",
       " ['I'],\n",
       " ['the immigrants', 'you'],\n",
       " [],\n",
       " ['greece', 'you'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['EU'],\n",
       " [],\n",
       " ['Bezos', 'we'],\n",
       " ['my', 'you', 'my tweet or my comments'],\n",
       " ['8,300 refugees'],\n",
       " ['Greece & Italy'],\n",
       " [],\n",
       " ['the illegal migrants'],\n",
       " ['Greece', 'WTF'],\n",
       " ['the EU'],\n",
       " ['you', 'all kids', 'their'],\n",
       " ['two different governments in Greece'],\n",
       " ['Leonid Afremovan', 'a Russian immigrant'],\n",
       " ['Sweden'],\n",
       " ['france'],\n",
       " [],\n",
       " ['400 refugees'],\n",
       " [],\n",
       " ['We'],\n",
       " [],\n",
       " [],\n",
       " ['Greece & Italy'],\n",
       " [],\n",
       " [],\n",
       " ['it', 'you'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['we'],\n",
       " [],\n",
       " [],\n",
       " ['Refugees in Greece', \"Greece ' s\"],\n",
       " [],\n",
       " [],\n",
       " ['People'],\n",
       " [],\n",
       " ['Pakistani refugee Muhammad Gulzar', 'The Greek government'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['thousands of refugee children'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Abdelkader'],\n",
       " [],\n",
       " ['Its'],\n",
       " ['Greece refugees'],\n",
       " [],\n",
       " [],\n",
       " ['their'],\n",
       " [],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['values', \"the EU '\"],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Europe'],\n",
       " ['Some 9,000 former asylum seekers who became recognized refugees in greece'],\n",
       " [],\n",
       " ['We'],\n",
       " ['the eu'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['I'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['our', 'I'],\n",
       " [],\n",
       " ['Greece', 'Or EU', 'ReliefWeb'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [\"Nixon '\"],\n",
       " ['we'],\n",
       " ['They', 'you', 'expired stuff'],\n",
       " ['our FREEDOM', 'we'],\n",
       " ['two old family business owners'],\n",
       " ['the people'],\n",
       " ['Its'],\n",
       " [],\n",
       " ['gassing refugees'],\n",
       " [],\n",
       " ['The Republicans in the state of Iowa', 'Steve King'],\n",
       " [],\n",
       " ['the trailer', 'your'],\n",
       " ['Kos'],\n",
       " [],\n",
       " ['Trump', 'Barr', 'We'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['afghan refugees'],\n",
       " ['you', 'migrant kids', 'their'],\n",
       " [],\n",
       " ['asylum seekers attending the mental health centre in Moria'],\n",
       " ['this fund'],\n",
       " ['my clothes', 'his', 'thousands of men', 'Ye'],\n",
       " [],\n",
       " ['Fire'],\n",
       " ['their'],\n",
       " ['migrants'],\n",
       " ['refugees', 'I'],\n",
       " ['refugees', 'I'],\n",
       " [],\n",
       " ['I'],\n",
       " ['his'],\n",
       " ['your'],\n",
       " ['NGOs'],\n",
       " ['Moria'],\n",
       " ['The immigration crisis'],\n",
       " ['Greek immigrants', 'my grandfather'],\n",
       " [],\n",
       " [],\n",
       " ['Turkey'],\n",
       " ['Attorney Urooj Rahman'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['100 refugees or recipients of subsidiary protection'],\n",
       " [],\n",
       " [],\n",
       " ['you', 'migrant kids', 'Hundreds of migrant kids'],\n",
       " [],\n",
       " [],\n",
       " ['Greece', 'Italy', 'we', 'them'],\n",
       " [],\n",
       " ['we', 'black people'],\n",
       " ['we', 'Greek police', 'me'],\n",
       " ['About migrant labours'],\n",
       " [],\n",
       " ['this time', 'I', 'George', 'the fire'],\n",
       " [],\n",
       " ['the EU', 'you', 'the refugees'],\n",
       " ['bad actors who are ignorant'],\n",
       " ['He'],\n",
       " [],\n",
       " ['them all'],\n",
       " ['I'],\n",
       " [],\n",
       " [],\n",
       " ['Attorney Urooj Rahman'],\n",
       " ['your'],\n",
       " [],\n",
       " ['Molotov cocktails'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['first nations people', 'I', 'her'],\n",
       " ['those'],\n",
       " ['them', 'I'],\n",
       " [],\n",
       " ['angry poor people'],\n",
       " ['these people'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [\"Don '\", 'your kids'],\n",
       " ['dictator Erdogan'],\n",
       " ['Golden Dawn'],\n",
       " ['this fire'],\n",
       " [],\n",
       " ['Turkey'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['those \" sensitive \" Greek people'],\n",
       " ['those \" sensitive \" Greek people'],\n",
       " ['immigrants'],\n",
       " ['fire bombing attempt'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Attorney Urooj Rahman'],\n",
       " ['your', 'the Sheriff'],\n",
       " [],\n",
       " ['I', 'the support', 'we'],\n",
       " ['you', 'they'],\n",
       " [],\n",
       " ['Greek anarchists'],\n",
       " ['they'],\n",
       " [],\n",
       " [],\n",
       " ['i', 'Greek anarchists'],\n",
       " ['Greek anarchists'],\n",
       " ['Turkey', 'I', 'you'],\n",
       " ['the refugees', \"Greece '\"],\n",
       " ['the same Greece'],\n",
       " ['our', 'Greece'],\n",
       " ['Urooj Rahman'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['a photo'],\n",
       " [],\n",
       " [],\n",
       " ['Obama'],\n",
       " ['You', 'he'],\n",
       " ['Turkey', 'I', 'you'],\n",
       " ['your not among the ones'],\n",
       " [],\n",
       " [],\n",
       " ['Black immigrants from other countries'],\n",
       " ['Greek citizens', 'you'],\n",
       " ['my parents', 'I'],\n",
       " ['Wow'],\n",
       " [],\n",
       " [],\n",
       " ['minorities', 'America', 'comapanies and managers', 'I'],\n",
       " ['these woke companies'],\n",
       " [],\n",
       " ['your', 'your friends'],\n",
       " ['Greece'],\n",
       " ['you'],\n",
       " [],\n",
       " ['a choice'],\n",
       " ['the homeless and fire refugees in caravans'],\n",
       " [],\n",
       " ['migrants'],\n",
       " ['Greek mayor'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Turkey'],\n",
       " [],\n",
       " [],\n",
       " ['Greece', 'thousands of refugees', 'France'],\n",
       " [],\n",
       " [],\n",
       " ['them', 'your', 'your great UAV'],\n",
       " [],\n",
       " ['migrant kids'],\n",
       " ['god Greek Coast Guard'],\n",
       " ['The terrorist Caliphate of Erdogan'],\n",
       " [],\n",
       " [],\n",
       " ['the migrant crisis'],\n",
       " ['Greece'],\n",
       " ['Lefteris Papagiannakis'],\n",
       " ['for the Chinese'],\n",
       " [],\n",
       " [],\n",
       " ['racist ignoramuses that run their mouth talking hurtful , harmful , and incorrect racist shit',\n",
       "  'we'],\n",
       " ['the elephant'],\n",
       " [],\n",
       " ['the oppressor', 'Greece', 'their', 'The EU'],\n",
       " [],\n",
       " ['Attorney Urooj Rahman'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['the pandemic', 'Greece'],\n",
       " [],\n",
       " ['kurdish people'],\n",
       " [],\n",
       " ['They', 'Wisegrad Group'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['they', \"Greece isn '\"],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [\"those in Greece that want open borders and either wear hoodies ( koukoulofori ) or support immigrants as NGO ' s ( MKO )\"],\n",
       " [],\n",
       " [],\n",
       " ['his', 'OBAMA'],\n",
       " ['a petition'],\n",
       " ['you'],\n",
       " ['asylum seekers on the Aegean island'],\n",
       " [],\n",
       " [],\n",
       " ['the EU protest'],\n",
       " [],\n",
       " ['you', 'greek islands'],\n",
       " [],\n",
       " ['her', 'I'],\n",
       " [],\n",
       " ['healthcare'],\n",
       " [],\n",
       " ['A new migrant camp in Malakasa , east of the Greek capital'],\n",
       " ['Our', 'It'],\n",
       " [],\n",
       " ['our'],\n",
       " ['yoga'],\n",
       " ['Fourteen afghan asylum seekers'],\n",
       " ['greece'],\n",
       " [],\n",
       " ['us', 'you'],\n",
       " ['fleeing migrants'],\n",
       " [],\n",
       " ['fleeing migrants'],\n",
       " [],\n",
       " [],\n",
       " ['I', 'him'],\n",
       " ['COVID - 19'],\n",
       " [],\n",
       " ['masked men'],\n",
       " [],\n",
       " [\"Greece doesn '\", 'migrants'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['this illegal immigrants'],\n",
       " [],\n",
       " ['these kids', 'Our partners'],\n",
       " ['ALL Eu countries'],\n",
       " [],\n",
       " ['our'],\n",
       " [],\n",
       " ['European Citizens'],\n",
       " ['you'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Syrian refugees'],\n",
       " [],\n",
       " [],\n",
       " ['we'],\n",
       " ['the Danish soldiers', 'rescued migrants'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['the refugee camp of Nea Kavala in northern Greece'],\n",
       " [],\n",
       " [],\n",
       " ['I'],\n",
       " ['a globalised world', 'I', 'all the immigrants'],\n",
       " ['Erdogan', 'his own people', 'We'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['they'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Migrants', 'animal cant talk'],\n",
       " [],\n",
       " [],\n",
       " ['We', 'the crisis'],\n",
       " [],\n",
       " ['workers'],\n",
       " [],\n",
       " ['she'],\n",
       " ['@AthensLiveGr journalists'],\n",
       " ['us', 'the USA'],\n",
       " [],\n",
       " [],\n",
       " ['refugees', 'you'],\n",
       " [],\n",
       " ['you'],\n",
       " [],\n",
       " [],\n",
       " ['fire WARNING shots on the French Navy'],\n",
       " [],\n",
       " ['My father'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['my'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_crf_list  = load_pickle('moria_crf_list')\n",
    "event_crf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_np_list,event_crf_list,event_tagged_tweets = load_event_data('moria')\n",
    "\n",
    "event_crf_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_candlist_for_batching(candidate):\n",
    "    #change noun_phrase_list to batching compatible format    \n",
    "    candidate = ['candidate_to_be_removed'] if len(candidate) == 0 else candidate     \n",
    "    return ' \\n '.join(candidate)\n",
    "\n",
    "#print(f'Tagging {event_name} noun phrase candidates...')\n",
    "#tag all tweets and save them in a list    \n",
    "#batched_np_list = cand_prep.prep_candlist_for_batching(event_np_list)\n",
    "# remove NP candidates longer than threshold and remove all child NPs of parent NPs\n",
    "event_np_list = cand_prep.remove_long_nps(event_np_list[0:100])\n",
    "event_np_list = cand_prep.remove_child_nps(event_np_list)\n",
    "tqdm.pandas()\n",
    "\n",
    "event_np_list = remove_weird_chars(event_np_list)\n",
    "event_np_series = pd.Series(event_np_list)\n",
    "#event_np_series = event_np_series.progress_apply(lowercase_except_first)\n",
    "#print(event_np_series)\n",
    "#batched_np_series = event_np_series.progress_apply(prep_candlist_for_batching)\n",
    "#batched_np_list = list(event_np_series.progress_apply(prep_candlist_for_batching))\n",
    "#print(batched_np_list)\n",
    "\n",
    "#which if these two will be faster should be used \n",
    "#tagged_np_cands = batched_np_series.progress_apply(en_nlp)\n",
    "tagged_np_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_np_list)]\n",
    "\n",
    "#fast but is breaking the text\n",
    "#tagged_np_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_np_list, en_nlp, batch_size=6000))]\n",
    "#tagged_np_cands = en_nlp('\\n\\n'.join(event_np_list))\n",
    "#tagged_np_cands = en_nlp('\\n\\n'.join(['. '.join(cand) for cand in event_np_list]))\n",
    "\n",
    "#print(tagged_np_cands[0].sentences[1])\n",
    "np_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tagged_np_cands]\n",
    "#print(np_cand_heads)\n",
    "\n",
    "\n",
    "# get easily accessible list of tuples (POS-tags of each word, NER-tags of each named entity) \n",
    "tweet_tags = cand_prep.get_tweet_tags(event_tagged_tweets) \n",
    "\n",
    "\n",
    "np_and_cand_list = cand_prep.get_cand_type(event_np_list,np_cand_heads, tweet_tags)\n",
    "\n",
    "\n",
    "nps_cands = [cand for cands in np_and_cand_list for cand in cands]\n",
    "nps_tagged = [sent for tagged_cand in tagged_np_cands for sent in tagged_cand.sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nps_tagged)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_np_cands = en_nlp('\\n\\n'.join(['. '.join(cand) for cand in event_np_list]))\n",
    "tagged_np_cands.text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tagged_np_cands[30].sentences[0].text)\n",
    "for i in range(len(event_np_list)):\n",
    "    print(f'at index {i}: {event_np_list[i]}')\n",
    "    #print(f'at index {i}: {event_tagged_tweets[i].text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 217\n",
    "print(f'at index {i}: {nps_cands[i][0]}')\n",
    "print(f'at index {i}: {nps_tagged[i].text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nps_cands),len(nps_tagged))\n",
    "\n",
    "for i in range(len(nps_cands)):\n",
    "    if nps_tagged[i].text != nps_cands[i][0]:\n",
    "        #print(f'index {i} doesnt match for {len(nps_tagged[i].text)} and {len(nps_cands[i][0])}')\n",
    "        print(f'index {i} doesnt match for {nps_tagged[i].text} and {nps_cands[i][0]}')\n",
    "        \n",
    "\n",
    "\n",
    "cand_df = pd.DataFrame(\n",
    "    {'candidates': nps_cands,\n",
    "     'cand_tags': nps_tagged\n",
    "    })\n",
    "\n",
    "cand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event_data(event_name):\n",
    "    assert event_name in ['moria','tigray','channel','all','beirut'], f\"Oh no! We do not analyze {event_name} event\"\n",
    "    \n",
    "    print(f'Loading {event_name} data...')\n",
    "    try:\n",
    "        #sample = 2000\n",
    "        event_np_list = load_pickle(event_name + '_np_list')#[1000:sample]\n",
    "        event_crf_list = load_pickle(event_name + '_crf_list')#[1000:sample]\n",
    "        event_tagged_tweets = load_pickle(event_name + '_tagged_tweets')#[1000:sample]\n",
    "        \n",
    "        return event_np_list,event_crf_list,event_tagged_tweets\n",
    "    except:\n",
    "        print(f'The {event_name} files not found! Run candidate_extraction.py file on the {eventname}_df')\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def remove_weird_chars(event_cand_list):\n",
    "    weird_chars = ['@','>','<','\\xa0','  -  ','.']\n",
    "    for char in weird_chars:\n",
    "        event_cand_list = cand_prep.remove_char(event_cand_list,char)\n",
    "    return event_cand_list\n",
    "\n",
    "def lowercase_except_first(tweet_cands):\n",
    "    #print(tweet_cands)\n",
    "    cand_lowercased = [\" \".join([word[0] + word[1:].lower() for word in cand.split()]) for cand in tweet_cands ]\n",
    "    #print(cand_lowercased)\n",
    "    return cand_lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pipeline(event_name):\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 1. LOAD THE DATA ~~~~~~~~~~~~~~~~~~~~~\n",
    "    event_np_list,event_crf_list,event_tagged_tweets = load_event_data(event_name)\n",
    "    \n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 2. GET POS AND NER TAGS ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # get easily accessible list of tuples (POS-tags of each word, NER-tags of each named entity) \n",
    "    tweet_tags = cand_prep.get_tweet_tags(event_tagged_tweets) \n",
    "    \n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 3. PREPROCESS CANDIDATES ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ~~~~~~~~~~~~ processing of noun phrases ~~~~~~~~~~~~~~~~~~~~~\n",
    "    print(f'Processing {event_name} noun phrase candidates...')\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    # remove NP candidates longer than threshold and remove all child NPs of parent NPs\n",
    "    event_np_list = cand_prep.remove_long_nps(event_np_list)\n",
    "    event_np_list = cand_prep.remove_child_nps(event_np_list) \n",
    "    #event_np_list = remove_weird_chars(event_np_list)\n",
    "    event_np_list = cand_prep.remove_char(event_np_list,'@')\n",
    "\n",
    "    event_np_list = [['no_candidate'] if len(noun_ps)==0 or noun_ps ==' ' else noun_ps for noun_ps in event_np_list ]\n",
    "    \n",
    "    #print(event_np_list)\n",
    "    print(f'Tagging {event_name} noun phrase candidates...')\n",
    "    #tag all tweets and save them in a list    \n",
    "\n",
    "    #tagged_np_cands = batched_np_list.progress_apply(en_nlp)\n",
    "    tagged_np_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_np_list)]\n",
    "    #tagged_np_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_np_list, en_nlp, batch_size=6000))]\n",
    "\n",
    "    np_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tagged_np_cands]\n",
    "    #print(np_cand_heads)\n",
    "    \n",
    "    np_and_cand_list = cand_prep.get_cand_type(event_np_list,np_cand_heads, tweet_tags)\n",
    "    #print(event_np_list)\n",
    "          \n",
    "    # ~~~~~~~~~~~~ processing of coref candidates ~~~~~~~~~~~~~~~~~~~~~\n",
    "    print(f'Processing {event_name} coreference candidates...')    \n",
    "    \n",
    "    #extract only the representative mentions as representative phrases of candidates\n",
    "    event_crf_list = [[coref_group[0][coref_group[1]] for coref_group in tweet_corefs] for tweet_corefs in event_crf_list]\n",
    "    \n",
    "    #event_crf_list = remove_weird_chars(event_crf_list)\n",
    "    event_crf_list = cand_prep.remove_char(event_crf_list,'@')\n",
    "\n",
    "    event_crf_list = [['no_candidate'] if len(crf_ps)==0 else crf_ps for crf_ps in event_crf_list ]\n",
    "    \n",
    "    print(f'Tagging {event_name} coreference candidates...')       \n",
    "    #tag all tweets and save them in a list    \n",
    "    #batched_coref_list = cand_prep.prep_candlist_for_batching(event_crf_list)\n",
    "    #print(batched_coref_list)\n",
    "    tagged_coref_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_crf_list)]\n",
    "    #tagged_coref_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_coref_list, en_nlp, batch_size=6000))] \n",
    "    #print(tagged_coref_cands)\n",
    "        \n",
    "    coref_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tagged_coref_cands]\n",
    "          \n",
    "    coref_and_cand_list = cand_prep.get_cand_type(event_crf_list, coref_cand_heads, tweet_tags)\n",
    "    print(len(coref_and_cand_list))\n",
    "    print(len(event_np_list))\n",
    "          \n",
    "    # ~~~~~~~~~~~~~~~~~~~~ combining candidate lists ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #concatenate corefs and noun phrase lists\n",
    "    nps_cands = [cand for cands in np_and_cand_list for cand in cands]\n",
    "    crf_cands = [cand for cands in coref_and_cand_list for cand in cands]\n",
    "    #candidate_list = coref_and_cand_list + np_and_cand_list\n",
    "    #print(f'Len = {len(candidate_list)} should be 2x amount of tweets')\n",
    "    #print(len(nps_cands), len(crf_cands))\n",
    "    #unpack list of lists into one list\n",
    "    candidate_list = nps_cands + crf_cands\n",
    "    print(f'The amount of all candidates is {len(candidate_list)} -  nps: {len(nps_cands)}, crfs:{len(crf_cands)}')\n",
    "          \n",
    "    nps_tagged = [sent for tagged_cand in tagged_np_cands for sent in tagged_cand.sentences ]\n",
    "    crf_tagged = [sent for tagged_cand in tagged_coref_cands for sent in tagged_cand.sentences ]\n",
    "    print(len(nps_tagged), len(crf_tagged))\n",
    "    all_cands_tagged = nps_tagged + crf_tagged\n",
    "\n",
    "        \n",
    "    #print(len(candidate_list),'vs', len(all_cands_tagged))\n",
    "    cand_df = pd.DataFrame(\n",
    "        {'candidates': candidate_list,\n",
    "         'cand_tags': all_cands_tagged\n",
    "        })\n",
    "\n",
    "    cand_df['cand_text'] = cand_df.candidates.apply(lambda x: x[0])\n",
    "    cand_df['cand_len'] = cand_df.cand_text.apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "    count_cands = Counter(cand_df['cand_text'])\n",
    "    cand_df['cand_freq'] = cand_df[\"cand_text\"].map(count_cands)\n",
    "    \n",
    "    #count_cands[cand_df['cand_text']]\n",
    "    #count_sorted = sorted(count_cands.items(),key=lambda x: x[1],reverse=True)\n",
    "    cand_df.columns = cand_df.columns.str.strip()\n",
    "    \n",
    "          \n",
    "    # we sort the candidates by their length\n",
    "    cand_df.sort_values('cand_freq', ascending=False,inplace=True)\n",
    "\n",
    "    #cand_df = cand_df[cand_df.cand_text not in  ['no_candidate', 'candidate_to_be_removed']]\n",
    "\n",
    "    cand_df.reset_index(drop=True, inplace = True)\n",
    "    #remove dummy candidates that were used to avoid errors\n",
    "\n",
    "    print(len(cand_df))\n",
    "    cand_df = cand_df[cand_df.cand_text != 'candidate_to_be_removed']\n",
    "    cand_df = cand_df[cand_df.cand_text != 'no_candidate']\n",
    "    len(cand_df)\n",
    "    cand_df.reset_index(drop=True,inplace=True)\n",
    "          \n",
    "    return cand_df\n",
    "          \n",
    "          \n",
    "moria_cands = pipeline('moria')\n",
    "\n",
    "pickle_files('moria_cands_df', moria_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_np_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beirut_cands['cand_text'] = beirut_cands['cand_text'].apply(lambda x: x.lower())\n",
    "beirut_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event_np_list = load_pickle('beirut_np_list')\n",
    "\n",
    "event_df['text'][37]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. We apply stanza module on the tweets to get NER and POS tags. We do it in batches to speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. As initial WCL candidates, we extract noun phrases (NPs) and coreference chains.\n",
    "\n",
    "## We do so using CoreNLPClient wrapper\n",
    "\n",
    "### SOME PREPROCESSING NEEDED\n",
    "* remove links - check\n",
    "* remove # from hashtags? - check\n",
    "* remove/merge mentions? - check\n",
    "\n",
    "\n",
    "* remove recurring texts (signatures of news media) - any new spotted should be added in preprocessing file's '__remove_tweet_signatures__' function\n",
    "* remove posts of some accounts (refugee_list)\n",
    "* exclude NERs that tag numbers - should we mark phrase as NE if the head is not NE? - check\n",
    "* play around with candidate types\n",
    "* optimize code and make it neater\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. We keep only NPs shorter than 20 words and remove children of parent NPs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. We get the heads of noun phrases (in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cand_heads(tagged_cands):\n",
    "    # each candidate will be stored as [(set_of_phrases_heads), cand_rep_head] \n",
    "    return [[set([cand.words[word.head-1].text for word in cand.words]), \n",
    "             [word.text for word in cand.words if word.head == 0]] #the root of NP has value 0 \n",
    "             for cand in tagged_cands.sentences]\n",
    "\n",
    "\n",
    "np_cand_heads = [get_cand_heads(tweet_cands) for tweet_cands in tagged_np_cands]\n",
    "print(np_cand_heads)\n",
    "\n",
    "#[print(tagged_np_cand.text) for tagged_np_cand in tagged_np_cands]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. We define candidate types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. We assign candidate types to noun phrase candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. We get coreference chains candidates from the tweet corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. We determine candidate's type for representative mentions of coref candidates (in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the spaces around dashes, slashes and apostrophes should increase stanza's ability to parse sentence correctly\n",
    "corefs_list = [[crf.replace(' - ','-') for crf in corefs] for corefs in corefs_list]\n",
    "corefs_list = [[crf.replace(' / ','/') for crf in corefs] for corefs in corefs_list]\n",
    "corefs_list = [[crf.replace(\" ' \",\"'\") for crf in corefs] for corefs in corefs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_and_cand_list = cand_prep.get_cand_type(corefs_list, coref_cand_heads, tweet_tags, cand_types_dict, corefs=True)\n",
    "\n",
    "\n",
    "print(coref_and_cand_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. We combine the candidate lists for candidate merging\n",
    "\n",
    "We organize candidates in a list sorted by their number of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate corefs and noun phrase lists\n",
    "nps_cands = [cand for cands in np_and_cand_list for cand in cands]\n",
    "crf_cands = [cand for cands in coref_and_cand_list for cand in cands]\n",
    "#candidate_list = coref_and_cand_list + np_and_cand_list\n",
    "#print(f'Len = {len(candidate_list)} should be 2x amount of tweets')\n",
    "print(len(nps_cands), len(crf_cands))\n",
    "#unpack list of lists into one list\n",
    "candidate_list = nps_cands + crf_cands\n",
    "print(f'The amount of all candidates is {len(candidate_list)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nps_tagged = [sent for tagged_cand in tagged_np_cands for sent in tagged_cand.sentences ]\n",
    "crf_tagged = [sent for tagged_cand in tagged_coref_cands for sent in tagged_cand.sentences ]\n",
    "print(len(nps_tagged), len(crf_tagged))\n",
    "all_cands_tagged = nps_tagged + crf_tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(candidate_list))\n",
    "print(len(all_cands_tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_df = pd.DataFrame(\n",
    "    {'candidates': candidate_list,\n",
    "     'cand_tags': all_cands_tagged\n",
    "    })\n",
    "\n",
    "cand_df['cand_text'] = cand_df.candidates.apply(lambda x: x[0])\n",
    "cand_df['cand_len'] = cand_df.cand_text.apply(lambda x: len(x.split()))\n",
    "cand_df.columns = cand_df.columns.str.strip()\n",
    "cand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sort the candidates by their length\n",
    "\n",
    "cand_df.sort_values('cand_len', ascending=False,inplace=True)\n",
    "\n",
    "#cand_df = cand_df[cand_df.cand_text not in  ['no_candidate', 'candidate_to_be_removed']]\n",
    "\n",
    "cand_df.reset_index(drop=True, inplace = True)\n",
    "cand_df\n",
    "#all_cands_tagged.sort(reverse=True,key=get_cand_len(candidate_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove dummy candidates that were used to avoid errors\n",
    "\n",
    "print(len(cand_df))\n",
    "cand_df = cand_df[cand_df.cand_text != 'candidate_to_be_removed']\n",
    "cand_df = cand_df[cand_df.cand_text != 'no_candidate']\n",
    "len(cand_df)\n",
    "cand_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "cand_df = pd.read_csv('beirut_test.csv',converters={\"candidates\": ast.literal_eval,\"cand_tags\": ast.literal_eval})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ast\n",
    "cand_df = pd.read_csv('beirut_test.csv',converters={\"candidates\": ast.literal_eval,\"cand_tags\": ast.literal_eval},index_col=0)\n",
    "count_cands = Counter(cand_df['cand_text'])\n",
    "\n",
    "cand_df['cand_freq'] = cand_df[\"cand_text\"].map(count_cands)\n",
    "#count_cands[cand_df['cand_text']]\n",
    "count_sorted = sorted(count_cands.items(),key=lambda x: x[1],reverse=True)\n",
    "cand_df = cand_df[cand_df['cand_len'] < 11].sort_values('cand_freq',ascending=False)\n",
    "cand_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First merging step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# THIS IS THE FIRST MERGING STEP\n",
    "#\n",
    "\n",
    "        \n",
    "def merging_step1(candidate_list):\n",
    "    \"\"\"\n",
    "    In the first merging step, we merge two candidates if the head of each of their representative phrase \n",
    "     is identical by string comparison.\n",
    "    \"\"\"\n",
    "    #print(list(candidate_list))\n",
    "    indices_to_remove = set()\n",
    "    for up_cand_id in tqdm(range(len(candidate_list))):   \n",
    "        ##if index is already marked to remove then skip\n",
    "        if up_cand_id in indices_to_remove:\n",
    "            continue\n",
    "        up_cand = candidate_list[up_cand_id]    \n",
    "            \n",
    "        for low_cand_id in range(up_cand_id+1,len(candidate_list)):\n",
    "            low_cand = candidate_list[low_cand_id]\n",
    "            #print(f'comparing {longer_cand} with {cand}')\n",
    "            #print(f'for index {candidate_list[longer_cand][1]} checking the index {candidate_list[cand][1]}')\n",
    "            #print(type(candidate_list[longer_cand]))\n",
    "            #mark for merging if the head and its head's cand type is the same for 2 candidates\n",
    "            #print(candidate_list[longer_cand][1],candidate_list[longer_cand][3])\n",
    "            if up_cand[1].lower() == low_cand[1].lower():# and upper_cand[3] == lower_cand[3]:\n",
    "                #print(f'matching \"{longer_cand}\" with \"{cand}\"')\n",
    "                #print(f'{candidate_list[longer_cand][1]} ===== {candidate_list[cand][1]}')\n",
    "                indices_to_remove.add(low_cand_id)\n",
    "                #what_merged[candidate_list[longer_cand][0]].append(candidate_list[cand][0])\n",
    "                \n",
    "    return indices_to_remove\n",
    "\n",
    "\n",
    "def merge_indices(cand_df,indices_to_remove):                \n",
    "\n",
    "    print(f'Initial amount of candidates: {len(cand_df)}')                \n",
    "    #print(len(sorted(indices_to_remove)))\n",
    "\n",
    "    #for index in reversed(sorted(indices_to_remove)):\n",
    "    cand_df.drop(indices_to_remove,inplace=True)\n",
    "        \n",
    "    cand_df.reset_index(drop=True,inplace=True)\n",
    "    print(f'Amount of candidates: {len(cand_df)}, after removing {len(sorted(indices_to_remove))} indices') \n",
    "    return cand_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in cand_df['candidates']:\n",
    "    print(cand[1], cand[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second merging step\n",
    "\n",
    "We merge 2 candidates if their sets of phrases heads are semantically similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "#load the GoogleNews 300dim model (fix path)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\nikodemicek\\Dropbox (CBS)\\Master thesis data\\GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "#adjust for sets of phrases in the candidate\n",
    "def merging_step2(candidate_list):\n",
    "    \n",
    "    indices_to_remove = set()\n",
    "    \n",
    "    for upper_cand_id in tqdm(range(len(candidate_list))):     \n",
    "        upper_cand = candidate_list[upper_cand_id]\n",
    "        \n",
    "        up_cand_mean_vec = phrase_heads_avg_vector(upper_cand[2])\n",
    "        \n",
    "        for lower_cand_id in range(upper_cand_id+1,len(candidate_list)): \n",
    "            lower_cand = candidate_list[lower_cand_id]\n",
    "            #print(f'for index {candidate_list.index(longer_cand)} checking the index {candidate_list.index(cand)}')\n",
    "            #if candidate_list[longer_cand][1] == candidate_list[cand][1]:\n",
    "                #print(f'matching \"{longer_cand}\" with \"{cand}\"')\n",
    "            low_cand_mean_vec = phrase_heads_avg_vector(lower_cand[2])\n",
    "\n",
    "            if upper_cand[3] == lower_cand[3]:\n",
    "                #try:\n",
    "                    #print(1-cosine(long_cand_mean_vec,cand_mean_vec))\n",
    "                    #print(long_cand_mean_vec.reshape(-1,1).shape, cand_mean_vec.reshape(1,-1).shape)\n",
    "                    if 1-cosine(up_cand_mean_vec,low_cand_mean_vec) >= 0.7:\n",
    "                        #print(f'matching \"{longer_cand}\" with \"{cand}\"') \n",
    "                        indices_to_remove.add(lower_cand_id)\n",
    "                        what_merged2[upper_cand[0].lower()].append(lower_cand[0].lower())\n",
    "                        \n",
    "                #except AttributeError:\n",
    "                    #pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                if 1-cosine(up_cand_mean_vec,low_cand_mean_vec) >= 0.7:\n",
    "                    #print(f'matching \"{longer_cand}\" with \"{cand}\"') \n",
    "                    indices_to_remove.add(lower_cand_id)\n",
    "                    what_merged2[upper_cand[0].lower()].append(lower_cand[0].lower())\n",
    "\n",
    "\n",
    "\n",
    "    return indices_to_remove\n",
    "\n",
    "def phrase_heads_avg_vector(phrase_set):\n",
    "    phrase_head_vectors = []\n",
    "    for phrase_head in phrase_set:    \n",
    "        try:\n",
    "            phrase_head_vectors.append(model[phrase_head])\n",
    "        except KeyError:\n",
    "            phrase_head_vectors.append(np.NaN)\n",
    "    #phrase_head_vectors = [model[phrase_head] for phrase_head in phrase_set]\n",
    "    if len(phrase_head_vectors) != 0:\n",
    "        return np.mean(phrase_head_vectors,axis=0)\n",
    "    else: \n",
    "        return np.NaN\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(event_cands_merged['cand_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in cand_df['candidates']:\n",
    "    print(cand[1], cand[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third merging step representative labeling\n",
    "\n",
    "currently working on average cosine similarity of each phrase in the candidate - maybe not optimal, maybe it will be better with a different threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def merging_step3(cand_df):\n",
    "    phrases = []\n",
    "    indices_to_remove = set()\n",
    "    # 1. first we find adj-nn phrases within the candidate\n",
    "    for candidate in cand_df['cand_tags']:  \n",
    "        #the head of noun phrase is marked with value 0 for the word.head\n",
    "        cand_heads_pos = [(word.text, word.head, word.xpos) for word in candidate.words]\n",
    "        #np_pos_tags = {word.text: word.xpos for sent in doc.sentences for word in sent.words}\n",
    "        #print(np_heads_pos)\n",
    "        cand_labeling_phrases = []\n",
    "        for word, head, pos in cand_heads_pos:\n",
    "            #head-1 because the pointer to head does not use 0 index\n",
    "            if (pos == 'JJ' or pos=='VBN') and 'NN' in cand_heads_pos[head-1][2]:\n",
    "                cand_labeling_phrases.append(f'{word}_{cand_heads_pos[head-1][0]}')\n",
    "        phrases.append(cand_labeling_phrases)\n",
    "    \n",
    "    candidate_list = cand_df['candidates']\n",
    "    # 2. we compare the similarities of candidates' phrases\n",
    "    for up_cand_id in range(len(candidate_list)):     \n",
    "        up_cand = candidate_list[up_cand_id]\n",
    "        up_cand_vectors = phrases_vectors(phrases[up_cand_id])\n",
    "        if len(up_cand_vectors)==0:\n",
    "            pass\n",
    "        else:\n",
    "            for low_cand_id in range(up_cand_id+1,len(candidate_list)): \n",
    "                low_cand = candidate_list[low_cand_id]\n",
    "                low_cand_vectors = phrases_vectors(phrases[low_cand_id])\n",
    "                if len(low_cand_vectors)==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    sim_matrix = np.zeros((len(up_cand_vectors),len(low_cand_vectors)))\n",
    "                    #print(sim_matrix)\n",
    "                    for i in range(len(up_cand_vectors)):\n",
    "                        for j in range(len(low_cand_vectors)):\n",
    "\n",
    "                            sim_matrix[i][j] = 1-cosine(up_cand_vectors[i],low_cand_vectors[j])\n",
    "\n",
    "                    # can we compute matrix mean like this? \n",
    "                    #print(sim_matrix)\n",
    "                    if np.mean(sim_matrix) > 0.5:\n",
    "                        #print(f'{longer_cand} and {cand} are {numpy.mean(sim_matrix)} similar' )\n",
    "                        indices_to_remove.add(low_cand_id)\n",
    "                        what_merged3[up_cand[0].lower()].append(low_cand[0].lower())\n",
    "                    #else:\n",
    "                        #print(f'{numpy.mean(sim_matrix)} is not similar' )\n",
    "                    \n",
    "    return indices_to_remove\n",
    "                \n",
    "\n",
    "\n",
    "def phrases_vectors(cand_phrases):\n",
    "    \n",
    "#for cand_phrases in phrases:\n",
    "    #print(cand_phrases)\n",
    "    cand_phrase_vectors = []\n",
    "    for phrase in cand_phrases:\n",
    "        try:\n",
    "            cand_phrase_vectors.append(model[phrase])\n",
    "            #print(f'for existing phrase \"{phrase}\" the vector is {model[phrase][0]}')\n",
    "        except KeyError:\n",
    "            phrase_words = phrase.split('_')\n",
    "            #print(model[phrase_words[1]])\n",
    "            try:\n",
    "                phrase_vectors = [model[phrase_word] for phrase_word in phrase_words]\n",
    "                #print(f'for phrase \"{phrase}\" avg vector is \"{sum(phrase_vectors)/len(phrase_vectors)}') \n",
    "                cand_phrase_vectors.append(sum(phrase_vectors)/len(phrase_vectors))\n",
    "            except KeyError:\n",
    "                cand_phrase_vectors.append(np.NaN)\n",
    "    #print(len(cand_phrase_vectors))\n",
    "    return cand_phrase_vectors\n",
    "    \n",
    " \n",
    "event_cands_merged = merge_indices(event_cands_merged, merging_step3(event_cands_merged))\n",
    "#print(indices_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_merged3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing the second method - we check for the lexical identity of specific stems in multiple candidates.\n",
    "\n",
    "def merging_step4(cand_df):\n",
    "    phrases = []\n",
    "    indices_to_remove = set()\n",
    "    # 1. first we find adj-nn phrases within the candidate\n",
    "    for candidate in cand_df['cand_tags']:\n",
    "\n",
    "        #the head of noun phrase is marked with value 0 for the word.head\n",
    "        cand_heads_pos = [(word.text, word.head, word.xpos) for word in candidate.words]\n",
    "\n",
    "        #print(np_heads_pos)\n",
    "        cand_compound_phrases = []\n",
    "        for word, head, pos in cand_heads_pos:\n",
    "            #i = np_heads_pos.index((word, head, pos))\n",
    "            #print(np_heads_pos)\n",
    "            #print(np_heads_pos[i])\n",
    "            #print(np_heads_pos[head-1])\n",
    "            #'NN' in np_heads_pos[head-1][2] and\n",
    "            try:\n",
    "                #if 'NN' in pos and 'NN' in cand_heads_pos[i+1][2] : \n",
    "                    #cand_compound_phrases.append(f'{word}_{cand_heads_pos[i+1][0]}')\n",
    "                if 'NN' in pos and 'NN' in cand_heads_pos[head-1][2]:\n",
    "                    cand_compound_phrases.append(f'{word}_{cand_heads_pos[head-1][0]}')\n",
    "            except IndexError:\n",
    "                pass\n",
    "        phrases.append(cand_compound_phrases)\n",
    "    \n",
    "    candidate_list = cand_df['candidates']\n",
    "    # 2. we compare the similarities of candidates' phrases\n",
    "    for up_cand_id in range(len(candidate_list)):     \n",
    "        up_cand = candidate_list[up_cand_id]\n",
    "        up_cand_vectors = phrases_vectors(phrases[up_cand_id])\n",
    "        if len(up_cand_vectors)==0:\n",
    "            pass\n",
    "        else:\n",
    "            for low_cand_id in range(up_cand_id+1,len(candidate_list)):\n",
    "                low_cand = candidate_list[low_cand_id]\n",
    "                low_cand_vectors = phrases_vectors(phrases[low_cand_id])\n",
    "                if len(low_cand_vectors)==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    sim_matrix = np.zeros((len(up_cand_vectors),len(low_cand_vectors)))\n",
    "                    #print(sim_matrix)\n",
    "                    for i in range(len(up_cand_vectors)):\n",
    "                        for j in range(len(low_cand_vectors)):\n",
    "                            #print(cosine_similarity(long_cand_vectors[i].reshape(1,-1),short_cand_vectors[j].reshape(1,-1)))\n",
    "                            sim_matrix[i][j] = 1-cosine(up_cand_vectors[i],low_cand_vectors[j])\n",
    "                            \"\"\"if cosine_similarity(long_cand_vectors[i].reshape(1,-1),short_cand_vectors[j].reshape(1,-1)) > 0.4:                \n",
    "                                sim_matrix[i][j] = 2\n",
    "                            elif cosine_similarity(long_cand_vectors[i].reshape(1,-1),short_cand_vectors[j].reshape(1,-1)) > 0.2:\n",
    "                                sim_matrix[i][j] = 1\n",
    "                            else:\n",
    "                                sim_matrix[i][j] = 0\"\"\"\n",
    "\n",
    "                    #print(sim_matrix, up_cand,low_cand)            \n",
    "                    if np.mean(sim_matrix) > 0.6:\n",
    "                        print(f'{up_cand_id} and {low_cand_id} are {np.mean(sim_matrix)} similar' )\n",
    "                        indices_to_remove.add(low_cand_id)\n",
    "                        what_merged4[up_cand[0].lower()].append(low_cand[0].lower())\n",
    "                    #else:\n",
    "                        #print(f'{numpy.mean(sim_matrix)} is not similar' )\n",
    "                    \n",
    "    return indices_to_remove\n",
    "\n",
    "\n",
    "event_cands_merged = merge_indices(event_cands_merged, merging_step4(event_cands_merged))\n",
    "#print(merging_step4(candidate_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging step 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_merged1,what_merged2,what_merged3,what_merged4 = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "event_cands = beirut_cands\n",
    "\n",
    "event_cands_merged = merge_indices(event_cands, merging_step1(event_cands['candidates']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands_merged = merge_indices(event_cands_merged, merging_step2(event_cands_merged['candidates']))\n",
    "\n",
    "what_merged2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands_merged = merge_indices(event_cands_merged, merging_step3(event_cands_merged))\n",
    "\n",
    "what_merged3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands_merged = merge_indices(event_cands_merged, merging_step4(event_cands_merged))\n",
    "what_merged4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_files('beirut_cands_merged',event_cands_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"frame_properties = {'affection':['affection','attachment', 'devotion', 'fondness','love','passion'],\n",
    "                    'refusal': ['refusal','declination','denial','disallowance','nay','no'],\n",
    "                    'trustworthiness':['trustworthiness','integrity','accuracy','credibility','authenticity','fairness'],\n",
    "                    'no trustworthiness':['falsehood','dishonesty','unfairness','deceit','corruption'],\n",
    "                    'reason': ['reason','logic','sense','rationale','argument','justification'],\n",
    "                    'unreason/irrationality': ['unreason','irrationality','fallaciousness','unsoundness'],\n",
    "                    'easiness': ['easiness','simplicity','obviousness','ease','comfort'],\n",
    "                    'difficulty': ['difficulty','adversity','hardship','crisis','obstacle','trouble' ],\n",
    "                    'honor': ['honor', 'dignity','esteem','reputation','praise'],\n",
    "                    'dishonor': ['disgrace','dishonor','reproach','opprobrium']}\"\"\" #from Hamborg's paper\n",
    "\n",
    "# from paper Shifting the refugee narratives? by Greussing & Boomgaarden (2015)\n",
    "frame_properties = {'settlement':['settlement','accomodation','permanent','temporary','barracks','accommodated','tent','camp', 'shelter'],\n",
    "                   'reception':['quota', 'distribution', 'limit', 'selection','reception','together','asylum','receive'],\n",
    "                    'security':['security', 'border','crossing','fence','control','flow'],\n",
    "                    'criminality':['officer','terror','suspicion','crime','offense','police','trafficking','suspect'],\n",
    "                    'economisation':['euro','economic','million','thousand','cost','money'],\n",
    "                    'humanitarian':['humane','voluntary','help','support','aid','care','solidarity'],\n",
    "                    'victimization':['islamic','fight','victim','war','dead','rescued','state'],\n",
    "                    'integration': ['labour','employed','unemployed','integration','positive'],\n",
    "                    \n",
    "                    #from hamborg\n",
    "                    'affection':['affection','attachment', 'devotion', 'fondness','love','passion'],\n",
    "                    'refusal': ['refusal','declination','denial','disallowance','nay','no'],\n",
    "                    'trustworthiness':['trustworthiness','integrity','accuracy','credibility','authenticity','fairness'],\n",
    "                    'no trustworthiness':['falsehood','dishonesty','unfairness','deceit','corruption'],\n",
    "                    'reason': ['reason','logic','sense','rationale','argument','justification'],\n",
    "                    'unreason/irrationality': ['unreason','irrationality','fallaciousness','unsoundness'],\n",
    "                    'easiness': ['easiness','simplicity','obviousness','ease','comfort'],\n",
    "                    'difficulty': ['difficulty','adversity','hardship','crisis','obstacle','trouble' ],\n",
    "                    'honor': ['honor', 'dignity','esteem','reputation','praise'],\n",
    "                    'dishonor': ['disgrace','dishonor','reproach','opprobrium']\n",
    "                   \n",
    "                   }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import conceptnet_lite as cn\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(r\"C:/Users/niol19ac/Dropbox (CBS)/Master thesis data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "manual_cands = ['refugee','migrant','greece','turkey','syria','beirut','immigrant','aoun']\n",
    "\n",
    "\n",
    "# to run on the server we should use larger model according to the paper - \"conceptnet-numberbatch-17-06-300\"\n",
    "model = api.load(\"glove-twitter-200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "#print('preprocessing tweets...')\n",
    "#tweets_corpus = list(preprocessing.preprocess_tweets(event_df['text']))\n",
    "\n",
    "\n",
    "print('assigning frame properties to words from tweets...')\n",
    "word_properties = defaultdict(dict)\n",
    "for i in tqdm(tweets_corpus):\n",
    "    tweet_words = [word.lower() for word in i.split() if word not in stop_words and len(word)>1]\n",
    "    for word in tweet_words:\n",
    "        #print(word)\n",
    "        word = lemma.lemmatize(word)\n",
    "        property_list = []\n",
    "        #print(list(frame_properties.keys()))\n",
    "        for prop in list(frame_properties.keys()):\n",
    "            #print(frame_properties[prop])\n",
    "            \n",
    "            try:\n",
    "                #print(f'sim of {word}, {prop} is {model.similarity(word, prop)}')\n",
    "                weights = [model.similarity(word, seed) for seed in frame_properties[prop]]\n",
    "                #print(weights)\n",
    "                if max(weights)>0.4:\n",
    "                    word_properties[word][prop] = max(weights)\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "print(word_properties)\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sampled_df series should be converted to list and sentences separated with \"\\n\\n\"\n",
    "all_tweets_list = list(tweets_corpus)[:200] \n",
    "\n",
    "#all_tweets_list = all_tweets_list + ['Muslim refugees is government']\n",
    "\n",
    "# IF WE DON'T CARE ABOUT (KINDA) LOSING THE TWEETS (BECAUSE WE WILL LOOK AT SENTENCES ONLY) THEN WE CAN BATCH WITH \\n\\n AND\n",
    "# SPEED THINGS UP SIGNIFICANTLY\n",
    "\n",
    "#all_tweets_list = '\\n\\n'.join(all_tweets_list)\n",
    "#print(all_tweets_list[:1000])\n",
    "\"\"\"for tweet in tqdm(range(len(all_tweets_list))):\n",
    "    tweet_sentokenized = sent_tokenize(all_tweets_list[tweet])\n",
    "    if len(tweet_sentokenized) == 0:\n",
    "        tweet_sentokenized.append('empty_tweet')\n",
    "        #print(f'empty tweet at index {tweet}')\n",
    "    all_tweets_list[tweet] = \"\\n\\n\".join(tweet_sentokenized)\"\"\"\n",
    "\n",
    "\"\"\"all_tweets_list = '\\n\\n'.join(all_tweets_list)  \n",
    "\n",
    "for tweet_id in range(len(all_tweets_list.split('\\n\\n'))):\n",
    "    if len(all_tweets_list[tweet_id]) == 0:\n",
    "        all_tweets_list[tweet_id] = 'empty_tweet'\n",
    "#print(all_tweets_list[:1000]\"\"\"\n",
    "\n",
    "\n",
    "#tag all tweets and save them in a list    \n",
    "tagged_tweets = [] \n",
    "for tweet in tqdm(batch(all_tweets_list, en_nlp, batch_size=2000)): # Default batch size is 32\n",
    "        tagged_tweets.append(tweet)\n",
    "\n",
    "# the tweet text can now be accessed using .text method        \n",
    "tagged_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tweets = load_pickle('beirut_tagged_tweets')\n",
    "coref_chains = load_pickle('beirut_crf_list')\n",
    "\n",
    "coref_chains[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_ps, coref_chains, tagged_tweets = cand_ex.candidate_identification(event_df[:20], en_nlp, 4096)\n",
    "\n",
    "#for tweet_id in tqdm(range(len(tagged_tweets[:50]))):\n",
    "#    print(coref_chains[tweet_id])\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus.reader.wordnet import NOUN\n",
    "import numpy as np\n",
    "  \n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "cand_frames = defaultdict(list)\n",
    "\n",
    "framed_words = pd.DataFrame(columns=['word','date',list(frame_properties.keys())])\n",
    "\n",
    "event_df[['date','time']] = event_df['created_at'].str.split(' ',expand=True)\n",
    "\n",
    "\n",
    "for tweet_id in tqdm(range(len(tagged_tweets))):\n",
    "    #print(tweet)\n",
    "    cand_words = [[word.id, word.text,word.head] for sent in tagged_tweets[tweet_id].sentences for word in sent.words]\n",
    "    #print(*[f'id: {word.id}\\tword: {word.text:<15}head id: {word.head:<5}head: {sent.words[word.head-1].text if word.head > 0 else \"root\":<10}deprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "    #print(len(cand_df['candidates']))\n",
    "    for cand in manual_cands:\n",
    "            # lemmatize representative head of candidate \n",
    "            rep_head = cand #lemma.lemmatize(cand[1].lower(),pos=NOUN)\n",
    "            \n",
    "            if rep_head in tagged_tweets[tweet_id].text.lower() and len(rep_head)>1:\n",
    "                #find all dependencies of the phrase head\n",
    "                for related in range(len(cand_words)):\n",
    "                    cand_word_lemma = lemma.lemmatize(cand_words[related][1].lower())\n",
    "                    #print(f'Yes it is, related = {lemma.lemmatize(cand_heads[related][1].lower(),pos=NOUN)}')\n",
    "                    #lemma.lemmatize(cand_heads[related][1].lower(),pos=NOUN)\n",
    "                    if rep_head == cand_word_lemma:\n",
    "                        related_word = lemma.lemmatize(cand_words[cand_words[related][2]-1][1].lower())\n",
    "                        cand_frames['word'].append(rep_head)\n",
    "                        cand_frames['date'].append(event_df['date'][tweet_id])\n",
    "                        #print(f'\\n\\n checking {phrase_head}_{related_word}')                \n",
    "                        #cand_frames['word'].append(rep_head)\n",
    "                        #cand_frames['date'].append(event_df['Date Short'][200+tweet_id])\n",
    "                        #cand_frames['word'].append(phrase_head)\n",
    "                        for frame_property in list(frame_properties.keys()):\n",
    "                            #print(frame_property)\n",
    "\n",
    "                            try:\n",
    "                                #print(word_properties[phrase_head][frame_property])\n",
    "                                cand_frames[frame_property].append(word_properties[related_word][frame_property])\n",
    "\n",
    "                            except KeyError:\n",
    "                                #print('Error')\n",
    "                                #cand_frames[frame_property].append(word_properties['tent'][frame_property])\n",
    "                                cand_frames[frame_property].append(np.NaN)\n",
    "                                \n",
    "\n",
    "                    #print(len(cand_frames[frame_property]))\n",
    "                    \n",
    "                #print('\\n')\n",
    "                \n",
    "                \n",
    "                \"\"\"for frame_property in list(frame_properties.keys()):\n",
    "                    for seed_word in frame_properties[frame_property]:\n",
    "                        try:\n",
    "                            for related in range(len(np_heads)):\n",
    "                                #print(np_heads[related])\n",
    "                                #if cand[1] == np_heads[related][1]:\n",
    "                                if phrase_head == np_heads[related][1]:\n",
    "                                    #print(f'checking {seed_word} and {phrase_head}_{np_heads[np_heads[related][2]-1][1]}')\n",
    "                                    cand_frames[seed_word][phrase_head].append(model.similarity(seed_word,np_heads[np_heads[related][2]-1][1]))\n",
    "                            #[cand_frames[seed_word][cand].append(model.similarity(seed_word,np_heads[np_heads[related][2]+1][1])) if cand == np_heads[related][1] else print('') for related in range(len(np_heads))]\n",
    "                        except KeyError:\n",
    "                            pass\"\"\"\n",
    "                        #[cand_frames[seed_word][cand].append(model.similarity(print(f'{cand} is related to {np_heads[np_heads[related][2]+1][1]}') if cand == np_heads[related][1] else print('nej') for related in range(len(np_heads))]\n",
    "            #print(get_head(cand))\n",
    "            #print(np_heads[19][1])\n",
    "            #[f(x) if condition else g(x) for x in sequence]\n",
    "            #[print(np_heads[np_heads[related][2]-1]) if get_head(cand)==np_heads[related][1] else print('hi') for related in range(len(np_heads))]\n",
    "\n",
    "            \n",
    "#became ___ (vb and vbx)\n",
    "#(VP sit/VB (PP on/IN (NP the/DT mat/NN))))) \n",
    "\n",
    "#common phrases = migrant camp, covid case, covid test\n",
    "\n",
    "#cand_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cand_frames.keys())\n",
    "\n",
    "framed_words = pd.DataFrame.from_dict(cand_frames)\n",
    "\n",
    "#framed_words[framed_words['word']=='migrants'].tail(50)\n",
    "\n",
    "#framed_words = framed_words.dropna(subset=['settlement', 'reception', 'security', 'criminality', 'economisation', 'humanitarian', 'victimization', 'integration', 'affection', 'refusal', 'trustworthiness', 'no trustworthiness', 'reason', 'unreason/irrationality', 'easiness', 'difficulty', 'honor', 'dishonor'],how='all')\n",
    "\n",
    "framed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framed_words = framed_words[framed_words['word'] == 'refugee']\n",
    "#framed_words[framed_words['date']=='2020-09-04']\n",
    "\n",
    "aggr_frames = framed_words.groupby(['word','date'],as_index=False).size()\n",
    "aggr_frames[64:].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.line(aggr_frames, x=\"date\", y=[\"reason\",'affection','reception','settlement'], title=f'Frame bias towards refugees')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching the tweets speeds the model considerably and is enabled by splitting sentences using '\\n\\n' \n",
    "from stanza_batch import batch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# the sampled_df series should be converted to list and sentences separated with \"\\n\\n\"\n",
    "all_tweets_list = list(tweets_corpus)[:50] \n",
    "for tweet in range(len(all_tweets_list)):\n",
    "    tweet_sentokenized = sent_tokenize(all_tweets_list[tweet])\n",
    "    if tweet_sentokenized == []:\n",
    "        tweet_sentokenized.append('empty_tweet')\n",
    "        print(f'empty tweet at index {tweet}')\n",
    "    all_tweets_list[tweet] = \"\\n\\n\".join(tweet_sentokenized)\n",
    "\n",
    "\n",
    "#tag all tweets and save them in a list    \n",
    "tagged_tweets = [] \n",
    "for tweet in tqdm(batch(all_tweets_list, en_nlp, batch_size=1000)): # Default batch size is 32\n",
    "        tagged_tweets.append(tweet)\n",
    "\n",
    "# the tweet text can now be accessed using .text method        \n",
    "tagged_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for tweet in tqdm(range(len(tweets_corpus))):\n",
    "    print(tweets_corpus[tweet])\n",
    "    np_heads = [[word.id, word.text,word.head,word.deprel] for sent in tagged_tweets[tweet].sentences for word in sent.words]\n",
    "    print(*[f'id: {word.id}\\tword: {word.text:<15}head id: {word.head:<5}head: {sent.words[word.head-1].text if word.head > 0 else \"root\":<10}deprel: {word.deprel}' for sent in tagged_tweets[tweet].sentences for word in sent.words], sep='\\n')\n",
    "    #print(np_heads)\n",
    "    ph_ids = set([np_heads[i][2] for i in range(len(np_heads))])\n",
    "    ph_words = [np_heads[i-1][1] for i in ph_ids]\n",
    "\n",
    "    word_pairs = [(np_heads[word][1], np_heads[np_heads[word][2]-1][1]) for word in range(len(np_heads)) if np_heads[word][2] != 0]\n",
    "    #print(word_pairs)\n",
    "    \n",
    "    compounds = [[np_heads[i][1]+'_'+np_heads[np_heads[i][2]-1][1]] for i in range(len(np_heads)) if 'compound' in np_heads[i][3]]\n",
    "    print(compounds)\n",
    "    \n",
    "    advmods = [[np_heads[i][1]+'_'+np_heads[np_heads[i][2]-1][1]] for i in range(len(np_heads)) if np_heads[i][3]=='advmod']\n",
    "    print(advmods)\n",
    "    \n",
    "    amods = [[np_heads[i][1]+'_'+np_heads[np_heads[i][2]-1][1]] for i in range(len(np_heads)) if np_heads[i][3]=='amod']\n",
    "    print(amods)\n",
    "    for pair in word_pairs:\n",
    "        phrase = pair[0]+'_'+pair[1]\n",
    "\n",
    "    #print(model.most_similar('illegal_immigrant'))\n",
    "\n",
    "    \n",
    "    \"\"\"#print(len(cand_df['candidates']))\n",
    "    candidate_list = cand_df['candidates']\n",
    "    for cand in cand_df['candidates']:\n",
    "        #print(cand[2])\n",
    "        #print(get_head(str(cand)))\n",
    "        for phrase_head in cand[2]:\n",
    "            #print(phrase_head)\n",
    "            #if str(cand[1]) in str(tweet):\n",
    "            if str(phrase_head) in str(tweet) and len(phrase_head)>2:\n",
    "                #print(phrase_head)\n",
    "                ph_words = [np_heads[i-1][1] for i in phrase_heads]\n",
    "                #print(ph_words)\n",
    "                for related in range(len(np_heads)):\n",
    "                    if phrase_head == np_heads[related][1]:\n",
    "                        pass\n",
    "                        #print(f'checking {phrase_head}_{np_heads[np_heads[related][2]-1][1]}')\n",
    "                  for frame_property in list(frame_properties.keys()):\n",
    "                        for seed_word in frame_properties[frame_property]:\n",
    "                        try:\n",
    "                            for related in range(len(np_heads)):\n",
    "                                #print(np_heads[related])\n",
    "                                #if cand[1] == np_heads[related][1]:\n",
    "                                if phrase_head == np_heads[related][1]:\n",
    "                                    print(f'checking {seed_word} and {phrase_head}_{np_heads[np_heads[related][2]-1][1]}')\n",
    "                                    #cand_frames[seed_word][phrase_head].append(model.similarity(seed_word,np_heads[np_heads[related][2]-1][1]))\n",
    "                                #[cand_frames[seed_word][cand].append(model.similarity(seed_word,np_heads[np_heads[related][2]+1][1])) if cand == np_heads[related][1] else print('') for related in range(len(np_heads))]\n",
    "                                except KeyError:\n",
    "                                pass\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
