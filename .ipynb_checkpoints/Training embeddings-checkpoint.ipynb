{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate merging and related preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages for the following parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# self written modules\n",
    "import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                           | 281/78450 [00:00<00:55, 1417.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 78450 tweets!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 78450/78450 [00:51<00:00, 1532.01it/s]\n"
     ]
    }
   ],
   "source": [
    "tigray_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_tigray.csv\" # location of Tigray dataset\n",
    "greece_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_greece.csv\" # location of Greece dataset\n",
    "rohingya_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_rohingya.csv\" # location of Rohingya dataset\n",
    "all_url = r\"Dropbox (CBS)/Master thesis data/df_tweets.csv\" # for all tweets\n",
    "\n",
    "def read_event_df(data_url):\n",
    "    directory_path = os.getcwd() + \"/../../../\" + data_url \n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df(tigray_url)\n",
    "#channel_df = read_event_df(channel_url)\n",
    "tqdm.pandas()\n",
    "event_df['text_clean'] = event_df['text'].progress_apply(preprocessing.preprocess_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading users dataframe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 78450/78450 [00:01<00:00, 78449.53it/s]\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = os.getcwd() + \"/../../Dropbox (CBS)/Master thesis data\"\n",
    "USERS_PATH = FILE_PATH + \"/df_users.csv\"\n",
    "# Read the users csv\n",
    "\n",
    "print(\"loading users dataframe...\")\n",
    "df_users = pd.read_csv(USERS_PATH)\n",
    "\n",
    "# Drop unnecessary index column\n",
    "df_users.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "df_users.head()\n",
    "\n",
    "# Create dict that maps usernames to actual names\n",
    "mapping = dict(df_users[[\"username\",\"name\"]].values)\n",
    "mapping = {f'@{key}': value for key, value in mapping.items()}\n",
    "\n",
    "\n",
    "def resolve_username_to_name(text):\n",
    "    new_text = text\n",
    "    for word in text.split(\" \"):\n",
    "        if word in mapping:\n",
    "            new_text = new_text.replace(word,mapping[word])\n",
    "    return new_text\n",
    "\n",
    "#tqdm.pandas()\n",
    "event_df['text_clean'] = event_df['text_clean'].progress_apply(resolve_username_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our own event-specific Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 118696/118696 [00:01<00:00, 62818.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets at the start: 118696\n",
      "Tweets after 100% duplicates removed: 91787\n",
      "calculating similarities across documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3636it [00:00, 36018.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity calculation completed in 724.0583999156952 seconds\n",
      "removing fuzzy duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167311it [00:04, 36867.00it/s]\n",
      "C:\\Users\\nikodemicek\\Documents\\GitHub\\refugee_project\\preprocessing.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dupl_removed['is_dup'][i] = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81327 tweets left after 70.0% similar tweets (by cosine similarity) removed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>calendar_week</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year_calendar_week</th>\n",
       "      <th>refugee</th>\n",
       "      <th>migrant</th>\n",
       "      <th>immigrant</th>\n",
       "      <th>asylum_seeker</th>\n",
       "      <th>other</th>\n",
       "      <th>is_dup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saperduper_robots</td>\n",
       "      <td>Migrants clash with Greek police as they exit ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1233904804134555649</td>\n",
       "      <td>2020-03-01 00:00:04+00:00</td>\n",
       "      <td>108944513</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_08</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Loomly</td>\n",
       "      <td>NATO urges Syria, Russia to halt airstrikes as...</td>\n",
       "      <td>en</td>\n",
       "      <td>1233904855774683136</td>\n",
       "      <td>2020-03-01 00:00:17+00:00</td>\n",
       "      <td>4717892303</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_08</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Good on Greece!!....Close all OUR Borders!!......</td>\n",
       "      <td>en</td>\n",
       "      <td>1233904867078393856</td>\n",
       "      <td>2020-03-01 00:00:19+00:00</td>\n",
       "      <td>1067006507189886976</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hootsuite Inc.</td>\n",
       "      <td>Increased risk of #MaternalDeath in immigrants...</td>\n",
       "      <td>en</td>\n",
       "      <td>1233904894425419776</td>\n",
       "      <td>2020-03-01 00:00:26+00:00</td>\n",
       "      <td>3397107171</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@OhGodPlsNOO @ilknurdarendeli @AFP Europe has ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1233904905108283393</td>\n",
       "      <td>2020-03-01 00:00:28+00:00</td>\n",
       "      <td>1188172981530435584</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_08</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118688</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@AngeloDDuca @GiuseppeConteIT @IslamSeries @Mi...</td>\n",
       "      <td>en</td>\n",
       "      <td>1241511809389137920</td>\n",
       "      <td>2020-03-21 23:47:36+00:00</td>\n",
       "      <td>738486583725305857</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_11</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118689</th>\n",
       "      <td>dlvr.it</td>\n",
       "      <td>Coronavirus risks taking heavy toll on migrant...</td>\n",
       "      <td>en</td>\n",
       "      <td>1241512682513002497</td>\n",
       "      <td>2020-03-21 23:51:04+00:00</td>\n",
       "      <td>2985565615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118691</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>@ConanOBrien @TheEllenShow @IngrahamAngle @Nob...</td>\n",
       "      <td>en</td>\n",
       "      <td>1241513000147664896</td>\n",
       "      <td>2020-03-21 23:52:20+00:00</td>\n",
       "      <td>309530329</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118694</th>\n",
       "      <td>Tweepsmap</td>\n",
       "      <td>RT @WRRoute\\n\\nNatasha Dailiani calls upon med...</td>\n",
       "      <td>en</td>\n",
       "      <td>1241514169532321793</td>\n",
       "      <td>2020-03-21 23:56:58+00:00</td>\n",
       "      <td>62632306</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118695</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>@CBlighty @AlynSmith Huge difference. If comin...</td>\n",
       "      <td>en</td>\n",
       "      <td>1241514217905291266</td>\n",
       "      <td>2020-03-21 23:57:10+00:00</td>\n",
       "      <td>1016605258448080898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2020_3</td>\n",
       "      <td>2020_11</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81327 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     source  \\\n",
       "0         saperduper_robots   \n",
       "1                    Loomly   \n",
       "2           Twitter Web App   \n",
       "3            Hootsuite Inc.   \n",
       "4        Twitter for iPhone   \n",
       "...                     ...   \n",
       "118688  Twitter for Android   \n",
       "118689              dlvr.it   \n",
       "118691      Twitter Web App   \n",
       "118694            Tweepsmap   \n",
       "118695      Twitter Web App   \n",
       "\n",
       "                                                     text lang  \\\n",
       "0       Migrants clash with Greek police as they exit ...   en   \n",
       "1       NATO urges Syria, Russia to halt airstrikes as...   en   \n",
       "2       Good on Greece!!....Close all OUR Borders!!......   en   \n",
       "3       Increased risk of #MaternalDeath in immigrants...   en   \n",
       "4       @OhGodPlsNOO @ilknurdarendeli @AFP Europe has ...   en   \n",
       "...                                                   ...  ...   \n",
       "118688  @AngeloDDuca @GiuseppeConteIT @IslamSeries @Mi...   en   \n",
       "118689  Coronavirus risks taking heavy toll on migrant...   en   \n",
       "118691  @ConanOBrien @TheEllenShow @IngrahamAngle @Nob...   en   \n",
       "118694  RT @WRRoute\\n\\nNatasha Dailiani calls upon med...   en   \n",
       "118695  @CBlighty @AlynSmith Huge difference. If comin...   en   \n",
       "\n",
       "                         id                 created_at            author_id  \\\n",
       "0       1233904804134555649  2020-03-01 00:00:04+00:00            108944513   \n",
       "1       1233904855774683136  2020-03-01 00:00:17+00:00           4717892303   \n",
       "2       1233904867078393856  2020-03-01 00:00:19+00:00  1067006507189886976   \n",
       "3       1233904894425419776  2020-03-01 00:00:26+00:00           3397107171   \n",
       "4       1233904905108283393  2020-03-01 00:00:28+00:00  1188172981530435584   \n",
       "...                     ...                        ...                  ...   \n",
       "118688  1241511809389137920  2020-03-21 23:47:36+00:00   738486583725305857   \n",
       "118689  1241512682513002497  2020-03-21 23:51:04+00:00           2985565615   \n",
       "118691  1241513000147664896  2020-03-21 23:52:20+00:00            309530329   \n",
       "118694  1241514169532321793  2020-03-21 23:56:58+00:00             62632306   \n",
       "118695  1241514217905291266  2020-03-21 23:57:10+00:00  1016605258448080898   \n",
       "\n",
       "        retweet_count  reply_count  like_count  quote_count  ...  year  \\\n",
       "0                   3            0           1            1  ...  2020   \n",
       "1                  12            5          14            3  ...  2020   \n",
       "2                   2            1           4            0  ...  2020   \n",
       "3                   1            0           2            0  ...  2020   \n",
       "4                   0            0           0            1  ...  2020   \n",
       "...               ...          ...         ...          ...  ...   ...   \n",
       "118688              2            0           0            0  ...  2020   \n",
       "118689              0            0           0            0  ...  2020   \n",
       "118691              0            2           1            0  ...  2020   \n",
       "118694              3            0           1            0  ...  2020   \n",
       "118695              0            1           5            0  ...  2020   \n",
       "\n",
       "       calendar_week year_month year_calendar_week refugee  migrant  \\\n",
       "0                  8     2020_3            2020_08   False     True   \n",
       "1                  8     2020_3            2020_08   False     True   \n",
       "2                  8     2020_3            2020_08   False    False   \n",
       "3                  8     2020_3            2020_08   False    False   \n",
       "4                  8     2020_3            2020_08    True    False   \n",
       "...              ...        ...                ...     ...      ...   \n",
       "118688            11     2020_3            2020_11    True    False   \n",
       "118689            11     2020_3            2020_11   False     True   \n",
       "118691            11     2020_3            2020_11   False    False   \n",
       "118694            11     2020_3            2020_11   False    False   \n",
       "118695            11     2020_3            2020_11    True    False   \n",
       "\n",
       "        immigrant asylum_seeker  other  is_dup  \n",
       "0           False         False  False   False  \n",
       "1           False         False  False   False  \n",
       "2           False         False  False   False  \n",
       "3            True         False  False   False  \n",
       "4           False         False  False   False  \n",
       "...           ...           ...    ...     ...  \n",
       "118688      False         False  False   False  \n",
       "118689      False         False  False   False  \n",
       "118691      False         False  False   False  \n",
       "118694      False         False  False   False  \n",
       "118695      False         False  False   False  \n",
       "\n",
       "[81327 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fuzzy duplicate removal (removes 100% duplicates before expensive operations) \n",
    "#done on dataframe level, we want to keep the ID column to match later on\n",
    "\n",
    "# the comparison will be done on lowercased texts consisting of only letters, digits and spaces\n",
    "event_df['text_clean'] = event_df['text_clean'].progress_apply(lambda tweet:re.sub(r'[^A-Za-z0-9 ]+', '', tweet.lower()))\n",
    "unique_tweets_df = preprocessing.fuzzy_duplicate_removal(event_df)\n",
    "unique_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'migrants_clash': 95,\n",
       " 'greek_police': 1575,\n",
       " 'migrants_clash_with_greek_police': 74,\n",
       " 'as': 11266,\n",
       " 'greek_police_as': 33,\n",
       " 'they': 19847,\n",
       " 'as_they': 467,\n",
       " 'exit': 78,\n",
       " 'they_exit': 2,\n",
       " 'turkey': 34257,\n",
       " 'exit_turkey': 6,\n",
       " 'bbc_news': 372,\n",
       " 'turkey_bbc_news': 6,\n",
       " 'nato': 1526,\n",
       " 'urges': 159,\n",
       " 'nato_urges': 6,\n",
       " 'syria': 10283,\n",
       " 'urges_syria': 6,\n",
       " 'russia': 1989,\n",
       " 'syria_russia': 72,\n",
       " 'halt': 102,\n",
       " 'russia_to_halt': 5,\n",
       " 'airstrikes': 38,\n",
       " 'halt_airstrikes': 5,\n",
       " 'airstrikes_as': 5,\n",
       " 'migrants': 20502,\n",
       " 'as_migrants': 398,\n",
       " 'move': 706,\n",
       " 'migrants_move': 12,\n",
       " 'westward': 7,\n",
       " 'move_westward': 6,\n",
       " 'good': 1292,\n",
       " 'greece': 29042,\n",
       " 'good_on_greece': 5,\n",
       " 'close': 696,\n",
       " 'greece_close': 4,\n",
       " 'all': 8366,\n",
       " 'close_all': 17,\n",
       " 'our': 4225,\n",
       " 'all_our': 39,\n",
       " 'borders': 7033,\n",
       " 'our_borders': 403,\n",
       " 'let': 1653,\n",
       " 'borders_let': 7,\n",
       " 'let_turkey': 44,\n",
       " 'pm': 285,\n",
       " 'turkey_and_pm': 1,\n",
       " 'erdogen': 14,\n",
       " 'pm_erdogen': 3,\n",
       " 'take': 3336,\n",
       " 'erdogen_take': 3,\n",
       " 'take_all': 113,\n",
       " 'these': 4820,\n",
       " 'all_these': 371,\n",
       " 'islamic': 502,\n",
       " 'these_islamic': 15,\n",
       " 'refugees': 43672,\n",
       " 'islamic_refugees': 35,\n",
       " 'there': 6112,\n",
       " 'refugees_there': 174,\n",
       " 'are': 28665,\n",
       " 'there_are': 1549,\n",
       " 'lot': 693,\n",
       " 'are_a_lot': 28,\n",
       " 'nice': 242,\n",
       " 'lot_of_nice': 1,\n",
       " 'safe': 962,\n",
       " 'nice_safe': 1,\n",
       " 'wealthy': 68,\n",
       " 'safe_wealthy': 1,\n",
       " 'wealthy_islamic': 1,\n",
       " 'places': 183,\n",
       " 'islamic_places': 1,\n",
       " 'places_they': 4,\n",
       " 'can': 4606,\n",
       " 'they_can': 621,\n",
       " 'go': 2611,\n",
       " 'can_go': 152,\n",
       " 'make': 1631,\n",
       " 'go_to_make': 1,\n",
       " 'them': 12014,\n",
       " 'make_them': 114,\n",
       " 'them_take': 14,\n",
       " 'take_them': 340,\n",
       " 'not': 17292,\n",
       " 'them_not': 49,\n",
       " 'not_greece': 148,\n",
       " 'any': 2522,\n",
       " 'greece_or_any': 6,\n",
       " 'any_of_our': 4,\n",
       " 'countries': 3328,\n",
       " 'our_countries': 41,\n",
       " 'please': 1159,\n",
       " 'countries_please': 5,\n",
       " 'retweet': 59,\n",
       " 'please_retweet': 13,\n",
       " 'this': 13117,\n",
       " 'retweet_this': 7,\n",
       " 'increased': 110,\n",
       " 'risk': 418,\n",
       " 'increased_risk': 4,\n",
       " 'maternal': 4,\n",
       " 'risk_of_maternal': 1,\n",
       " 'death': 652,\n",
       " 'maternal_death': 1,\n",
       " 'immigrants': 6715,\n",
       " 'death_in_immigrants': 1,\n",
       " 'europe': 18177,\n",
       " 'immigrants_in_europe': 29,\n",
       " 'usa': 636,\n",
       " 'europe_the_usa': 3,\n",
       " 'usa_risk': 1,\n",
       " 'depends': 45,\n",
       " 'risk_depends': 1,\n",
       " 'depends_on_maternal': 1,\n",
       " 'birthplace': 11,\n",
       " 'maternal_birthplace': 1,\n",
       " 'region': 443,\n",
       " 'birthplace_region': 1,\n",
       " 'where': 2752,\n",
       " 'region_where': 11,\n",
       " 'prenatal': 1,\n",
       " 'where_prenatal': 1,\n",
       " 'checkups': 1,\n",
       " 'prenatal_checkups': 1,\n",
       " 'delivery': 19,\n",
       " 'checkups_delivery': 1,\n",
       " 'took': 632,\n",
       " 'delivery_took': 1,\n",
       " 'place': 620,\n",
       " 'took_place': 19,\n",
       " 'other': 1961,\n",
       " 'place_other': 1,\n",
       " 'other_maternal': 1,\n",
       " 'characteristics': 6,\n",
       " 'maternal_characteristics': 1,\n",
       " 'such_as': 177,\n",
       " 'characteristics_such_as': 1,\n",
       " 'age': 116,\n",
       " 'such_as_age': 1,\n",
       " 'has': 7832,\n",
       " 'europe_has': 356,\n",
       " 'three': 296,\n",
       " 'has_three': 2,\n",
       " 'choices': 24,\n",
       " 'three_choices': 1,\n",
       " 'first': 1214,\n",
       " 'choices_first': 1,\n",
       " 'first_take': 8,\n",
       " 'take_refugees': 121,\n",
       " 'second': 294,\n",
       " 'refugees_in_second': 1,\n",
       " 'help': 4154,\n",
       " 'second_help': 1,\n",
       " 'help_turkey': 155,\n",
       " 'create': 597,\n",
       " 'turkey_to_create': 6,\n",
       " 'buffer_zone': 139,\n",
       " 'create_a_buffer_zone': 7,\n",
       " 'buffer_zone_in_syria': 7,\n",
       " 'syria_to_move': 3,\n",
       " 'those': 2554,\n",
       " 'move_those': 1,\n",
       " 'those_refugees': 340,\n",
       " 'into': 4179,\n",
       " 'refugees_into': 296,\n",
       " 'third': 104,\n",
       " 'into_third': 1,\n",
       " 'gave': 407,\n",
       " 'third_gave': 1,\n",
       " 'gave_turkey': 28,\n",
       " 'least': 518,\n",
       " 'turkey_at_least': 3,\n",
       " '20': 344,\n",
       " 'least_20': 3,\n",
       " 'billion': 271,\n",
       " '20_billion': 5,\n",
       " 'usd': 33,\n",
       " 'billion_usd': 13,\n",
       " 'right': 1888,\n",
       " 'usd_right': 1,\n",
       " 'away': 902,\n",
       " 'right_away': 10,\n",
       " 'take_care': 377,\n",
       " 'away_to_take_care': 1,\n",
       " 'take_care_of_those': 4,\n",
       " 'no': 5814,\n",
       " 'are_no': 232,\n",
       " 'no_other': 75,\n",
       " 'other_choices': 1,\n",
       " 'people': 8096,\n",
       " 'choices_people': 1,\n",
       " 'needs': 942,\n",
       " 'people_needs': 2,\n",
       " 'be': 4902,\n",
       " 'needs_to_be': 102,\n",
       " 'realistic': 28,\n",
       " 'be_realistic': 11,\n",
       " 'syrian': 10753,\n",
       " 'syrian_refugees': 5202,\n",
       " 'refugees_are': 1912,\n",
       " 'erdogan': 7658,\n",
       " 'are_erdogan': 19,\n",
       " 's': 9262,\n",
       " 'erdogan_s': 533,\n",
       " 'pawns': 130,\n",
       " 's_pawns': 2,\n",
       " 'gambit': 8,\n",
       " 'pawns_in_gambit': 2,\n",
       " 'thwart': 19,\n",
       " 'gambit_to_thwart': 2,\n",
       " 'assad': 1693,\n",
       " 'thwart_assad': 2,\n",
       " 'austrian_chancellor': 29,\n",
       " 'hints': 7,\n",
       " 'austrian_chancellor_hints': 1,\n",
       " 'closing': 202,\n",
       " 'hints_at_closing': 2,\n",
       " 'closing_borders': 41,\n",
       " 'borders_as': 85,\n",
       " 'police': 1379,\n",
       " 'as_police': 3,\n",
       " 'clash': 93,\n",
       " 'police_clash': 5,\n",
       " 'clash_with_refugees': 5,\n",
       " 'refugees_on_greece': 13,\n",
       " 'greece_turkey': 977,\n",
       " 'border': 14717,\n",
       " 'turkey_border': 553,\n",
       " 'frustrated': 27,\n",
       " 'turkey_frustrated': 3,\n",
       " 'refugee_crisis': 1978,\n",
       " 'frustrated_by_refugee_crisis': 3,\n",
       " 'refugee_crisis_erdogan': 6,\n",
       " 'aide': 12,\n",
       " 'erdogan_aide': 2,\n",
       " 'aide_bbc_news': 1,\n",
       " 'reporters': 87,\n",
       " 'reporters_without_borders': 2,\n",
       " 'condemns': 57,\n",
       " 'borders_condemns': 1,\n",
       " 'greek': 13443,\n",
       " 'condemns_greek': 3,\n",
       " 'public': 235,\n",
       " 'greek_public': 4,\n",
       " 'tv': 209,\n",
       " 'public_tv': 3,\n",
       " 'radio': 66,\n",
       " 'tv_radio': 2,\n",
       " 'broadcaster': 15,\n",
       " 'radio_broadcaster': 1,\n",
       " 'erts': 1,\n",
       " 'broadcaster_erts': 1,\n",
       " 'censorship': 7,\n",
       " 'erts_censorship': 1,\n",
       " 'reporting': 264,\n",
       " 'censorship_of_reporting': 1,\n",
       " 'islands': 390,\n",
       " 'reporting_from_islands': 1,\n",
       " 'islands_where': 3,\n",
       " 'creation': 43,\n",
       " 'where_the_creation': 1,\n",
       " 'new': 2136,\n",
       " 'creation_of_new': 1,\n",
       " 'migrant_camps': 183,\n",
       " 'new_migrant_camps': 10,\n",
       " 'migrant_camps_has': 1,\n",
       " 'triggered': 41,\n",
       " 'has_triggered': 6,\n",
       " 'protests': 93,\n",
       " 'triggered_protests': 1,\n",
       " 'coverage': 68,\n",
       " 'protests_coverage': 1,\n",
       " 'such': 670,\n",
       " 'coverage_of_such': 1,\n",
       " 'crucial': 31,\n",
       " 'such_a_crucial': 1,\n",
       " 'subject': 69,\n",
       " 'crucial_subject': 1,\n",
       " 'clearly': 268,\n",
       " 'subject_clearly': 1,\n",
       " 'serves': 24,\n",
       " 'clearly_serves': 1,\n",
       " 'serves_public': 1,\n",
       " 'interest': 161,\n",
       " 'public_interest': 6,\n",
       " 'journalism': 45,\n",
       " 'interest_journalism': 1,\n",
       " 'regional': 75,\n",
       " 'refugee': 7247,\n",
       " 'regional_refugee': 3,\n",
       " 'resilience': 42,\n",
       " 'refugee_and_resilience': 2,\n",
       " 'plan': 479,\n",
       " 'resilience_plan': 3,\n",
       " '3rp': 3,\n",
       " 'plan_3rp': 2,\n",
       " 'response': 429,\n",
       " '3rp_in_response': 1,\n",
       " 'response_to_the_syria': 1,\n",
       " 'crisis': 1796,\n",
       " 'syria_crisis': 39,\n",
       " 'country': 5167,\n",
       " 'crisis_country': 1,\n",
       " 'chapter': 17,\n",
       " 'country_chapter': 1,\n",
       " 'chapter_turkey': 1,\n",
       " 'january': 86,\n",
       " 'turkey_january': 1,\n",
       " '2020': 835,\n",
       " 'january_2020': 9,\n",
       " '2020_turkey': 15,\n",
       " 'reliefweb': 20,\n",
       " 'turkey_reliefweb': 4,\n",
       " 'change': 392,\n",
       " 'farming': 10,\n",
       " 'change_in_the_farming': 1,\n",
       " 'fishing': 33,\n",
       " 'farming_and_fishing': 1,\n",
       " 'vote': 210,\n",
       " 'fishing_vote': 1,\n",
       " 'is': 34810,\n",
       " 'vote_is': 1,\n",
       " 'is_crucial': 9,\n",
       " 'scottish': 18,\n",
       " 'crucial_for_scottish': 1,\n",
       " 'independence': 29,\n",
       " 'scottish_independence': 1,\n",
       " 'they_re': 711,\n",
       " 'independence_they_re': 1,\n",
       " 'already': 1638,\n",
       " 'they_re_already': 5,\n",
       " 'worried': 36,\n",
       " 'already_worried': 1,\n",
       " 'migrant': 3414,\n",
       " 'worried_by_migrant': 1,\n",
       " 'labour': 156,\n",
       " 'migrant_labour': 11,\n",
       " 'shortages': 9,\n",
       " 'labour_shortages': 2,\n",
       " 'imports': 9,\n",
       " 'shortages_imports': 1,\n",
       " 'cheap': 99,\n",
       " 'imports_of_cheap': 1,\n",
       " 'lower': 46,\n",
       " 'cheap_lower': 1,\n",
       " 'quality': 43,\n",
       " 'lower_quality': 1,\n",
       " 'products': 29,\n",
       " 'quality_products': 1,\n",
       " 'access': 314,\n",
       " 'products_and_access': 1,\n",
       " 'eu': 22468,\n",
       " 'access_to_eu': 3,\n",
       " 'markets': 61,\n",
       " 'eu_markets': 1,\n",
       " 'cut_through': 33,\n",
       " 'migrants_cut_through': 16,\n",
       " 'barbed_wire': 76,\n",
       " 'cut_through_barbed_wire': 16,\n",
       " 'barbed_wire_at_greek': 9,\n",
       " 'greek_border': 3331,\n",
       " 'via_youtube': 403,\n",
       " 'border_via_youtube': 41,\n",
       " 'probably': 354,\n",
       " 'probably_any': 1,\n",
       " 'idnflation': 1,\n",
       " 'any_idnflation': 1,\n",
       " 'numbers': 392,\n",
       " 'idnflation_of_the_numbers': 1,\n",
       " 'numbers_is': 4,\n",
       " 'intended': 40,\n",
       " 'is_intended': 10,\n",
       " 'mollify': 2,\n",
       " 'intended_to_mollify': 1,\n",
       " 'domestic': 48,\n",
       " 'mollify_domestic': 1,\n",
       " 'anger': 58,\n",
       " 'domestic_anger': 1,\n",
       " 'anger_at_syrian': 1,\n",
       " 'syrian_refugee': 1142,\n",
       " 'refugee_numbers': 18,\n",
       " 'numbers_and_turkey': 1,\n",
       " 'turkey_s': 1529,\n",
       " 'involvement': 51,\n",
       " 's_involvement': 6,\n",
       " 'ongoing': 154,\n",
       " 'involvement_in_the_ongoing': 2,\n",
       " 'war': 4219,\n",
       " 'ongoing_war': 5,\n",
       " 'war_in_syria': 436,\n",
       " 'i_stand': 1837,\n",
       " 'i_stand_with_greece': 1632,\n",
       " 'greece_erdogan': 39,\n",
       " 'erdogan_has': 295,\n",
       " 'opened': 518,\n",
       " 'has_opened': 62,\n",
       " 'his': 2771,\n",
       " 'opened_his': 11,\n",
       " 'his_borders': 33,\n",
       " 'borders_and_let': 21,\n",
       " 'thousands': 2214,\n",
       " 'let_thousands': 22,\n",
       " 'illegal': 2084,\n",
       " 'thousands_of_illegal': 16,\n",
       " 'illegal_migrants': 953,\n",
       " 'enter': 1106,\n",
       " 'migrants_enter': 24,\n",
       " 'enter_greece': 193,\n",
       " 'europeans': 725,\n",
       " 'greece_europeans': 1,\n",
       " 'you': 18500,\n",
       " 'europeans_you': 6,\n",
       " 'should': 3793,\n",
       " 'you_should': 332,\n",
       " 'know': 2064,\n",
       " 'should_know': 39,\n",
       " 'that': 17590,\n",
       " 'know_that': 319,\n",
       " 'that_all': 109,\n",
       " 'all_of_them': 116,\n",
       " 'wish': 195,\n",
       " 'them_wish': 1,\n",
       " 'wish_to_go': 7,\n",
       " 'rich': 265,\n",
       " 'go_to_the_rich': 8,\n",
       " 'rich_countries': 35,\n",
       " 'countries_of_europe': 9,\n",
       " 'we': 10936,\n",
       " 'europe_we': 105,\n",
       " 'we_all': 159,\n",
       " 'all_should': 8,\n",
       " 'prevent': 838,\n",
       " 'should_prevent': 4,\n",
       " 'prevent_this': 14,\n",
       " 'army': 1297,\n",
       " 'this_army': 6,\n",
       " 'army_of_illegal': 2,\n",
       " 'enter_europe': 187,\n",
       " 'new_refugee_crisis': 102,\n",
       " 'refugee_crisis_as': 49,\n",
       " '30000': 115,\n",
       " 'as_30000': 28,\n",
       " '30000_migrants': 48,\n",
       " 'migrants_from_countries': 20,\n",
       " 'including': 585,\n",
       " 'countries_including': 30,\n",
       " 'including_syria': 19,\n",
       " 'mass': 544,\n",
       " 'syria_mass': 14,\n",
       " 'turkish': 8325,\n",
       " 'mass_on_turkish': 35,\n",
       " 'turkish_borders': 196,\n",
       " 'landing': 49,\n",
       " 'migrants_landing': 4,\n",
       " 'lesbos': 1518,\n",
       " 'landing_on_lesbos': 2,\n",
       " 'insinuating': 4,\n",
       " 'lesbos_insinuating': 1,\n",
       " 'it': 12169,\n",
       " 'insinuating_it': 1,\n",
       " 'it_is': 2421,\n",
       " 'staged': 93,\n",
       " 'is_staged': 9,\n",
       " 'staged_as': 2,\n",
       " 'coincidental': 2,\n",
       " 'as_coincidental': 1,\n",
       " 'coincidental_that': 2,\n",
       " 'photographers': 16,\n",
       " 'that_photographers': 1,\n",
       " 'were': 3890,\n",
       " 'photographers_were': 1,\n",
       " 'were_there': 21,\n",
       " 'greeks': 2035,\n",
       " 'we_greeks': 9,\n",
       " 'as_well': 533,\n",
       " 'greeks_as_well': 1,\n",
       " 'but': 8479,\n",
       " 'as_well_but': 5,\n",
       " 'did': 2097,\n",
       " 'but_did': 12,\n",
       " 'i': 6672,\n",
       " 'did_i': 22,\n",
       " 'i_you': 2,\n",
       " 'also': 2866,\n",
       " 'you_also': 43,\n",
       " 'kindly': 35,\n",
       " 'also_kindly': 1,\n",
       " 'reminded': 19,\n",
       " 'kindly_reminded': 1,\n",
       " 'him': 1038,\n",
       " 'reminded_him': 1,\n",
       " 'him_that': 12,\n",
       " 'that_illegal': 22,\n",
       " 'shouldnt': 100,\n",
       " 'migrants_shouldnt': 2,\n",
       " 'shouldnt_be': 30,\n",
       " 'weaponized': 182,\n",
       " 'be_weaponized': 5,\n",
       " 'enering': 1,\n",
       " 'weaponized_enering': 1,\n",
       " 'illegally': 624,\n",
       " 'enering_illegally': 1,\n",
       " 'sovereign': 118,\n",
       " 'illegally_sovereign': 1,\n",
       " 'statrs': 1,\n",
       " 'sovereign_statrs': 1,\n",
       " 'because': 2988,\n",
       " 'statrs_because': 1,\n",
       " 'what': 6213,\n",
       " 'because_what': 4,\n",
       " 'what_we': 150,\n",
       " 'see': 2161,\n",
       " 'we_see': 117,\n",
       " 'see_in_our': 1,\n",
       " 'our_border': 108,\n",
       " 'border_are': 295,\n",
       " 'are_not': 3118,\n",
       " 'not_refugees': 1259,\n",
       " 'refugees_they': 867,\n",
       " 'they_are': 4941,\n",
       " 'are_immigrants': 121,\n",
       " 'immigrants_that': 120,\n",
       " 'that_turkey': 664,\n",
       " 'admitted': 46,\n",
       " 'turkey_admitted': 3,\n",
       " 'admitted_in_turkey': 1,\n",
       " 'visa': 211,\n",
       " 'turkey_without_a_visa': 1,\n",
       " 'requirement': 11,\n",
       " 'visa_requirement': 2,\n",
       " 'frankly': 17,\n",
       " 'speaking': 159,\n",
       " 'frankly_speaking': 1,\n",
       " 'speaking_we': 3,\n",
       " 'we_should': 233,\n",
       " 'should_not': 331,\n",
       " 'not_be': 409,\n",
       " 'retaining': 3,\n",
       " 'be_retaining': 1,\n",
       " 'retaining_refugees': 1,\n",
       " 'refugees_in_turkey': 770,\n",
       " 'first_place': 161,\n",
       " 'turkey_in_the_first_place': 4,\n",
       " 'first_place_they': 5,\n",
       " 'they_should': 315,\n",
       " 'have': 10744,\n",
       " 'should_have': 192,\n",
       " 'every': 791,\n",
       " 'have_in_every': 1,\n",
       " 'part': 864,\n",
       " 'every_part': 2,\n",
       " 'part_of_the_eu': 32,\n",
       " 'world': 2294,\n",
       " 'eu_and_the_world': 4,\n",
       " 'so': 5416,\n",
       " 'world_so': 9,\n",
       " 'so_this': 45,\n",
       " 'policy': 741,\n",
       " 'this_policy': 8,\n",
       " 'policy_is': 41,\n",
       " 'correct': 168,\n",
       " 'is_correct': 18,\n",
       " 'one': 2720,\n",
       " 'correct_one': 1,\n",
       " 'right_now': 375,\n",
       " 'one_right_now': 1,\n",
       " 'right_now_eu': 3,\n",
       " 'felt': 33,\n",
       " 'eu_felt': 1,\n",
       " 'heat': 20,\n",
       " 'felt_the_heat': 2,\n",
       " 'now': 5653,\n",
       " 'heat_now': 1,\n",
       " 'now_eu': 48,\n",
       " 'sees': 93,\n",
       " 'eu_sees': 16,\n",
       " 'sees_refugees': 4,\n",
       " 'refugees_as': 628,\n",
       " 'invaders': 667,\n",
       " 'as_invaders': 18,\n",
       " 'how': 3399,\n",
       " 'invaders_how': 2,\n",
       " 'humanly': 4,\n",
       " 'how_humanly': 1,\n",
       " 'humanly_right': 1,\n",
       " 'translation': 37,\n",
       " 'translation_turkey': 1,\n",
       " 'turkey_is': 3144,\n",
       " 'creating': 440,\n",
       " 'is_creating': 47,\n",
       " 'creating_refugees': 60,\n",
       " 'paid': 570,\n",
       " 'turkey_paid': 16,\n",
       " 'much': 697,\n",
       " 'paid_much': 4,\n",
       " 'more': 5122,\n",
       " 'much_more': 90,\n",
       " 'more_for_them': 1,\n",
       " 'than': 1101,\n",
       " 'them_than': 8,\n",
       " 'than_eu': 14,\n",
       " 'eu_paid': 62,\n",
       " 'paid_turkey': 47,\n",
       " 'turkey_to_help': 25,\n",
       " 'help_them': 258,\n",
       " 'them_and_also': 5,\n",
       " 'also_eu': 23,\n",
       " 'don_t': 3105,\n",
       " 'eu_don_t': 29,\n",
       " 'even': 2308,\n",
       " 'don_t_even': 48,\n",
       " 'pay': 867,\n",
       " 'even_pay': 12,\n",
       " 'pay_it': 6,\n",
       " 'properly': 57,\n",
       " 'it_properly': 1,\n",
       " 'properly_we': 1,\n",
       " 'see_how': 185,\n",
       " 'we_see_how': 6,\n",
       " 'see_how_eu': 3,\n",
       " 'threat': 571,\n",
       " 'eu_threat': 1,\n",
       " 'threat_to_them': 1,\n",
       " 'video': 1391,\n",
       " 'them_at_the_video': 1,\n",
       " 'video_and_turkey': 1,\n",
       " 'had': 1564,\n",
       " 'turkey_had': 64,\n",
       " 'had_that': 3,\n",
       " 'that_refugees': 146,\n",
       " 'before': 1017,\n",
       " 'refugees_before': 31,\n",
       " 'before_turkish': 2,\n",
       " 'operations': 116,\n",
       " 'turkish_operations': 5,\n",
       " 'operations_also': 1,\n",
       " 'esed': 25,\n",
       " 'also_esed': 1,\n",
       " 'was': 4675,\n",
       " 'esed_was': 1,\n",
       " 'killing': 985,\n",
       " 'was_killing': 6,\n",
       " 'his_own': 167,\n",
       " 'killing_his_own': 13,\n",
       " 'his_own_people': 56,\n",
       " 'people_and_they': 10,\n",
       " 'they_were': 682,\n",
       " 'running': 234,\n",
       " 'were_running': 4,\n",
       " 'running_to_turkey': 2,\n",
       " 'fire_tear': 158,\n",
       " 'greek_police_fire_tear': 87,\n",
       " 'gas': 728,\n",
       " 'fire_tear_gas': 155,\n",
       " 'gas_at_migrants': 59,\n",
       " 'tur': 31,\n",
       " 'migrants_on_tur': 2,\n",
       " 'transit': 78,\n",
       " 'is_a_transit': 5,\n",
       " 'state': 1102,\n",
       " 'transit_state': 1,\n",
       " 'state_and_not': 2,\n",
       " 'not_a_third': 1,\n",
       " 'third_country': 26,\n",
       " 'additionally': 10,\n",
       " 'country_additionally': 1,\n",
       " 'additionally_it': 1,\n",
       " 'it_has': 278,\n",
       " 'reached': 237,\n",
       " 'has_reached': 38,\n",
       " 'social': 274,\n",
       " 'reached_the_social': 1,\n",
       " 'economic': 676,\n",
       " 'social_and_economic': 1,\n",
       " 'limits': 50,\n",
       " 'economic_limits': 1,\n",
       " 'limits_to_take_care': 1,\n",
       " 'millions': 2137,\n",
       " 'take_care_of_millions': 5,\n",
       " 'millions_of_refugees': 549,\n",
       " 'refugees_greece': 256,\n",
       " 'must': 1736,\n",
       " 'greece_must': 117,\n",
       " 'accept': 1113,\n",
       " 'must_accept': 33,\n",
       " 'accept_them': 115,\n",
       " 'them_it': 36,\n",
       " 'has_not': 170,\n",
       " 'got': 722,\n",
       " 'not_got': 6,\n",
       " 'any_other': 183,\n",
       " 'got_any_other': 1,\n",
       " 'option': 92,\n",
       " 'any_other_option': 2,\n",
       " 'es': 14,\n",
       " 'geht': 3,\n",
       " 'es_geht': 1,\n",
       " 'erst': 1,\n",
       " 'geht_erst': 1,\n",
       " 'los': 14,\n",
       " 'erst_los': 1,\n",
       " 'wartet': 1,\n",
       " 'los_wartet': 1,\n",
       " 'mal': 1,\n",
       " 'wartet_mal': 1,\n",
       " 'ab': 7,\n",
       " 'mal_ab': 1,\n",
       " '2015': 611,\n",
       " 'ab_2015': 1,\n",
       " 'wird': 1,\n",
       " '2015_wird': 1,\n",
       " 'sich': 2,\n",
       " 'wird_sich': 1,\n",
       " 'wiederholen': 1,\n",
       " 'sich_wiederholen': 1,\n",
       " 'over': 3514,\n",
       " 'wiederholen_over': 1,\n",
       " '47k': 1,\n",
       " 'over_47k': 1,\n",
       " '47k_immigrants': 1,\n",
       " 'march': 480,\n",
       " 'immigrants_march': 1,\n",
       " 'march_to_eu': 1,\n",
       " 'eu_from_turkey': 26,\n",
       " 'minister': 395,\n",
       " 'turkey_minister': 4,\n",
       " 'turkish_interior': 99,\n",
       " 'minister_turkish_interior': 1,\n",
       " 'minister_suleyman': 30,\n",
       " 'turkish_interior_minister_suleyman': 30,\n",
       " 'soylu': 86,\n",
       " 'minister_suleyman_soylu': 30,\n",
       " 'stated': 90,\n",
       " 'soylu_stated': 2,\n",
       " 'saturday': 188,\n",
       " 'stated_on_saturday': 1,\n",
       " 'saturday_that': 3,\n",
       " 'that_over': 7,\n",
       " '47000': 8,\n",
       " 'over_47000': 5,\n",
       " '47000_immigrants': 1,\n",
       " 'crossed': 145,\n",
       " 'immigrants_crossed': 5,\n",
       " 'crossed_turkey': 5,\n",
       " 's_border': 355,\n",
       " 'their': 8054,\n",
       " 'border_on_their': 1,\n",
       " 'way': 2069,\n",
       " 'their_way': 273,\n",
       " 'towards': 1014,\n",
       " 'way_towards': 4,\n",
       " 'towards_europe': 102,\n",
       " 'better': 713,\n",
       " 'ask': 711,\n",
       " 'better_ask': 3,\n",
       " 'turkish_airlines': 41,\n",
       " 'ask_the_turkish_airlines': 1,\n",
       " 'turkish_airlines_how': 1,\n",
       " 'how_the_refugees': 11,\n",
       " 'sudan': 53,\n",
       " 'refugees_from_sudan': 1,\n",
       " 'bangladesh': 152,\n",
       " 'sudan_bangladesh': 1,\n",
       " 'nigeria': 33,\n",
       " 'bangladesh_nigeria': 2,\n",
       " 'morocco': 166,\n",
       " 'nigeria_morocco': 1,\n",
       " 'algeria': 122,\n",
       " 'morocco_algeria': 18,\n",
       " 'arrived': 280,\n",
       " 'algeria_arrived': 1,\n",
       " 'arrived_in_turkey': 9,\n",
       " 'businesses': 53,\n",
       " 'turkey_businesses': 3,\n",
       " 'are_already': 185,\n",
       " 'safe_country': 301,\n",
       " 'already_in_a_safe_country': 13,\n",
       " 'safe_country_turkey': 18,\n",
       " 'turkey_so': 133,\n",
       " 'so_they': 428,\n",
       " 'not_by_any': 2,\n",
       " 'definition': 55,\n",
       " 'any_definition': 2,\n",
       " 'definition_refugees': 1,\n",
       " 'totally': 206,\n",
       " 'i_totally': 17,\n",
       " 'agree': 327,\n",
       " 'totally_agree': 21,\n",
       " 'since': 1294,\n",
       " 'agree_since': 1,\n",
       " 'since_they': 66,\n",
       " 'threatened': 177,\n",
       " 'not_threatened': 4,\n",
       " 'threatened_by_turkish': 1,\n",
       " 'autorities': 1,\n",
       " 'turkish_autorities': 1,\n",
       " 'autorities_they': 1,\n",
       " 'should_be': 991,\n",
       " 'they_should_be': 84,\n",
       " 'assisted': 33,\n",
       " 'should_be_assisted': 1,\n",
       " 'apply_for_asylum': 120,\n",
       " 'assisted_to_apply_for_asylum': 1,\n",
       " 'closest': 20,\n",
       " 'apply_for_asylum_to_the_closest': 1,\n",
       " 'european': 2072,\n",
       " 'closest_european': 1,\n",
       " 'embassy': 112,\n",
       " 'european_embassy': 3,\n",
       " 'embassy_turkey': 1,\n",
       " 'is_a_safe_country': 96,\n",
       " 'safe_country_for_refugees': 8,\n",
       " 'do': 5552,\n",
       " 'refugees_do': 124,\n",
       " 'do_you': 1005,\n",
       " 'suggest': 108,\n",
       " 'you_suggest': 14,\n",
       " 'otherwise': 184,\n",
       " 'suggest_otherwise': 2,\n",
       " 'all_the_eu': 38,\n",
       " 'eu_were': 25,\n",
       " 'telling': 245,\n",
       " 'were_telling': 8,\n",
       " 'telling_turkey': 3,\n",
       " 'get': 2709,\n",
       " 'turkey_to_get': 24,\n",
       " 'get_more': 28,\n",
       " 'more_refugees': 676,\n",
       " 'humanity': 1051,\n",
       " 'refugees_in_for_humanity': 1,\n",
       " 'humanity_but': 13,\n",
       " 'but_humanity': 2,\n",
       " 'doesn_t': 824,\n",
       " 'humanity_doesn_t': 3,\n",
       " 'exist': 87,\n",
       " 'doesn_t_exist': 5,\n",
       " 'when': 3571,\n",
       " 'exist_when': 2,\n",
       " 'when_those': 17,\n",
       " 'your': 5234,\n",
       " 'are_on_your': 5,\n",
       " 'your_border': 74,\n",
       " 'i_guess': 164,\n",
       " 'border_i_guess': 3,\n",
       " 'i_guess_turkey': 7,\n",
       " 'feeding': 94,\n",
       " 'is_feeding': 19,\n",
       " '4_million': 884,\n",
       " 'feeding_4_million': 6,\n",
       " 'after': 2967,\n",
       " '4_million_and_after': 1,\n",
       " 'couple': 148,\n",
       " 'after_a_couple': 1,\n",
       " 'thousand': 249,\n",
       " 'couple_thousand': 1,\n",
       " 'thousand_you': 3,\n",
       " 're': 943,\n",
       " 'you_re': 452,\n",
       " 'starting': 111,\n",
       " 're_starting': 3,\n",
       " 'cry': 222,\n",
       " 'starting_to_cry': 1,\n",
       " 'not_because': 49,\n",
       " 'because_that': 20,\n",
       " 'poster': 27,\n",
       " 'that_poster': 2,\n",
       " 'poster_was': 4,\n",
       " 'used': 723,\n",
       " 'was_used': 27,\n",
       " 'brexit': 446,\n",
       " 'used_for_brexit': 1,\n",
       " '2016': 433,\n",
       " 'brexit_in_2016': 1,\n",
       " '2016_where': 2,\n",
       " 'issue': 660,\n",
       " 'where_the_issue': 1,\n",
       " 'issue_was': 2,\n",
       " 'was_eu': 8,\n",
       " 'free_movement': 75,\n",
       " 'eu_free_movement': 5,\n",
       " 'free_movement_not': 1,\n",
       " 'not_turkey': 114,\n",
       " 'weaponizing': 200,\n",
       " 'turkey_weaponizing': 25,\n",
       " 'weaponizing_syrian': 5,\n",
       " 'refugees_in_2020': 5,\n",
       " 'open_borders': 344,\n",
       " 'turkey_to_open_borders': 1,\n",
       " 'open_borders_and_let': 1,\n",
       " 'let_refugees': 93,\n",
       " 'into_europe': 837,\n",
       " 'europe_after': 98,\n",
       " '33': 88,\n",
       " 'after_33': 8,\n",
       " 'soldiers_killed': 46,\n",
       " '33_soldiers_killed': 1,\n",
       " 'soldiers_killed_by_syrian': 1,\n",
       " 'regime': 857,\n",
       " 'syrian_regime': 130,\n",
       " 'eye': 63,\n",
       " 'refugees_eye': 3,\n",
       " 'eye_europe': 3,\n",
       " 'europe_as': 205,\n",
       " 'as_turkey': 296,\n",
       " 'hits': 124,\n",
       " 'turkey_hits': 11,\n",
       " 'breaking_point': 59,\n",
       " 'hits_breaking_point': 3,\n",
       " 'those_poor': 87,\n",
       " 'those_poor_refugees': 33,\n",
       " 'accepted': 295,\n",
       " 'turkey_accepted': 10,\n",
       " 'nearly': 310,\n",
       " 'accepted_nearly': 2,\n",
       " '5': 684,\n",
       " 'nearly_5': 2,\n",
       " 'mio': 40,\n",
       " '5_mio': 2,\n",
       " 'mio_refugees': 21,\n",
       " 'helped': 434,\n",
       " 'refugees_and_helped': 2,\n",
       " 'helped_them': 31,\n",
       " 'them_as': 320,\n",
       " 'as_much': 103,\n",
       " 'much_as': 65,\n",
       " 'possible': 337,\n",
       " 'as_possible': 66,\n",
       " 'possible_but': 6,\n",
       " 'but_turkey': 147,\n",
       " 'turkey_has': 1192,\n",
       " 'has_also': 56,\n",
       " 'also_his': 6,\n",
       " 'his_limits': 4,\n",
       " 'limits_now': 2,\n",
       " 'it_s': 2611,\n",
       " 'now_it_s': 74,\n",
       " 'time': 1776,\n",
       " 'it_s_time': 95,\n",
       " 'other_countries': 516,\n",
       " 'time_for_other_countries': 1,\n",
       " 'prove': 102,\n",
       " 'other_countries_to_prove': 1,\n",
       " 'prove_humanity': 1,\n",
       " 'if': 6276,\n",
       " 'idlib': 2382,\n",
       " 'if_idlib': 24,\n",
       " 'falls': 55,\n",
       " 'idlib_falls': 19,\n",
       " 'falls_to_assad': 6,\n",
       " 'assad_it': 15,\n",
       " 'will': 7303,\n",
       " 'it_will': 386,\n",
       " 'will_not': 792,\n",
       " 'just': 4168,\n",
       " 'not_just': 267,\n",
       " 'just_be': 27,\n",
       " 'syrians': 2528,\n",
       " 'be_syrians': 17,\n",
       " 'who': 6721,\n",
       " 'syrians_who': 91,\n",
       " 'pay_the_price': 42,\n",
       " 'who_pay_the_price': 6,\n",
       " 'labib': 2,\n",
       " 'pay_the_price_labib': 2,\n",
       " 'al': 305,\n",
       " 'labib_al': 2,\n",
       " 'nahhas': 2,\n",
       " 'al_nahhas': 2,\n",
       " 'opinion': 180,\n",
       " 'nahhas_opinion': 1,\n",
       " 'guardian': 248,\n",
       " 'opinion_the_guardian': 1,\n",
       " 'measures': 252,\n",
       " 'what_measures': 1,\n",
       " 'measures_has': 1,\n",
       " 'european_commission': 386,\n",
       " 'has_the_european_commission': 1,\n",
       " 'taken': 536,\n",
       " 'european_commission_taken': 1,\n",
       " 'date': 74,\n",
       " 'taken_to_date': 1,\n",
       " 'date_to_prevent': 1,\n",
       " 'prevent_turkey': 3,\n",
       " 'weaponising': 98,\n",
       " 'turkey_from_weaponising': 1,\n",
       " 'migration': 1025,\n",
       " 'weaponising_migration': 3,\n",
       " 'migration_and_refugees': 8,\n",
       " 'blackmail': 1012,\n",
       " 'refugees_to_blackmail': 71,\n",
       " 'blackmail_the_eu': 88,\n",
       " 'its': 5030,\n",
       " 'eu_and_its': 30,\n",
       " 'citizens': 862,\n",
       " 'its_citizens': 39,\n",
       " 'is_it': 367,\n",
       " 'true': 679,\n",
       " 'it_true': 13,\n",
       " 'true_that': 29,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases,ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "tweet_corpus_tokens = [tweet.split() for tweet in unique_tweets_df['text_clean']]\n",
    "#tweet_corpus_tokens\n",
    "bigram = Phrases(tweet_corpus_tokens, min_count=25, threshold=10,connector_words=ENGLISH_CONNECTOR_WORDS) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[tweet_corpus_tokens],min_count=25, threshold=10,connector_words=ENGLISH_CONNECTOR_WORDS) \n",
    "\n",
    "\n",
    "trigram.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: use this only if training of ngram models is complete, it only serves purpose of saving memory\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['migrants_clash', 'with', 'greek_police', 'as', 'they', 'exit', 'turkey', 'bbc_news'], ['nato', 'urges', 'syria', 'russia', 'to', 'halt', 'airstrikes', 'as', 'migrants', 'move', 'westward'], ['good', 'on', 'greece', 'close', 'all', 'our', 'borders', 'let', 'turkey', 'and', 'pm', 'erdogen', 'take', 'all', 'these', 'islamic', 'refugees', 'there', 'are', 'a', 'lot', 'of', 'nice', 'safe', 'wealthy', 'islamic', 'places', 'they', 'can', 'go', 'to', 'make', 'them', 'take', 'them', 'not', 'greece', 'or', 'any', 'of', 'our', 'countries', 'please', 'retweet', 'this'], ['increased', 'risk', 'of', 'maternal', 'death', 'in', 'immigrants', 'in', 'europe', 'the', 'usa', 'risk', 'depends', 'on', 'maternal', 'birthplace', 'region', 'where', 'prenatal', 'checkups', 'delivery', 'took', 'place', 'other', 'maternal', 'characteristics', 'such_as', 'age'], ['europe', 'has', 'three', 'choices', 'first', 'take', 'refugees', 'in', 'second', 'help', 'turkey', 'to', 'create', 'a', 'buffer_zone', 'in', 'syria', 'to', 'move', 'those', 'refugees', 'into', 'third', 'gave', 'turkey', 'at', 'least', '20', 'billion', 'usd', 'right', 'away', 'to', 'take_care', 'of', 'those', 'refugees', 'there', 'are', 'no', 'other', 'choices', 'people', 'needs_to_be', 'realistic'], ['syrian', 'refugees', 'are', 'erdogan', 's', 'pawns', 'in', 'gambit', 'to', 'thwart', 'assad'], ['austrian_chancellor', 'hints', 'at', 'closing', 'borders', 'as', 'police', 'clash', 'with', 'refugees', 'on', 'greece', 'turkey', 'border'], ['turkey', 'frustrated', 'by', 'refugee_crisis', 'erdogan', 'aide', 'bbc_news'], ['reporters', 'without', 'borders', 'condemns', 'greek', 'public', 'tv', 'radio', 'broadcaster', 'erts', 'censorship', 'of', 'reporting', 'from', 'islands', 'where', 'the', 'creation', 'of', 'new', 'migrant_camps', 'has', 'triggered', 'protests', 'coverage', 'of', 'such', 'a', 'crucial', 'subject', 'clearly', 'serves', 'public', 'interest', 'journalism'], ['regional', 'refugee', 'and', 'resilience', 'plan', '3rp', 'in', 'response', 'to', 'the', 'syria', 'crisis', 'country', 'chapter', 'turkey', 'january', '2020', 'turkey', 'reliefweb']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "model_phrases = [trigram[tweet] for tweet in tweet_corpus_tokens]\n",
    "print(model_phrases[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINDING BEST PARAMETERS FOR WORD2VEC MODEL\n",
    "\n",
    "#negatives = [5,10,20]\n",
    "sizes = [100,200,300]\n",
    "sgs=[0,1]\n",
    "windows =[3,5] \n",
    "#cbow_means = [0,1]\n",
    "#iters=[10]\n",
    "\n",
    "for size in sizes:\n",
    "        for window in windows:\n",
    "            #print(f'\\nfor params size={size},negative={neg},sg={sg},hs={hs},window={window},cbow_mean={cbow},iter={it}')\n",
    "            print(f'\\nfor params size={size},window={window}')\n",
    "            model = Word2Vec(model_phrases,vector_size=size,window=window)\n",
    "            print(model.wv.most_similar('refugees'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first model done\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(model_phrases,vector_size=300,window=3,sg=0)\n",
    "print('first model done')\n",
    "#model2 = api.load(\"glove-twitter-200\")\n",
    "#print('second model done')\n",
    "#model3 = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\nikodemicek\\Dropbox (CBS)\\Master thesis data\\GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "#print('third model done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('migrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "model2 = FastText(vector_size=100, window=4, min_count=5, sentences=model_phrases, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('refugess', 0.8553880453109741),\n",
       " ('refugee_boy', 0.8527049422264099),\n",
       " ('refuge', 0.8440694808959961),\n",
       " ('refugeesgr', 0.8257689476013184),\n",
       " ('refugee_flows', 0.8112236857414246),\n",
       " ('womenrefugeeroute', 0.8065913915634155),\n",
       " ('refugee_flow', 0.8042846918106079),\n",
       " ('largest_refugee', 0.8042380809783936),\n",
       " ('refugeesand', 0.8039513230323792),\n",
       " ('refugee_issue', 0.7974311113357544)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('refugee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6360404"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.similarity('refugee','migrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('moria_fasttext_model',model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. We instantiate stanza english language module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-17 14:52:56 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-05-17 14:52:56 INFO: Use device: cpu\n",
      "2021-05-17 14:52:56 INFO: Loading: tokenize\n",
      "2021-05-17 14:52:56 INFO: Loading: pos\n",
      "2021-05-17 14:52:57 INFO: Loading: lemma\n",
      "2021-05-17 14:52:57 INFO: Loading: depparse\n",
      "2021-05-17 14:52:58 INFO: Loading: sentiment\n",
      "2021-05-17 14:52:59 INFO: Loading: ner\n",
      "2021-05-17 14:53:01 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ needed when running first time ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "\n",
    "#stanza.download(\"en\")\n",
    "\n",
    "#stanza.install_corenlp()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# loading the pipeline\n",
    "en_nlp = stanza.Pipeline(\"en\", tokenize_pretokenized=True, ner_batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle_file(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event_data(event_name):\n",
    "    assert event_name in ['moria','tigray','channel','all','beirut'], f\"Oh no! We do not analyze {event_name} event\"\n",
    "    \n",
    "    print(f'Loading {event_name} data...')\n",
    "    try:\n",
    "        #sample = 2000\n",
    "        event_np_list = load_pickle(event_name + '_np_list')#[1000:sample]\n",
    "        event_crf_list = load_pickle(event_name + '_crf_list')#[1000:sample]\n",
    "        event_tagged_tweets = load_pickle(event_name + '_tagged_tweets')#[1000:sample]\n",
    "        \n",
    "        return event_np_list,event_crf_list,event_tagged_tweets\n",
    "    except:\n",
    "        print(f'The {event_name} files not found! Run candidate_extraction.py file on the {eventname}_df')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading moria data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 92806/92806 [08:40<00:00, 178.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing moria noun phrase candidates...\n",
      "removing long candidates...\n",
      "Removed 0 candidates longer than 9 words!\n",
      "removing child NP candidates...\n",
      "Removed 0 child NP candidates!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/92806 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging moria noun phrase candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 92806/92806 [11:17:03<00:00,  2.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 92806/92806 [16:39<00:00, 92.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751019\n"
     ]
    }
   ],
   "source": [
    "def pipeline2(event_name,np_list):\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 1. LOAD THE DATA ~~~~~~~~~~~~~~~~~~~~~\n",
    "    event_np_list,event_crf_list,event_tagged_tweets = load_event_data(event_name)\n",
    "    event_np_list = np_list\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 2. GET POS AND NER TAGS ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # get easily accessible list of tuples (POS-tags of each word, NER-tags of each named entity) \n",
    "    tweet_tags = cand_prep.get_tweet_tags(event_tagged_tweets) \n",
    "    \n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 3. PREPROCESS CANDIDATES ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ~~~~~~~~~~~~ processing of noun phrases ~~~~~~~~~~~~~~~~~~~~~\n",
    "    print(f'Processing {event_name} noun phrase candidates...')\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    # remove NP candidates longer than threshold and remove all child NPs of parent NPs\n",
    "    event_np_list = cand_prep.remove_long_nps(event_np_list)\n",
    "    event_np_list = cand_prep.remove_child_nps(event_np_list) \n",
    "    #event_np_list = remove_weird_chars(event_np_list)\n",
    "    event_np_list = cand_prep.remove_char(event_np_list,'@')\n",
    "\n",
    "    event_np_list = [['no_candidate'] if len(noun_ps)==0 or noun_ps ==' ' else noun_ps for noun_ps in event_np_list ]\n",
    "    \n",
    "    #print(event_np_list)\n",
    "    print(f'Tagging {event_name} noun phrase candidates...')\n",
    "    #tag all tweets and save them in a list    \n",
    "\n",
    "    #tagged_np_cands = batched_np_list.progress_apply(en_nlp)\n",
    "    tagged_np_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_np_list)]\n",
    "    #tagged_np_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_np_list, en_nlp, batch_size=6000))]\n",
    "\n",
    "    np_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tagged_np_cands]\n",
    "    #print(np_cand_heads)\n",
    "    \n",
    "    np_and_cand_list = cand_prep.get_cand_type(event_np_list,np_cand_heads, tweet_tags)\n",
    "    #print(event_np_list)\n",
    "          \n",
    "          \n",
    "    # ~~~~~~~~~~~~~~~~~~~~ combining candidate lists ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #concatenate corefs and noun phrase lists\n",
    "    nps_cands = [cand for cands in np_and_cand_list for cand in cands]\n",
    "    #candidate_list = coref_and_cand_list + np_and_cand_list\n",
    "\n",
    "    #unpack list of lists into one list\n",
    "    candidate_list = nps_cands\n",
    "          \n",
    "    nps_tagged = [sent for tagged_cand in tagged_np_cands for sent in tagged_cand.sentences ]\n",
    "\n",
    "    all_cands_tagged = nps_tagged\n",
    "\n",
    "        \n",
    "    #print(len(candidate_list),'vs', len(all_cands_tagged))\n",
    "    cand_df = pd.DataFrame(\n",
    "        {'candidates': candidate_list,\n",
    "         'cand_tags': all_cands_tagged\n",
    "        })\n",
    "\n",
    "    cand_df['cand_text'] = cand_df.candidates.apply(lambda x: x[0])\n",
    "    cand_df['cand_len'] = cand_df.cand_text.apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "    count_cands = Counter(cand_df['cand_text'])\n",
    "    cand_df['cand_freq'] = cand_df[\"cand_text\"].map(count_cands)\n",
    "    \n",
    "    #count_cands[cand_df['cand_text']]\n",
    "    #count_sorted = sorted(count_cands.items(),key=lambda x: x[1],reverse=True)\n",
    "    cand_df.columns = cand_df.columns.str.strip()\n",
    "    \n",
    "          \n",
    "    # we sort the candidates by their length\n",
    "    cand_df.sort_values('cand_freq', ascending=False,inplace=True)\n",
    "\n",
    "    #cand_df = cand_df[cand_df.cand_text not in  ['no_candidate', 'candidate_to_be_removed']]\n",
    "\n",
    "    cand_df.reset_index(drop=True, inplace = True)\n",
    "    #remove dummy candidates that were used to avoid errors\n",
    "\n",
    "    \n",
    "    cand_df = cand_df[cand_df.cand_text != 'candidate_to_be_removed']\n",
    "    cand_df = cand_df[cand_df.cand_text != 'no_candidate']\n",
    "    print(len(cand_df))    \n",
    "    cand_df.reset_index(drop=True,inplace=True)\n",
    "          \n",
    "    return cand_df\n",
    "          \n",
    "#import random\n",
    "\n",
    "#random.seed(42)\n",
    "#np_list_sample = random.sample(np_list,10000)\n",
    "event_cands = pipeline2('moria',np_list)\n",
    "\n",
    "#pickle_file('moria_cands_df', moria_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1e938",
   "metadata": {},
   "source": [
    "Candidates as identified by stanza library still have a lot of noise that we want to remove so candidates merge better and we can throw away candidates that do not carry any information valuable for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally the candidates are cleaned before storing in a file prior to merging\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_cands(event_cands):\n",
    "    \"\"\"\n",
    "    Applying cleaning steps on candidates and engineering some features:\n",
    "     1. creating a column with length of the tweet (in chars)\n",
    "     2. lowercase the candidate information in the tuple with cand, candidate representative head and set of phrases heads\n",
    "     3. extract candidate text and keep only alphanumeric chars\n",
    "     4. remove candidates that are stopwords\n",
    "     5. remove candidates that are only numeric\n",
    "     6. remove candidates that are only 1 char long\n",
    "     \"\"\"\n",
    "    def clean_cand(cand):\n",
    "        cand = list(cand)\n",
    "        cand[0] = re.sub(r'[^A-Za-z0-9 ]+', '', cand[0].lower())\n",
    "        cand[1] = re.sub(r'[^A-Za-z0-9 ]+', '', cand[1].lower())\n",
    "        cand[2] = set([re.sub(r'[^A-Za-z0-9 ]+', '', phrase_word.lower()) for phrase_word in cand[2]])\n",
    "\n",
    "        return tuple(cand)\n",
    "\n",
    "    #stopwords\n",
    "    tqdm.pandas()\n",
    "    event_cands_clean = event_cands.copy()\n",
    "    \n",
    "    \n",
    "    event_cands_clean['candidates'] = event_cands_clean['candidates'].progress_apply(clean_cand)\n",
    "    \n",
    "    event_cands_clean['cand_text'] = event_cands_clean['cand_text'].progress_apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', '', x.lower()).strip())\n",
    "    event_cands_clean = event_cands_clean[~event_cands_clean['cand_text'].isin(stopwords.words('english'))]\n",
    "    event_cands_clean['pure_chars'] = event_cands_clean['cand_text'].progress_apply(lambda x: x.replace(' ', ''))\n",
    "    event_cands_clean = event_cands_clean[~event_cands_clean['pure_chars'].str.isnumeric()]\n",
    "    event_cands_clean.drop('pure_chars',axis=1,inplace=True)\n",
    "    \n",
    "    event_cands_clean['string_len'] = event_cands_clean['cand_text'].progress_apply(len)\n",
    "    event_cands_clean = event_cands_clean[event_cands_clean['string_len']>1]\n",
    "    event_cands_clean = event_cands_clean.drop_duplicates(subset = [\"cand_text\"])\n",
    "    event_cands_clean.reset_index(drop=True, inplace=True)\n",
    "    print(f'The event has  {len(event_cands_clean)} unique candidates after cleaning')\n",
    "    return event_cands_clean\n",
    "\n",
    "event_cands_clean = clean_cands(event_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('moria_short_cands', event_cands_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands = load_pickle('moria_short_cands')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. We apply stanza module on the tweets to get NER and POS tags. We do it in batches to speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. As initial WCL candidates, we extract noun phrases (NPs) and coreference chains.\n",
    "\n",
    "## We do so using CoreNLPClient wrapper\n",
    "\n",
    "### SOME PREPROCESSING NEEDED\n",
    "* remove links - check\n",
    "* remove # from hashtags? - check\n",
    "* remove/merge mentions? - check\n",
    "\n",
    "\n",
    "* remove recurring texts (signatures of news media) - any new spotted should be added in preprocessing file's '__remove_tweet_signatures__' function\n",
    "* remove posts of some accounts (refugee_list)\n",
    "* exclude NERs that tag numbers - should we mark phrase as NE if the head is not NE? - check\n",
    "* play around with candidate types\n",
    "* optimize code and make it neater\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. We keep only NPs shorter than 20 words and remove children of parent NPs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. We get the heads of noun phrases (in batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. We define candidate types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. We assign candidate types to noun phrase candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. We get coreference chains candidates from the tweet corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. We determine candidate's type for representative mentions of coref candidates (in batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. We combine the candidate lists for candidate merging\n",
    "\n",
    "We organize candidates in a list sorted by their number of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands_merged = load_pickle(\"moria_short_cands_merged\")\n",
    "merged_dict = load_pickle(\"moria_short_whatmerged2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from paper Shifting the refugee narratives? by Greussing & Boomgaarden (2015)\n",
    "frame_properties = {'settlement':['settlement','accomodation','permanent','temporary','barracks','accommodated','shelter'],\n",
    "                   'reception':['quota', 'distribution', 'limit', 'selection','reception','together','asylum','receive'],\n",
    "                    'security':['security', 'border','crossing','fence','control','flow'],\n",
    "                    'criminality':['officer','terror','suspicion','crime','offense','police','trafficking','suspect'],\n",
    "                    'economisation':['euro','economic','million','thousand','cost','money'],\n",
    "                    'humanitarian':['humane','voluntary','help','support','aid','care','solidarity'],\n",
    "                    'victimization':['fight','victim','war','dead','rescued','state'],\n",
    "                    'integration': ['labour','employed','unemployed','integration','positive'],\n",
    "                    \n",
    "                    #from hamborg\n",
    "                    'affection':['affection','attachment', 'devotion', 'fondness','love','passion'],\n",
    "                    'refusal': ['refusal','declination','denial','disallowance','nay','no'],\n",
    "                    'trustworthiness':['trustworthiness','integrity','accuracy','credibility','authenticity','fairness'],\n",
    "                    'no trustworthiness':['falsehood','dishonesty','unfairness','deceit','corruption'],\n",
    "                    'reason': ['reason','logic','sense','rationale','argument','justification'],\n",
    "                    'irrationality': ['unreason','irrationality','fallaciousness','unsoundness'],\n",
    "                    'easiness': ['easiness','simplicity','obviousness','ease','comfort'],\n",
    "                    'difficulty': ['difficulty','adversity','hardship','crisis','obstacle','trouble' ],\n",
    "                    'honor': ['honor', 'dignity','esteem','reputation','praise'],\n",
    "                    'dishonor': ['disgrace','dishonor','reproach','opprobrium']\n",
    "                   \n",
    "                   }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import conceptnet_lite as cn\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(r\"C:/Users/nikodemicek/Dropbox (CBS)/Master thesis data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "manual_cands = ['refugee','migrant','greece','turkey','syria','beirut','immigrant','aoun']\n",
    "\n",
    "\n",
    "# to run on the server we should use larger model according to the paper - \"conceptnet-numberbatch-17-06-300\"\n",
    "#model = api.load(\"glove-twitter-200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 5886/5886 [00:00<00:00, 39270.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>cand_tags</th>\n",
       "      <th>cand_text</th>\n",
       "      <th>cand_len</th>\n",
       "      <th>cand_freq</th>\n",
       "      <th>string_len</th>\n",
       "      <th>avg_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Greece, Greece, {Greece}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"Greece\",\\n ...</td>\n",
       "      <td>Greece</td>\n",
       "      <td>1</td>\n",
       "      <td>2066</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.29665634, -0.7863764, 1.0028423, -0.2250602...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(refugees, refugees, {refugees}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...</td>\n",
       "      <td>refugees</td>\n",
       "      <td>1</td>\n",
       "      <td>1156</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.92243963, 0.7983021, 0.7725275, -0.3225837...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(fire, fire, {fire}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"fire\",\\n   ...</td>\n",
       "      <td>fire</td>\n",
       "      <td>1</td>\n",
       "      <td>778</td>\n",
       "      <td>4</td>\n",
       "      <td>[1.0113711, 0.5400115, -2.0947576, 0.044214696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Turkey, Turkey, {Turkey}, loc-ne)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"Turkey\",\\n ...</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>1</td>\n",
       "      <td>545</td>\n",
       "      <td>6</td>\n",
       "      <td>[-1.0134584, -0.9566454, 1.0516194, -0.4690687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(migrants, migrants, {migrants}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"migrants\",\\...</td>\n",
       "      <td>migrants</td>\n",
       "      <td>1</td>\n",
       "      <td>488</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.3621457, 0.042272426, 0.37909022, -0.82739...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>(newsupdate, newsupdate, {newsupdate}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"newsupdate\"...</td>\n",
       "      <td>newsupdate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>(western detention facilties, facilties, {faci...</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"western\",\\n...</td>\n",
       "      <td>western detention facilties</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>(The child separations?, separations?, {separa...</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"The\",\\n    ...</td>\n",
       "      <td>The child separations</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>(Australias, Australias, {Australias}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"Australias\"...</td>\n",
       "      <td>Australias</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.008103137, 0.036747757, 0.01575262, 0.00544...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5885</th>\n",
       "      <td>(goal (still low), (still, {low), (still}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"goal\",\\n   ...</td>\n",
       "      <td>goal still low</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5886 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             candidates  \\\n",
       "0                      (Greece, Greece, {Greece}, misc)   \n",
       "1                (refugees, refugees, {refugees}, misc)   \n",
       "2                            (fire, fire, {fire}, misc)   \n",
       "3                    (Turkey, Turkey, {Turkey}, loc-ne)   \n",
       "4                (migrants, migrants, {migrants}, misc)   \n",
       "...                                                 ...   \n",
       "5881       (newsupdate, newsupdate, {newsupdate}, misc)   \n",
       "5882  (western detention facilties, facilties, {faci...   \n",
       "5883  (The child separations?, separations?, {separa...   \n",
       "5884       (Australias, Australias, {Australias}, misc)   \n",
       "5885   (goal (still low), (still, {low), (still}, misc)   \n",
       "\n",
       "                                              cand_tags  \\\n",
       "0     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"Greece\",\\n ...   \n",
       "1     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...   \n",
       "2     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"fire\",\\n   ...   \n",
       "3     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"Turkey\",\\n ...   \n",
       "4     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"migrants\",\\...   \n",
       "...                                                 ...   \n",
       "5881  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"newsupdate\"...   \n",
       "5882  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"western\",\\n...   \n",
       "5883  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"The\",\\n    ...   \n",
       "5884  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"Australias\"...   \n",
       "5885  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"goal\",\\n   ...   \n",
       "\n",
       "                        cand_text  cand_len  cand_freq  string_len  \\\n",
       "0                          Greece         1       2066           6   \n",
       "1                        refugees         1       1156           8   \n",
       "2                            fire         1        778           4   \n",
       "3                          Turkey         1        545           6   \n",
       "4                        migrants         1        488           8   \n",
       "...                           ...       ...        ...         ...   \n",
       "5881                   newsupdate         1          1          10   \n",
       "5882  western detention facilties         3          1          27   \n",
       "5883        The child separations         3          1          22   \n",
       "5884                   Australias         1          1          10   \n",
       "5885               goal still low         3          1          16   \n",
       "\n",
       "                                                avg_vec  \n",
       "0     [0.29665634, -0.7863764, 1.0028423, -0.2250602...  \n",
       "1     [-0.92243963, 0.7983021, 0.7725275, -0.3225837...  \n",
       "2     [1.0113711, 0.5400115, -2.0947576, 0.044214696...  \n",
       "3     [-1.0134584, -0.9566454, 1.0516194, -0.4690687...  \n",
       "4     [-0.3621457, 0.042272426, 0.37909022, -0.82739...  \n",
       "...                                                 ...  \n",
       "5881                                                NaN  \n",
       "5882                                                NaN  \n",
       "5883                                                NaN  \n",
       "5884  [0.008103137, 0.036747757, 0.01575262, 0.00544...  \n",
       "5885                                                NaN  \n",
       "\n",
       "[5886 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "model = load_pickle('moria_w2v_model')\n",
    "tqdm.pandas()\n",
    "event_cands_merged['avg_vec'] = event_cands_merged['candidates'].progress_apply(lambda x: phrase_heads_avg_vector(x[2]))\n",
    "event_cands_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>cand_tags</th>\n",
       "      <th>cand_text</th>\n",
       "      <th>cand_len</th>\n",
       "      <th>cand_freq</th>\n",
       "      <th>string_len</th>\n",
       "      <th>avg_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Greece, Greece, {Greece}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"Greece\",\\n ...</td>\n",
       "      <td>Greece</td>\n",
       "      <td>1</td>\n",
       "      <td>2066</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.29665634, -0.7863764, 1.0028423, -0.2250602...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(refugees, refugees, {refugees}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...</td>\n",
       "      <td>refugees</td>\n",
       "      <td>1</td>\n",
       "      <td>1156</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.92243963, 0.7983021, 0.7725275, -0.3225837...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(fire, fire, {fire}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"fire\",\\n   ...</td>\n",
       "      <td>fire</td>\n",
       "      <td>1</td>\n",
       "      <td>778</td>\n",
       "      <td>4</td>\n",
       "      <td>[1.0113711, 0.5400115, -2.0947576, 0.044214696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Turkey, Turkey, {Turkey}, loc-ne)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"Turkey\",\\n ...</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>1</td>\n",
       "      <td>545</td>\n",
       "      <td>6</td>\n",
       "      <td>[-1.0134584, -0.9566454, 1.0516194, -0.4690687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(migrants, migrants, {migrants}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"migrants\",\\...</td>\n",
       "      <td>migrants</td>\n",
       "      <td>1</td>\n",
       "      <td>488</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.3621457, 0.042272426, 0.37909022, -0.82739...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>(France' s, France', {France', s}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"France'\",\\n...</td>\n",
       "      <td>France s</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.44062924, 0.55070215, -0.049154773, 0.35485...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>(battering ram (again), battering, {battering,...</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"battering\",...</td>\n",
       "      <td>battering ram again</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>[-0.008449046, 0.04736015, 0.028447097, 0.0139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>(Yall, Yall, {Yall}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"Yall\",\\n   ...</td>\n",
       "      <td>Yall</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.107103385, 0.2241038, 0.0412581, 0.0283824...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>(this statement insinuate, insinuate, {insinua...</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"this\",\\n   ...</td>\n",
       "      <td>this statement insinuate</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>[-0.016040474, 0.567505, 0.2532831, 0.31219295...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>(Australias, Australias, {Australias}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"Australias\"...</td>\n",
       "      <td>Australias</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.008103137, 0.036747757, 0.01575262, 0.00544...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2996 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             candidates  \\\n",
       "0                      (Greece, Greece, {Greece}, misc)   \n",
       "1                (refugees, refugees, {refugees}, misc)   \n",
       "2                            (fire, fire, {fire}, misc)   \n",
       "3                    (Turkey, Turkey, {Turkey}, loc-ne)   \n",
       "4                (migrants, migrants, {migrants}, misc)   \n",
       "...                                                 ...   \n",
       "5874           (France' s, France', {France', s}, misc)   \n",
       "5875  (battering ram (again), battering, {battering,...   \n",
       "5879                         (Yall, Yall, {Yall}, misc)   \n",
       "5880  (this statement insinuate, insinuate, {insinua...   \n",
       "5884       (Australias, Australias, {Australias}, misc)   \n",
       "\n",
       "                                              cand_tags  \\\n",
       "0     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"Greece\",\\n ...   \n",
       "1     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...   \n",
       "2     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"fire\",\\n   ...   \n",
       "3     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"Turkey\",\\n ...   \n",
       "4     [\\n  {\\n    \"id\": 1,\\n    \"text\": \"migrants\",\\...   \n",
       "...                                                 ...   \n",
       "5874  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"France'\",\\n...   \n",
       "5875  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"battering\",...   \n",
       "5879  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"Yall\",\\n   ...   \n",
       "5880  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"this\",\\n   ...   \n",
       "5884  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"Australias\"...   \n",
       "\n",
       "                     cand_text  cand_len  cand_freq  string_len  \\\n",
       "0                       Greece         1       2066           6   \n",
       "1                     refugees         1       1156           8   \n",
       "2                         fire         1        778           4   \n",
       "3                       Turkey         1        545           6   \n",
       "4                     migrants         1        488           8   \n",
       "...                        ...       ...        ...         ...   \n",
       "5874                  France s         2          1           9   \n",
       "5875       battering ram again         3          1          21   \n",
       "5879                      Yall         1          1           4   \n",
       "5880  this statement insinuate         3          1          24   \n",
       "5884                Australias         1          1          10   \n",
       "\n",
       "                                                avg_vec  \n",
       "0     [0.29665634, -0.7863764, 1.0028423, -0.2250602...  \n",
       "1     [-0.92243963, 0.7983021, 0.7725275, -0.3225837...  \n",
       "2     [1.0113711, 0.5400115, -2.0947576, 0.044214696...  \n",
       "3     [-1.0134584, -0.9566454, 1.0516194, -0.4690687...  \n",
       "4     [-0.3621457, 0.042272426, 0.37909022, -0.82739...  \n",
       "...                                                 ...  \n",
       "5874  [0.44062924, 0.55070215, -0.049154773, 0.35485...  \n",
       "5875  [-0.008449046, 0.04736015, 0.028447097, 0.0139...  \n",
       "5879  [-0.107103385, 0.2241038, 0.0412581, 0.0283824...  \n",
       "5880  [-0.016040474, 0.567505, 0.2532831, 0.31219295...  \n",
       "5884  [0.008103137, 0.036747757, 0.01575262, 0.00544...  \n",
       "\n",
       "[2996 rows x 7 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecm = event_cands_merged.dropna()\n",
    "ecm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 328 iterations.\n",
      "it took 165.0170841217041 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "import numpy as np\n",
    "X = np.array(list(ecm['avg_vec']))\n",
    "t0 = time.time()\n",
    "clustering = AffinityPropagation(damping = 0.95, max_iter=1000, convergence_iter=10,verbose=True, random_state=42).fit(X)\n",
    "print(f'it took {time.time()-t0} seconds')\n",
    "\n",
    "ecm['label'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>cand_tags</th>\n",
       "      <th>cand_text</th>\n",
       "      <th>cand_len</th>\n",
       "      <th>cand_freq</th>\n",
       "      <th>string_len</th>\n",
       "      <th>avg_vec</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>(safe shelter Have, Have, {shelter, Have}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"safe\",\\n   ...</td>\n",
       "      <td>safe shelter Have</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>[-0.04584418, -0.42060813, -0.8032175, 0.38723...</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            candidates  \\\n",
       "2742  (safe shelter Have, Have, {shelter, Have}, misc)   \n",
       "\n",
       "                                              cand_tags          cand_text  \\\n",
       "2742  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"safe\",\\n   ...  safe shelter Have   \n",
       "\n",
       "      cand_len  cand_freq  string_len  \\\n",
       "2742         3          1          17   \n",
       "\n",
       "                                                avg_vec  label  \n",
       "2742  [-0.04584418, -0.42060813, -0.8032175, 0.38723...    201  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecm[ecm['label']==201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year_calendar_week</th>\n",
       "      <th>refugee</th>\n",
       "      <th>migrant</th>\n",
       "      <th>immigrant</th>\n",
       "      <th>asylum_seeker</th>\n",
       "      <th>other</th>\n",
       "      <th>is_dup</th>\n",
       "      <th>text_clean_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Canada's immigrant population is 20%, USA is 1...</td>\n",
       "      <td>en</td>\n",
       "      <td>1267244723103690753</td>\n",
       "      <td>2020-06-01 00:01:00+00:00</td>\n",
       "      <td>442949745</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_6</td>\n",
       "      <td>2020_22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>canada s immigrant population is 20 usa is 13 ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Hi @EUHomeAffairs @Place_Beauvau @BMI_Bund @uk...</td>\n",
       "      <td>en</td>\n",
       "      <td>1267247183725621248</td>\n",
       "      <td>2020-06-01 00:10:47+00:00</td>\n",
       "      <td>211570886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_6</td>\n",
       "      <td>2020_22</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hi euhomeaffairs placebeauvau bundesministeriu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>#Greece Dozens of Asylum seekers, who face the...</td>\n",
       "      <td>en</td>\n",
       "      <td>1267251185838407681</td>\n",
       "      <td>2020-06-01 00:26:41+00:00</td>\n",
       "      <td>119888012</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_6</td>\n",
       "      <td>2020_22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>greece dozens of asylum seekers who face the r...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Hmmm? Maybe not, Spain is the COVID-19 hot spo...</td>\n",
       "      <td>en</td>\n",
       "      <td>1267260557213806599</td>\n",
       "      <td>2020-06-01 01:03:56+00:00</td>\n",
       "      <td>4839872717</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_6</td>\n",
       "      <td>2020_22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hmmm maybe not spain is the covid  19 hot spot...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Greece to evict over 10,000 refugees from shel...</td>\n",
       "      <td>en</td>\n",
       "      <td>1267264681108025346</td>\n",
       "      <td>2020-06-01 01:20:19+00:00</td>\n",
       "      <td>1171990967001526272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_6</td>\n",
       "      <td>2020_22</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>greece to evict over 10000 refugees from shelters</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92801</th>\n",
       "      <td>SocialFlow</td>\n",
       "      <td>UPDATE: Officials say 19 people have been disp...</td>\n",
       "      <td>en</td>\n",
       "      <td>1366162926491598851</td>\n",
       "      <td>2021-02-28 23:06:38+00:00</td>\n",
       "      <td>15357193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_2</td>\n",
       "      <td>2021_08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>update officials say 19 people have been displ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92802</th>\n",
       "      <td>dlvr.it</td>\n",
       "      <td>19 people displaced from home following apartm...</td>\n",
       "      <td>en</td>\n",
       "      <td>1366163153772404739</td>\n",
       "      <td>2021-02-28 23:07:32+00:00</td>\n",
       "      <td>133496245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_2</td>\n",
       "      <td>2021_08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>19 people displaced from home following apartm...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92803</th>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>we all hope a better year that we can touch ea...</td>\n",
       "      <td>en</td>\n",
       "      <td>1366166599800139780</td>\n",
       "      <td>2021-02-28 23:21:14+00:00</td>\n",
       "      <td>75055396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_2</td>\n",
       "      <td>2021_08</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>we all hope a better year that we can touch ea...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92804</th>\n",
       "      <td>Nicholas Franklin</td>\n",
       "      <td>HEADLINE: Greece migrants: Afghan father charg...</td>\n",
       "      <td>en</td>\n",
       "      <td>1366169485162336257</td>\n",
       "      <td>2021-02-28 23:32:42+00:00</td>\n",
       "      <td>205329051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_2</td>\n",
       "      <td>2021_08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92805</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>@Idonttakemeser1 @MFATurkey we know very well ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1366174262864842756</td>\n",
       "      <td>2021-02-28 23:51:41+00:00</td>\n",
       "      <td>1299237735648104453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_2</td>\n",
       "      <td>2021_08</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>we know very well that greece cannot even shit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92806 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  source                                               text  \\\n",
       "0        Twitter Web App  Canada's immigrant population is 20%, USA is 1...   \n",
       "1        Twitter Web App  Hi @EUHomeAffairs @Place_Beauvau @BMI_Bund @uk...   \n",
       "2        Twitter Web App  #Greece Dozens of Asylum seekers, who face the...   \n",
       "3        Twitter Web App  Hmmm? Maybe not, Spain is the COVID-19 hot spo...   \n",
       "4        Twitter Web App  Greece to evict over 10,000 refugees from shel...   \n",
       "...                  ...                                                ...   \n",
       "92801         SocialFlow  UPDATE: Officials say 19 people have been disp...   \n",
       "92802            dlvr.it  19 people displaced from home following apartm...   \n",
       "92803          TweetDeck  we all hope a better year that we can touch ea...   \n",
       "92804  Nicholas Franklin  HEADLINE: Greece migrants: Afghan father charg...   \n",
       "92805    Twitter Web App  @Idonttakemeser1 @MFATurkey we know very well ...   \n",
       "\n",
       "      lang                   id                 created_at  \\\n",
       "0       en  1267244723103690753  2020-06-01 00:01:00+00:00   \n",
       "1       en  1267247183725621248  2020-06-01 00:10:47+00:00   \n",
       "2       en  1267251185838407681  2020-06-01 00:26:41+00:00   \n",
       "3       en  1267260557213806599  2020-06-01 01:03:56+00:00   \n",
       "4       en  1267264681108025346  2020-06-01 01:20:19+00:00   \n",
       "...    ...                  ...                        ...   \n",
       "92801   en  1366162926491598851  2021-02-28 23:06:38+00:00   \n",
       "92802   en  1366163153772404739  2021-02-28 23:07:32+00:00   \n",
       "92803   en  1366166599800139780  2021-02-28 23:21:14+00:00   \n",
       "92804   en  1366169485162336257  2021-02-28 23:32:42+00:00   \n",
       "92805   en  1366174262864842756  2021-02-28 23:51:41+00:00   \n",
       "\n",
       "                 author_id  retweet_count  reply_count  like_count  \\\n",
       "0                442949745              1            1           9   \n",
       "1                211570886              0            0           0   \n",
       "2                119888012             12            2          11   \n",
       "3               4839872717              0            0           0   \n",
       "4      1171990967001526272              0            0           0   \n",
       "...                    ...            ...          ...         ...   \n",
       "92801             15357193              1            0           1   \n",
       "92802            133496245              0            0           0   \n",
       "92803             75055396              0            0           0   \n",
       "92804            205329051              0            0           0   \n",
       "92805  1299237735648104453              0            0           1   \n",
       "\n",
       "       quote_count  ... year_month year_calendar_week refugee migrant  \\\n",
       "0                1  ...     2020_6            2020_22   False   False   \n",
       "1                0  ...     2020_6            2020_22    True   False   \n",
       "2                0  ...     2020_6            2020_22   False   False   \n",
       "3                0  ...     2020_6            2020_22   False   False   \n",
       "4                0  ...     2020_6            2020_22    True   False   \n",
       "...            ...  ...        ...                ...     ...     ...   \n",
       "92801            0  ...     2021_2            2021_08   False   False   \n",
       "92802            0  ...     2021_2            2021_08   False   False   \n",
       "92803            0  ...     2021_2            2021_08    True    True   \n",
       "92804            0  ...     2021_2            2021_08   False   False   \n",
       "92805            0  ...     2021_2            2021_08    True   False   \n",
       "\n",
       "       immigrant  asylum_seeker  other is_dup  \\\n",
       "0           True          False  False  False   \n",
       "1          False          False  False  False   \n",
       "2          False          False  False  False   \n",
       "3           True          False  False  False   \n",
       "4          False          False  False  False   \n",
       "...          ...            ...    ...    ...   \n",
       "92801      False          False   True  False   \n",
       "92802      False          False   True  False   \n",
       "92803      False          False  False  False   \n",
       "92804      False          False  False   True   \n",
       "92805      False          False  False  False   \n",
       "\n",
       "                                        text_clean_right  label  \n",
       "0      canada s immigrant population is 20 usa is 13 ...    0.0  \n",
       "1      hi euhomeaffairs placebeauvau bundesministeriu...    0.0  \n",
       "2      greece dozens of asylum seekers who face the r...    5.0  \n",
       "3      hmmm maybe not spain is the covid  19 hot spot...    0.0  \n",
       "4      greece to evict over 10000 refugees from shelters    0.0  \n",
       "...                                                  ...    ...  \n",
       "92801  update officials say 19 people have been displ...    0.0  \n",
       "92802  19 people displaced from home following apartm...    0.0  \n",
       "92803  we all hope a better year that we can touch ea...    3.0  \n",
       "92804                                                NaN    NaN  \n",
       "92805  we know very well that greece cannot even shit...    1.0  \n",
       "\n",
       "[92806 rows x 26 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_df= load_pickle('moria_df_with_clusters')\n",
    "event_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
