{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate merging and related preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages for the following parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "#python libraries\n",
    "import stanza\n",
    "from stanza_batch import batch\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# self written modules\n",
    "import preprocessing\n",
    "\n",
    "import candidate_processing as cand_prep\n",
    "import candidate_extraction as cand_ex\n",
    "\n",
    "\"\"\"import candidate_extraction as cand_ex\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "seg = Segmenter() \"\"\"\n",
    "\n",
    "from ekphrasis.classes.tokenizer import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. We import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                          | 559/24511 [00:00<00:08, 2679.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 24511 tweets!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24511/24511 [00:08<00:00, 2754.93it/s]\n"
     ]
    }
   ],
   "source": [
    "beirut_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_beirut.csv\" # for Beirut\n",
    "\n",
    "moria_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_moria.csv\" # for Moria\n",
    "\n",
    "\n",
    "def read_event_df(data_url):\n",
    "    directory_path = os.getcwd() + \"/../../../\" + data_url \n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df(beirut_url)\n",
    "#channel_df = read_event_df(channel_url)\n",
    "tqdm.pandas()\n",
    "event_df['text_clean']= event_df['text'].progress_apply(preprocessing.preprocess_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24511/24511 [00:00<00:00, 95374.35it/s]\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"/Users/nikodemicek/Dropbox (CBS)/Master thesis data\"\n",
    "USERS_PATH = FILE_PATH + \"/df_users.csv\"\n",
    "# Read the users csv\n",
    "df_users = pd.read_csv(USERS_PATH)\n",
    "\n",
    "# Drop unnecessary index column\n",
    "df_users.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "df_users.head()\n",
    "\n",
    "# Create dict that maps usernames to actual names\n",
    "mapping = dict(df_users[[\"username\",\"name\"]].values)\n",
    "mapping = {f'@{key}': value for key, value in mapping.items()}\n",
    "\n",
    "\n",
    "def resolve_username_to_name(text):\n",
    "    new_text = text\n",
    "    for word in text.split(\" \"):\n",
    "        if word in mapping:\n",
    "            new_text = new_text.replace(word,mapping[word])\n",
    "    return new_text\n",
    "\n",
    "#tqdm.pandas()\n",
    "event_df['text_clean'] = event_df['text_clean'].progress_apply(resolve_username_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-10 14:14:18 INFO: Writing properties to tmp file: corenlp_server-ba00c23f696f48af.props\n",
      "2021-04-10 14:14:18 INFO: Starting server with command: java -Xmx8G -cp C:\\Users\\nikodemicek\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 600000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-ba00c23f696f48af.props -annotators tokenize,ssplit,pos,parse,coref -preload -outputFormat serialized\n",
      "  0%|▎                                                                           | 118/24511 [00:00<00:20, 1168.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting coreference chains...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24511/24511 [00:15<00:00, 1542.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 24511/24511 [4:27:31<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from stanza.server import CoreNLPClient\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer \n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def replace_corefs(tweet_series, all=True):\n",
    "\n",
    "    #corefs_list = list()\n",
    "    #tweets_list = list(event_df)\n",
    "\n",
    "    #so we have control over whether we extract only np or coref candidates\n",
    "    #nps = True if all == True or all == 'nps' else False\n",
    "    #corefs = True if all == True or all == 'corefs' else False\n",
    "    \n",
    "    with CoreNLPClient(annotators=['tokenize','ssplit','pos','parse',\"coref\"], \n",
    "                       properties ={'coref.algorithm' : 'neural','ssplit':'eolonly'}, \n",
    "                       timeout=600000, memory='8G') as client:\n",
    "\n",
    "        def resolve_corefs(tweet,client=client):\n",
    "\n",
    "            ann = client.annotate(tweet)        \n",
    "            tweet_chains = ann.corefChain\n",
    "            all_chains = list()\n",
    "            all_locs = list()\n",
    "            #print(tweet)\n",
    "            \n",
    "            for chain in tweet_chains:\n",
    "                chain_words = list()\n",
    "                word_locs = list()\n",
    "                # Loop through every mention of this chain\n",
    "                for mention in chain.mention:\n",
    "                    # Get the sentence in which this mention is located, and get the words which are part of this mention\n",
    "                    words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]\n",
    "                    #build a string out of the words of this mention\n",
    "                    coref_mention = ' '.join([word.word for word in words_list])\n",
    "                    identified_mention_loc = (mention.sentenceIndex,mention.beginIndex,mention.endIndex)\n",
    "                    \n",
    "                    chain_words.append(coref_mention)\n",
    "                    word_locs.append(identified_mention_loc)\n",
    "                    \n",
    "                #the corefering words will be stored alongside the index of their representative in a tuple\n",
    "                coref_group = (chain_words,chain.representative)\n",
    "                #coref_cand = coref_group[0][coref_group[1]]\n",
    "                all_chains.append(coref_group)\n",
    "                all_locs.append(word_locs)\n",
    "            \n",
    "            #print(all_locs)\n",
    "            #print(all_chains)\n",
    "            tweet = sent_tokenize(tweet)\n",
    "            for sent_id in range(len(tweet)):\n",
    "                tweet[sent_id]=word_tokenize(tweet[sent_id])\n",
    "            #print(tweet)\n",
    "            for coref_words,chain_locs in zip(all_chains,all_locs):\n",
    "                #print(coref,lc)\n",
    "                rep_mention_id = coref_words[1]\n",
    "                rep_mention = coref_words[0][rep_mention_id]\n",
    "                for word,loc in zip(coref_words[0],chain_locs):\n",
    "                    tweet[loc[0]][loc[1]:loc[2]] = [rep_mention]\n",
    "                    #print(tweet)\n",
    "\n",
    "            for sent_id in range(len(tweet)):\n",
    "                tweet[sent_id] = detokenize(tweet[sent_id])\n",
    "                #print(tweet[sent_id])\n",
    "                \n",
    "                \n",
    "            tweet = detokenize(tweet)  \n",
    "                \n",
    "            #tweet = [detokenize(sent) for sents in tweet for sent in detokenize(sents)]\n",
    "            #print(tweet)\n",
    "            return tweet\n",
    "        \n",
    "        \n",
    "        def tokenizer(tweet):\n",
    "            tweet = word_tokenize(tweet)\n",
    "            tweet = ' '.join(tweet)\n",
    "            tweet = sent_tokenize(tweet)\n",
    "            tweet = '\\n'.join(tweet)\n",
    "            return tweet\n",
    "        # get noun phrases with tregex using get_noun_phrases function\n",
    "        #print('extracting noun phrases...')\n",
    "        tqdm.pandas()\n",
    "        #noun_phrase_list = list(event_df.progress_apply(get_noun_phrases,args=(client,\"tokenize,ssplit,pos,lemma,parse\")))\n",
    "        #noun_phrase_list = [get_noun_phrases(client,tweets_list[tweet_id], annotators=\"tokenize,ssplit,pos,lemma,parse\") for tweet_id in tqdm(range(len(tweets_list)))]\n",
    "\n",
    "\n",
    "        print('extracting coreference chains...')\n",
    "        # get coreference chains using the .annotate method of client handled by get_coref_chain function  \n",
    "        tweet_series = tweet_series.progress_apply(tokenizer)\n",
    "        \n",
    "        print('extracting coreference chains...')    \n",
    "        corefs_list = tweet_series.progress_apply(resolve_corefs)\n",
    "        #for tweet_id in tqdm(range(len(tweets_list))):\n",
    "            #coref_chains = [chain for chain in get_coref_chain(event_df[tweet_id],client)] \n",
    "\n",
    "        #corefs_list.append(['no_candidate']) if len(corefs_list) == 0 else corefs_list.append(coref_chains)\n",
    "                \n",
    "             \n",
    "\n",
    "    return corefs_list\n",
    "\n",
    "event_corefs_resolved = replace_corefs(event_df['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I read all your books professor, and even wait...\n",
       "1        Im was a Lebanese immigrant and fought amongst...\n",
       "2        migrant _ workers _ lives _ matter Lets not fo...\n",
       "3        More than a dozen refugees in eastern Lebanon ...\n",
       "4        What a lovely way to start the week with inspi...\n",
       "                               ...                        \n",
       "24506    Why was the CIA not issued a report on the (Be...\n",
       "24507    , I am a Syrian refugee in Lebanon. I have fou...\n",
       "24508    i am literally lebanese?????? im not a refuge ...\n",
       "24509    Whenever a Palestinian auntie asks me what a P...\n",
       "24510    we all hope a better year that we can touch ea...\n",
       "Name: text_clean, Length: 24511, dtype: object"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_corefs_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-11 10:52:00 INFO: Writing properties to tmp file: corenlp_server-8dada1fab204493d.props\n",
      "2021-04-11 10:52:00 INFO: Starting server with command: java -Xmx8G -cp C:\\Users\\nikodemicek\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 6000000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-8dada1fab204493d.props -annotators tokenize,ssplit,pos,parse -preload -outputFormat serialized\n",
      "  0%|                                                                                        | 0/24511 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting noun phrases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24511/24511 [4:35:37<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_noun_phrases(tweet, client, annotators=None):\n",
    "    \"\"\"\n",
    "    Input: client = CoreNLPClient instance\n",
    "           tweet = tweet text\n",
    "           annotators = allowed CoreNLP operations\n",
    "    Output: list of all noun phrases in the tweet\n",
    "    \"\"\"\n",
    "    pattern = 'NP'\n",
    "    matches = client.tregex(tweet,pattern,annotators=annotators)\n",
    "    list_of_nps = [sentence[match_id]['spanString']  for sentence in matches['sentences'] for match_id in sentence if len(sentence[match_id]['spanString'].split())<5 ]\n",
    "    #print(list_of_nps)\n",
    "\n",
    "    return list_of_nps \n",
    "\n",
    "\n",
    "with CoreNLPClient(annotators=[\"tokenize,ssplit,pos,parse\"], \n",
    "                   timeout=6000000, memory='8G') as client:\n",
    "\n",
    "        # get noun phrases with tregex using get_noun_phrases function\n",
    "        print('extracting noun phrases...')\n",
    "        tqdm.pandas()\n",
    "        noun_phrase_list = list(event_corefs_resolved.progress_apply(get_noun_phrases,args=(client,\"tokenize,ssplit,pos,parse\")))\n",
    "\n",
    "np_list = noun_phrase_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_list = noun_phrase_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing child NP candidates...\n",
      "Removed 44551 child NP candidates!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_cand_len(cand_list):\n",
    "    # calculates number of candidates in the corpus\n",
    "    sum_len = 0\n",
    "    for tweet_cands in cand_list:\n",
    "        sum_len += len(tweet_cands)\n",
    "    return sum_len\n",
    "\n",
    "def remove_child_nps(noun_phrase_list):\n",
    "    print(f'removing child NP candidates...')\n",
    "    initial_len = get_cand_len(noun_phrase_list)\n",
    "    # remove the child NPs and keep only parents, run until the sum_len stops decreasing\n",
    "    after_removal_len = 0\n",
    "    while after_removal_len != get_cand_len(noun_phrase_list):\n",
    "        after_removal_len = get_cand_len(noun_phrase_list)\n",
    "        for tweet_nps in noun_phrase_list:\n",
    "            for noun_p in range(len(tweet_nps)):\n",
    "                try:\n",
    "                    #if the subsequent noun_p (child np) is contained in the current one, remove the child np\n",
    "                    if tweet_nps[noun_p].find(tweet_nps[noun_p+1]) != -1:\n",
    "                        tweet_nps.remove(tweet_nps[noun_p+1])\n",
    "                        \n",
    "                #ignore the error caused with end of the list\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "    len_after_removal = get_cand_len(noun_phrase_list)\n",
    "    print(f'Removed {initial_len-len_after_removal} child NP candidates!')\n",
    "    return noun_phrase_list\n",
    "\n",
    "np_list = remove_child_nps(np_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        i read all your books professor and even waite...\n",
       "1        my grandfather was a lebanese immigrant and fo...\n",
       "2        migrant  workers  lives  matter lets not forge...\n",
       "3        more than a dozen refugees in eastern lebanon ...\n",
       "4        what a lovely way to start the week with inspi...\n",
       "                               ...                        \n",
       "24506    why was the cia not issued a report on the bei...\n",
       "24507     i am a syrian refugee in lebanon  i have four...\n",
       "24508    i am literally lebanese im not a refuge or imm...\n",
       "24509    whenever a palestinian auntie asks me what my ...\n",
       "24510    we all hope a better year that we can touch ea...\n",
       "Name: text_clean, Length: 24511, dtype: object"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_corpus = event_df['text_clean'].apply(lambda tweet:re.sub(r'[^A-Za-z0-9 ]+', '', tweet.lower()))\n",
    "tweet_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {b'i': 3607,\n",
       "             b'read': 425,\n",
       "             b'i_read': 14,\n",
       "             b'all': 1901,\n",
       "             b'read_all': 5,\n",
       "             b'your': 1039,\n",
       "             b'all_your': 10,\n",
       "             b'books': 27,\n",
       "             b'your_books': 2,\n",
       "             b'professor': 10,\n",
       "             b'books_professor': 1,\n",
       "             b'and': 19174,\n",
       "             b'professor_and': 2,\n",
       "             b'even': 577,\n",
       "             b'and_even': 33,\n",
       "             b'waited': 5,\n",
       "             b'even_waited': 1,\n",
       "             b'on': 4233,\n",
       "             b'waited_on': 1,\n",
       "             b'you': 3364,\n",
       "             b'on_you': 20,\n",
       "             b'when': 933,\n",
       "             b'you_when': 6,\n",
       "             b'when_you': 63,\n",
       "             b'came': 266,\n",
       "             b'you_came': 5,\n",
       "             b'to': 17961,\n",
       "             b'came_to': 81,\n",
       "             b'ilili': 1,\n",
       "             b'to_ilili': 1,\n",
       "             b'this': 3341,\n",
       "             b'ilili_this': 1,\n",
       "             b'tweet': 78,\n",
       "             b'this_tweet': 5,\n",
       "             b'gave': 63,\n",
       "             b'tweet_gave': 1,\n",
       "             b'so': 1338,\n",
       "             b'gave_so': 1,\n",
       "             b'much': 406,\n",
       "             b'so_much': 133,\n",
       "             b'confidence': 16,\n",
       "             b'much_confidence': 1,\n",
       "             b'because': 650,\n",
       "             b'confidence_because': 1,\n",
       "             b'we': 3251,\n",
       "             b'because_we': 32,\n",
       "             b'did': 340,\n",
       "             b'we_did': 6,\n",
       "             b'did_this': 8,\n",
       "             b'this_and': 26,\n",
       "             b'and_we': 162,\n",
       "             b'also': 716,\n",
       "             b'we_also': 19,\n",
       "             b'also_did': 6,\n",
       "             b'jerusalem': 35,\n",
       "             b'did_jerusalem': 1,\n",
       "             b'jerusalem_and': 4,\n",
       "             b'beirut': 4883,\n",
       "             b'and_beirut': 25,\n",
       "             b'next': 195,\n",
       "             b'beirut_next': 1,\n",
       "             b'week': 305,\n",
       "             b'next_week': 6,\n",
       "             b'in': 24139,\n",
       "             b'week_in': 18,\n",
       "             b'a': 11001,\n",
       "             b'in_a': 540,\n",
       "             b'non': 165,\n",
       "             b'a_non': 13,\n",
       "             b'a7zab': 1,\n",
       "             b'non_a7zab': 1,\n",
       "             b'related': 43,\n",
       "             b'a7zab_related': 1,\n",
       "             b'kitchen': 16,\n",
       "             b'related_kitchen': 1,\n",
       "             b'skin': 13,\n",
       "             b'kitchen_skin': 1,\n",
       "             b'inthe': 2,\n",
       "             b'skin_inthe': 1,\n",
       "             b'game': 19,\n",
       "             b'inthe_game': 1,\n",
       "             b'my': 1407,\n",
       "             b'grandfather': 31,\n",
       "             b'my_grandfather': 16,\n",
       "             b'was': 2288,\n",
       "             b'grandfather_was': 8,\n",
       "             b'was_a': 222,\n",
       "             b'lebanese': 11040,\n",
       "             b'a_lebanese': 499,\n",
       "             b'immigrant': 717,\n",
       "             b'lebanese_immigrant': 277,\n",
       "             b'immigrant_and': 33,\n",
       "             b'fought': 31,\n",
       "             b'and_fought': 3,\n",
       "             b'amongst': 28,\n",
       "             b'fought_amongst': 1,\n",
       "             b'the': 32266,\n",
       "             b'amongst_the': 11,\n",
       "             b'second': 139,\n",
       "             b'the_second': 64,\n",
       "             b'wave': 38,\n",
       "             b'second_wave': 5,\n",
       "             b'at': 1846,\n",
       "             b'wave_at': 1,\n",
       "             b'normandy': 1,\n",
       "             b'at_normandy': 1,\n",
       "             b'im': 249,\n",
       "             b'normandy_im': 1,\n",
       "             b'proud': 105,\n",
       "             b'im_proud': 1,\n",
       "             b'proud_to': 25,\n",
       "             b'have': 3224,\n",
       "             b'to_have': 116,\n",
       "             b'his': 678,\n",
       "             b'have_his': 1,\n",
       "             b'blood': 122,\n",
       "             b'his_blood': 1,\n",
       "             b'pumping': 2,\n",
       "             b'blood_pumping': 1,\n",
       "             b'under': 440,\n",
       "             b'pumping_under': 1,\n",
       "             b'under_my': 1,\n",
       "             b'my_skin': 2,\n",
       "             b'skin_i': 1,\n",
       "             b'am': 421,\n",
       "             b'i_am': 376,\n",
       "             b'antifa': 6,\n",
       "             b'am_antifa': 1,\n",
       "             b'migrant': 5214,\n",
       "             b'workers': 4965,\n",
       "             b'migrant_workers': 1303,\n",
       "             b'lives': 402,\n",
       "             b'workers_lives': 2,\n",
       "             b'matter': 136,\n",
       "             b'lives_matter': 46,\n",
       "             b'lets': 87,\n",
       "             b'matter_lets': 1,\n",
       "             b'not': 2473,\n",
       "             b'lets_not': 13,\n",
       "             b'forget': 307,\n",
       "             b'not_forget': 37,\n",
       "             b'forget_lebanese': 1,\n",
       "             b'racism': 272,\n",
       "             b'lebanese_racism': 2,\n",
       "             b'more': 2071,\n",
       "             b'than': 1181,\n",
       "             b'more_than': 658,\n",
       "             b'than_a': 41,\n",
       "             b'dozen': 18,\n",
       "             b'a_dozen': 8,\n",
       "             b'refugees': 11490,\n",
       "             b'dozen_refugees': 3,\n",
       "             b'refugees_in': 2482,\n",
       "             b'eastern': 94,\n",
       "             b'in_eastern': 21,\n",
       "             b'lebanon': 18780,\n",
       "             b'eastern_lebanon': 13,\n",
       "             b'lebanon_have': 222,\n",
       "             b'tested': 10,\n",
       "             b'have_tested': 2,\n",
       "             b'positive': 36,\n",
       "             b'tested_positive': 4,\n",
       "             b'for': 9524,\n",
       "             b'positive_for': 7,\n",
       "             b'for_the': 922,\n",
       "             b'coronavirus': 184,\n",
       "             b'the_coronavirus': 60,\n",
       "             b'which': 783,\n",
       "             b'coronavirus_which': 1,\n",
       "             b'shows': 68,\n",
       "             b'which_shows': 2,\n",
       "             b'shows_the': 14,\n",
       "             b'vulnerability': 39,\n",
       "             b'the_vulnerability': 5,\n",
       "             b'of': 16579,\n",
       "             b'vulnerability_of': 11,\n",
       "             b'of_refugees': 821,\n",
       "             b'in_the': 2911,\n",
       "             b'country': 1539,\n",
       "             b'the_country': 425,\n",
       "             b'many': 1357,\n",
       "             b'country_many': 2,\n",
       "             b'many_of': 146,\n",
       "             b'whom': 62,\n",
       "             b'of_whom': 39,\n",
       "             b'live': 587,\n",
       "             b'whom_live': 8,\n",
       "             b'live_in': 297,\n",
       "             b'crowded': 16,\n",
       "             b'in_crowded': 4,\n",
       "             b'poor': 201,\n",
       "             b'crowded_poor': 1,\n",
       "             b'conditions': 399,\n",
       "             b'poor_conditions': 11,\n",
       "             b'what': 1110,\n",
       "             b'what_a': 42,\n",
       "             b'lovely': 20,\n",
       "             b'a_lovely': 5,\n",
       "             b'way': 411,\n",
       "             b'lovely_way': 1,\n",
       "             b'way_to': 75,\n",
       "             b'start': 144,\n",
       "             b'to_start': 36,\n",
       "             b'start_the': 3,\n",
       "             b'the_week': 5,\n",
       "             b'with': 4912,\n",
       "             b'week_with': 5,\n",
       "             b'inspiring': 18,\n",
       "             b'with_inspiring': 1,\n",
       "             b'art': 17,\n",
       "             b'inspiring_art': 1,\n",
       "             b'check': 127,\n",
       "             b'art_check': 1,\n",
       "             b'out': 1072,\n",
       "             b'check_out': 60,\n",
       "             b'out_the': 47,\n",
       "             b'beautiful': 109,\n",
       "             b'the_beautiful': 11,\n",
       "             b'creations': 1,\n",
       "             b'beautiful_creations': 1,\n",
       "             b'by': 3588,\n",
       "             b'creations_by': 1,\n",
       "             b'young': 213,\n",
       "             b'by_young': 4,\n",
       "             b'people': 3057,\n",
       "             b'young_people': 15,\n",
       "             b'people_in': 289,\n",
       "             b'in_lebanon': 9707,\n",
       "             b'who': 2263,\n",
       "             b'lebanon_who': 165,\n",
       "             b'are': 5764,\n",
       "             b'who_are': 236,\n",
       "             b'proving': 9,\n",
       "             b'are_proving': 2,\n",
       "             b'proving_in': 1,\n",
       "             b'their': 3363,\n",
       "             b'in_their': 199,\n",
       "             b'own': 341,\n",
       "             b'their_own': 144,\n",
       "             b'own_way': 2,\n",
       "             b'that': 4272,\n",
       "             b'way_that': 6,\n",
       "             b'everyone': 174,\n",
       "             b'that_everyone': 7,\n",
       "             b'counts': 22,\n",
       "             b'everyone_counts': 1,\n",
       "             b'counts_in': 4,\n",
       "             b'fight': 196,\n",
       "             b'the_fight': 10,\n",
       "             b'against': 462,\n",
       "             b'fight_against': 10,\n",
       "             b'covid': 1996,\n",
       "             b'against_covid': 12,\n",
       "             b'19': 1846,\n",
       "             b'covid_19': 1805,\n",
       "             b'including': 351,\n",
       "             b'19_including': 1,\n",
       "             b'including_refugees': 33,\n",
       "             b'refugees_you': 30,\n",
       "             b'too': 326,\n",
       "             b'you_too': 3,\n",
       "             b'can': 1351,\n",
       "             b'too_can': 1,\n",
       "             b'submit': 5,\n",
       "             b'can_submit': 1,\n",
       "             b'yours': 12,\n",
       "             b'submit_yours': 1,\n",
       "             b'identify': 16,\n",
       "             b'we_identify': 1,\n",
       "             b'as': 3192,\n",
       "             b'identify_as': 5,\n",
       "             b'as_that': 5,\n",
       "             b'that_people': 12,\n",
       "             b'people_because': 3,\n",
       "             b'literally': 79,\n",
       "             b'we_literally': 2,\n",
       "             b'literally_are': 1,\n",
       "             b'are_that': 4,\n",
       "             b'just': 748,\n",
       "             b'people_just': 7,\n",
       "             b'just_as': 18,\n",
       "             b'modern': 80,\n",
       "             b'as_modern': 2,\n",
       "             b'day': 622,\n",
       "             b'modern_day': 40,\n",
       "             b'day_lebanese': 4,\n",
       "             b'lebanese_identify': 1,\n",
       "             b'phoenicians': 4,\n",
       "             b'as_phoenicians': 1,\n",
       "             b'phoenicians_and': 2,\n",
       "             b'and_modern': 4,\n",
       "             b'iranians': 6,\n",
       "             b'day_iranians': 1,\n",
       "             b'iranians_identify': 1,\n",
       "             b'persians': 3,\n",
       "             b'as_persians': 1,\n",
       "             b'persians_the': 1,\n",
       "             b'only': 647,\n",
       "             b'the_only': 109,\n",
       "             b'difference': 81,\n",
       "             b'only_difference': 3,\n",
       "             b'being': 640,\n",
       "             b'difference_being': 1,\n",
       "             b'being_we': 1,\n",
       "             b'were': 1524,\n",
       "             b'we_were': 52,\n",
       "             b'displaced': 1241,\n",
       "             b'were_displaced': 31,\n",
       "             b'from': 4205,\n",
       "             b'displaced_from': 88,\n",
       "             b'our': 1790,\n",
       "             b'from_our': 39,\n",
       "             b'land': 257,\n",
       "             b'our_land': 12,\n",
       "             b'land_for': 1,\n",
       "             b'generations': 37,\n",
       "             b'for_generations': 8,\n",
       "             b'generations_that': 2,\n",
       "             b'doesn': 100,\n",
       "             b'that_doesn': 11,\n",
       "             b't': 1062,\n",
       "             b'doesn_t': 100,\n",
       "             b'negate': 3,\n",
       "             b't_negate': 1,\n",
       "             b'negate_our': 1,\n",
       "             b'identity': 45,\n",
       "             b'our_identity': 3,\n",
       "             b'one': 1000,\n",
       "             b'identity_one': 1,\n",
       "             b'iota': 2,\n",
       "             b'one_iota': 1,\n",
       "             b'civil': 375,\n",
       "             b'the_civil': 61,\n",
       "             b'war': 851,\n",
       "             b'civil_war': 242,\n",
       "             b'war_in': 123,\n",
       "             b'lebanon_was': 322,\n",
       "             b'was_in': 46,\n",
       "             b'1970s': 25,\n",
       "             b'the_1970s': 23,\n",
       "             b'typical': 7,\n",
       "             b'1970s_typical': 1,\n",
       "             b'white': 147,\n",
       "             b'typical_white': 1,\n",
       "             b'person': 109,\n",
       "             b'white_person': 3,\n",
       "             b'comment': 24,\n",
       "             b'person_comment': 1,\n",
       "             b'comment_im': 1,\n",
       "             b'im_not': 31,\n",
       "             b'surprised': 19,\n",
       "             b'not_surprised': 3,\n",
       "             b'surprised_just': 1,\n",
       "             b'shocked': 18,\n",
       "             b'just_shocked': 1,\n",
       "             b'shocked_that': 1,\n",
       "             b'hes': 34,\n",
       "             b'that_hes': 2,\n",
       "             b'an': 1444,\n",
       "             b'hes_an': 1,\n",
       "             b'australian': 22,\n",
       "             b'an_australian': 2,\n",
       "             b'reporter': 12,\n",
       "             b'australian_reporter': 1,\n",
       "             b'reporter_and': 1,\n",
       "             b'would': 661,\n",
       "             b'and_would': 10,\n",
       "             b'be': 1971,\n",
       "             b'would_be': 192,\n",
       "             b'well': 416,\n",
       "             b'be_well': 3,\n",
       "             b'aware': 32,\n",
       "             b'well_aware': 1,\n",
       "             b'aware_of': 17,\n",
       "             b'of_the': 2733,\n",
       "             b'strong': 58,\n",
       "             b'the_strong': 3,\n",
       "             b'prescence': 1,\n",
       "             b'strong_prescence': 1,\n",
       "             b'prescence_of': 1,\n",
       "             b'of_lebanese': 557,\n",
       "             b'lebanese_migrant': 29,\n",
       "             b'migrant_and': 13,\n",
       "             b'and_australian': 1,\n",
       "             b'born': 254,\n",
       "             b'australian_born': 2,\n",
       "             b'citizens': 303,\n",
       "             b'born_citizens': 1,\n",
       "             b'citizens_in': 21,\n",
       "             b'in_this': 161,\n",
       "             b'this_country': 55,\n",
       "             b'agree': 75,\n",
       "             b'i_agree': 31,\n",
       "             b'agree_i': 2,\n",
       "             b'hear': 110,\n",
       "             b'i_hear': 11,\n",
       "             b'hear_that': 5,\n",
       "             b'often': 87,\n",
       "             b'that_often': 2,\n",
       "             b'often_from': 1,\n",
       "             b'primary': 12,\n",
       "             b'from_primary': 2,\n",
       "             b'high': 270,\n",
       "             b'primary_high': 1,\n",
       "             b'school': 195,\n",
       "             b'high_school': 9,\n",
       "             b'kids': 141,\n",
       "             b'school_kids': 1,\n",
       "             b'descended': 8,\n",
       "             b'kids_descended': 1,\n",
       "             b'descended_from': 6,\n",
       "             b'from_lebanese': 68,\n",
       "             b'italian': 41,\n",
       "             b'lebanese_italian': 3,\n",
       "             b'etc': 182,\n",
       "             b'italian_etc': 1,\n",
       "             b'immigrants': 1269,\n",
       "             b'etc_immigrants': 1,\n",
       "             b'immigrants_who': 62,\n",
       "             b'who_came': 34,\n",
       "             b'here': 735,\n",
       "             b'came_here': 19,\n",
       "             b'80': 55,\n",
       "             b'here_80': 1,\n",
       "             b'80_to': 3,\n",
       "             b'100': 138,\n",
       "             b'to_100': 1,\n",
       "             b'years': 877,\n",
       "             b'100_years': 10,\n",
       "             b'ago': 297,\n",
       "             b'years_ago': 171,\n",
       "             b'how': 1124,\n",
       "             b'ago_how': 2,\n",
       "             b'long': 231,\n",
       "             b'how_long': 26,\n",
       "             b'does': 220,\n",
       "             b'long_does': 1,\n",
       "             b'it': 3087,\n",
       "             b'does_it': 15,\n",
       "             b'take': 609,\n",
       "             b'it_take': 4,\n",
       "             b'besides': 15,\n",
       "             b'take_besides': 1,\n",
       "             b'besides_how': 1,\n",
       "             b'how_many': 121,\n",
       "             b'many_lebanese': 75,\n",
       "             b'lebanese_are': 124,\n",
       "             b'blonde': 1,\n",
       "             b'are_blonde': 1,\n",
       "             b'blue': 6,\n",
       "             b'blonde_blue': 1,\n",
       "             b'eyed': 2,\n",
       "             b'blue_eyed': 1,\n",
       "             b'like': 1051,\n",
       "             b'eyed_like': 1,\n",
       "             b'like_a': 16,\n",
       "             b'number': 380,\n",
       "             b'a_number': 28,\n",
       "             b'number_i': 1,\n",
       "             b'know': 580,\n",
       "             b'i_know': 78,\n",
       "             b'confinement': 5,\n",
       "             b'measures': 44,\n",
       "             b'confinement_measures': 1,\n",
       "             b'measures_have': 2,\n",
       "             b'exacerbated': 36,\n",
       "             b'have_exacerbated': 3,\n",
       "             b'mental': 99,\n",
       "             b'exacerbated_mental': 1,\n",
       "             b'health': 514,\n",
       "             b'mental_health': 83,\n",
       "             b'issues': 99,\n",
       "             b'health_issues': 4,\n",
       "             b'worldwide': 48,\n",
       "             b'issues_worldwide': 1,\n",
       "             b'but': 1723,\n",
       "             b'worldwide_but': 1,\n",
       "             b'they': 3150,\n",
       "             b'but_they': 110,\n",
       "             b'they_are': 488,\n",
       "             b'particularly': 72,\n",
       "             b'are_particularly': 9,\n",
       "             b'noticeable': 1,\n",
       "             b'particularly_noticeable': 1,\n",
       "             b'among': 306,\n",
       "             b'noticeable_among': 1,\n",
       "             b'lebanons': 207,\n",
       "             b'among_lebanons': 3,\n",
       "             b'refugee': 5318,\n",
       "             b'lebanons_refugee': 9,\n",
       "             b'population': 560,\n",
       "             b'refugee_population': 70,\n",
       "             b'syrians': 609,\n",
       "             b'population_syrians': 1,\n",
       "             b'syrians_came': 1,\n",
       "             b'to_lebanon': 670,\n",
       "             b'lebanon_for': 275,\n",
       "             b'safety': 129,\n",
       "             b'for_safety': 9,\n",
       "             b'safety_and': 19,\n",
       "             b'and_it': 72,\n",
       "             b'doesnt': 93,\n",
       "             b'it_doesnt': 13,\n",
       "             b'give': 258,\n",
       "             b'doesnt_give': 2,\n",
       "             b'give_it': 5,\n",
       "             b'it_all': 27,\n",
       "             b'via': 4230,\n",
       "             b'all_via': 1,\n",
       "             b'thenationaluae': 15,\n",
       "             b'via_thenationaluae': 13,\n",
       "             b'mrs': 11,\n",
       "             b'read_mrs': 1,\n",
       "             b'there': 1404,\n",
       "             b'mrs_there': 1,\n",
       "             b'is': 6447,\n",
       "             b'there_is': 205,\n",
       "             b'is_a': 702,\n",
       "             b'lot': 229,\n",
       "             b'a_lot': 211,\n",
       "             b'lot_of': 141,\n",
       "             b'material': 16,\n",
       "             b'of_material': 1,\n",
       "             b'material_on': 2,\n",
       "             b'on_the': 1056,\n",
       "             b'web': 14,\n",
       "             b'the_web': 1,\n",
       "             b'visit': 163,\n",
       "             b'web_visit': 1,\n",
       "             b'israel': 857,\n",
       "             b'visit_israel': 5,\n",
       "             b'talk': 125,\n",
       "             b'israel_talk': 1,\n",
       "             b'talk_to': 19,\n",
       "             b'to_the': 1337,\n",
       "             b'the_people': 322,\n",
       "             b'people_talk': 2,\n",
       "             b'israeli': 507,\n",
       "             b'the_israeli': 195,\n",
       "             b'arabs': 98,\n",
       "             b'israeli_arabs': 5,\n",
       "             b'ask': 95,\n",
       "             b'arabs_ask': 1,\n",
       "             b'them': 1788,\n",
       "             b'ask_them': 5,\n",
       "             b'if': 1168,\n",
       "             b'them_if': 6,\n",
       "             b'if_they': 81,\n",
       "             b'they_live': 35,\n",
       "             b'badly': 39,\n",
       "             b'live_badly': 1,\n",
       "             b'badly_talk': 1,\n",
       "             b'druze': 21,\n",
       "             b'the_druze': 3,\n",
       "             b'druze_talk': 1,\n",
       "             b'the_lebanese': 1197,\n",
       "             b'christian': 371,\n",
       "             b'lebanese_christian': 148,\n",
       "             b'christian_refugees': 22,\n",
       "             b'refugees_how': 15,\n",
       "             b'do': 1028,\n",
       "             b'how_do': 20,\n",
       "             b'do_they': 25,\n",
       "             b'live_there': 24,\n",
       "             b'there_a': 9,\n",
       "             b'crime': 73,\n",
       "             b'a_crime': 24,\n",
       "             b'crime_is': 3,\n",
       "             b'is_to': 87,\n",
       "             b'reject': 21,\n",
       "             b'to_reject': 1,\n",
       "             b'reject_all': 1,\n",
       "             b'all_the': 263,\n",
       "             b'peace': 275,\n",
       "             b'the_peace': 6,\n",
       "             b'schemes': 3,\n",
       "             b'peace_schemes': 1,\n",
       "             b'proposed': 4,\n",
       "             b'schemes_proposed': 1,\n",
       "             b'proposed_by': 1,\n",
       "             b'by_israel': 60,\n",
       "             b'protect': 181,\n",
       "             b'those': 753,\n",
       "             b'protect_those': 3,\n",
       "             b'those_migrant': 2,\n",
       "             b'workers_i': 5,\n",
       "             b'i_tweet': 2,\n",
       "             b'tweet_and': 3,\n",
       "             b'and_have': 64,\n",
       "             b'discussions': 9,\n",
       "             b'have_discussions': 1,\n",
       "             b'about': 1620,\n",
       "             b'discussions_about': 3,\n",
       "             b'about_lebanon': 46,\n",
       "             b'lebanon_and': 1411,\n",
       "             b'its': 1241,\n",
       "             b'and_its': 82,\n",
       "             b'toxic': 5,\n",
       "             b'its_toxic': 1,\n",
       "             b'nationality': 49,\n",
       "             b'toxic_nationality': 1,\n",
       "             b'nationality_too': 1,\n",
       "             b'idc': 4,\n",
       "             b'too_idc': 1,\n",
       "             b'idc_i': 1,\n",
       "             b'will': 1549,\n",
       "             b'i_will': 70,\n",
       "             b'call': 239,\n",
       "             b'will_call': 1,\n",
       "             b'call_it': 9,\n",
       "             b'it_out': 19,\n",
       "             b'out_i': 2,\n",
       "             b'i_call': 3,\n",
       "             b'call_out': 4,\n",
       "             b'american': 161,\n",
       "             b'out_american': 1,\n",
       "             b'prison': 19,\n",
       "             b'american_prison': 1,\n",
       "             b'systems': 22,\n",
       "             b'prison_systems': 1,\n",
       "             b'systems_as': 1,\n",
       "             b'slavery': 97,\n",
       "             b'modern_slavery': 10,\n",
       "             b'slavery_too': 1,\n",
       "             b'too_do': 1,\n",
       "             b'do_your': 13,\n",
       "             b'part': 303,\n",
       "             b'your_part': 2,\n",
       "             b'part_and': 2,\n",
       "             b'stop': 315,\n",
       "             b'and_stop': 14,\n",
       "             b'crying': 14,\n",
       "             b'stop_crying': 1,\n",
       "             b'crying_because': 3,\n",
       "             b'someone': 88,\n",
       "             b'because_someone': 1,\n",
       "             b'someone_is': 5,\n",
       "             b'calling': 88,\n",
       "             b'is_calling': 16,\n",
       "             b'calling_the': 8,\n",
       "             b'uae': 60,\n",
       "             b'the_uae': 14,\n",
       "             b'uae_out': 1,\n",
       "             b'using': 67,\n",
       "             b'using_their': 1,\n",
       "             b'money': 377,\n",
       "             b'their_money': 20,\n",
       "             b'aid': 1594,\n",
       "             b'money_aid': 2,\n",
       "             b'aid_to': 127,\n",
       "             b'purchase': 9,\n",
       "             b'to_purchase': 5,\n",
       "             b'power': 99,\n",
       "             b'purchase_power': 1,\n",
       "             b'power_and': 7,\n",
       "             b'bias': 7,\n",
       "             b'and_bias': 1,\n",
       "             b'bias_to': 1,\n",
       "             b'line': 118,\n",
       "             b'to_line': 1,\n",
       "             b'line_their': 3,\n",
       "             b'pockets': 7,\n",
       "             b'their_pockets': 2,\n",
       "             b'pockets_when': 1,\n",
       "             b'when_the': 102,\n",
       "             b'subject': 37,\n",
       "             b'the_subject': 5,\n",
       "             b'subject_matter': 1,\n",
       "             b'matter_is': 2,\n",
       "             b'is_not': 287,\n",
       "             b'islamic': 45,\n",
       "             b'not_islamic': 1,\n",
       "             b'killing': 85,\n",
       "             b'islamic_killing': 1,\n",
       "             b'yemenis': 6,\n",
       "             b'killing_yemenis': 1,\n",
       "             b'ignoring': 87,\n",
       "             b'yemenis_ignoring': 1,\n",
       "             b'palestinians': 878,\n",
       "             b'ignoring_palestinians': 1,\n",
       "             b'palestinians_ignoring': 1,\n",
       "             b'syria': 2096,\n",
       "             b'ignoring_syria': 1,\n",
       "             b'allowing': 38,\n",
       "             b'syria_allowing': 1,\n",
       "             b'allowing_the': 11,\n",
       "             b'the_much': 2,\n",
       "             b'poorer': 14,\n",
       "             b'much_poorer': 3,\n",
       "             b'poorer_lebanon': 1,\n",
       "             b'jordan': 926,\n",
       "             b'lebanon_jordan': 74,\n",
       "             b'jordan_and': 156,\n",
       "             b'turkey': 644,\n",
       "             b'and_turkey': 94,\n",
       "             b'turkey_to': 46,\n",
       "             b'to_take': 223,\n",
       "             b'take_the': 48,\n",
       "             b'millions': 306,\n",
       "             b'the_millions': 29,\n",
       "             b'millions_of': 208,\n",
       "             b'refugees_our': 13,\n",
       "             b'brothers': 46,\n",
       "             b'our_brothers': 5,\n",
       "             b'migrants': 1944,\n",
       "             b'working': 335,\n",
       "             b'migrants_working': 2,\n",
       "             b'working_in': 60,\n",
       "             b'suffer': 84,\n",
       "             b'lebanon_suffer': 13,\n",
       "             b'suffer_under': 1,\n",
       "             b'under_the': 163,\n",
       "             b'economic': 874,\n",
       "             b'the_economic': 126,\n",
       "             b'downturn': 20,\n",
       "             b'economic_downturn': 20,\n",
       "             b'downturn_of': 1,\n",
       "             b'country_as': 20,\n",
       "             b'as_well': 198,\n",
       "             b'well_as': 108,\n",
       "             b'new': 715,\n",
       "             b'as_new': 2,\n",
       "             b'daily': 137,\n",
       "             b'new_daily': 2,\n",
       "             b'realities': 8,\n",
       "             b'daily_realities': 1,\n",
       "             b'realities_under': 1,\n",
       "             b'ongoing': 112,\n",
       "             b'the_ongoing': 19,\n",
       "             b'ongoing_covid': 2,\n",
       "             b'pandemic': 365,\n",
       "             b'19_pandemic': 91,\n",
       "             b'however': 58,\n",
       "             b'pandemic_however': 1,\n",
       "             b'however_many': 2,\n",
       "             b'many_can': 3,\n",
       "             b'can_t': 170,\n",
       "             b'leave': 226,\n",
       "             b't_leave': 2,\n",
       "             b'leave_the': 36,\n",
       "             b'country_and': 93,\n",
       "             b'return': 738,\n",
       "             b'and_return': 9,\n",
       "             b'home': 823,\n",
       "             b'return_home': 103,\n",
       "             b'either': 67,\n",
       "             b'home_either': 1,\n",
       "             b'either_at': 1,\n",
       "             b'least': 255,\n",
       "             b'at_least': 232,\n",
       "             b'least_not': 2,\n",
       "             b'without': 390,\n",
       "             b'not_without': 1,\n",
       "             b'without_their': 7,\n",
       "             b'employers': 162,\n",
       "             b'their_employers': 59,\n",
       "             b'consent': 3,\n",
       "             b'employers_consent': 1,\n",
       "             b'global': 1074,\n",
       "             b'the_global': 23,\n",
       "             b'global_coronavirus': 2,\n",
       "             b'coronavirus_pandemic': 34,\n",
       "             b'has': 2162,\n",
       "             b'pandemic_has': 11,\n",
       "             b'has_only': 15,\n",
       "             b'only_exacerbated': 1,\n",
       "             b'exacerbated_the': 9,\n",
       "             b'pre': 35,\n",
       "             b'the_pre': 2,\n",
       "             b'existing': 29,\n",
       "             b'pre_existing': 7,\n",
       "             b'crisis': 2088,\n",
       "             b'existing_crisis': 1,\n",
       "             b'though': 74,\n",
       "             b'crisis_though': 1,\n",
       "             b'though_lebanon': 2,\n",
       "             b'lebanon_has': 498,\n",
       "             b'been': 1225,\n",
       "             b'has_been': 318,\n",
       "             b'relatively': 14,\n",
       "             b'been_relatively': 1,\n",
       "             b'spared': 5,\n",
       "             b'relatively_spared': 1,\n",
       "             b'spared_with': 1,\n",
       "             b'26': 26,\n",
       "             b'with_26': 1,\n",
       "             b'deaths': 100,\n",
       "             b'26_deaths': 1,\n",
       "             b'deaths_and': 8,\n",
       "             b'1172': 1,\n",
       "             b'and_1172': 1,\n",
       "             b'confirmed': 55,\n",
       "             b'1172_confirmed': 1,\n",
       "             b'cases': 117,\n",
       "             b'confirmed_cases': 1,\n",
       "             b'cases_including': 1,\n",
       "             b'16': 155,\n",
       "             b'including_16': 1,\n",
       "             b'16_among': 1,\n",
       "             b'syrian': 5383,\n",
       "             b'among_syrian': 23,\n",
       "             b'syrian_refugees': 2945,\n",
       "             b'refugees_read': 14,\n",
       "             b'read_more': 158,\n",
       "             b'below': 153,\n",
       "             b'more_below': 12,\n",
       "             b'mr': 56,\n",
       "             b'martin': 9,\n",
       "             b'mr_martin': 1,\n",
       "             b'martin_i': 1,\n",
       "             b'already': 405,\n",
       "             b'i_already': 3,\n",
       "             b'sent': 148,\n",
       "             b'already_sent': 3,\n",
       "             b'sent_a': 3,\n",
       "             b'message': 78,\n",
       "             b'a_message': 8,\n",
       "             b'message_to': 17,\n",
       "             b'red': 114,\n",
       "             b'the_red': 23,\n",
       "             b'cross': 135,\n",
       "             b'red_cross': 86,\n",
       "             b'cross_to': 6,\n",
       "             b'geneva': 37,\n",
       "             b'to_geneva': 1,\n",
       "             b'actually': 121,\n",
       "             b'geneva_actually': 1,\n",
       "             b'actually_mr': 1,\n",
       "             b'matthew': 2,\n",
       "             b'mr_matthew': 1,\n",
       "             b'called': 198,\n",
       "             b'matthew_called': 1,\n",
       "             b'me': 687,\n",
       "             b'called_me': 35,\n",
       "             b'me_from': 6,\n",
       "             b'from_lebanon': 621,\n",
       "             b'lebanon_but': 88,\n",
       "             b'but_i': 104,\n",
       "             b'i_did': 18,\n",
       "             b'did_not': 70,\n",
       "             b'find': 283,\n",
       "             b'not_find': 47,\n",
       "             b'find_a': 35,\n",
       "             b'result': 71,\n",
       "             b'a_result': 45,\n",
       "             b'result_i': 1,\n",
       "             b'am_a': 127,\n",
       "             b'a_refugee': 449,\n",
       "             b'refugee_in': 330,\n",
       "             b'previously': 13,\n",
       "             b'and_previously': 2,\n",
       "             b'previously_i': 1,\n",
       "             b'i_was': 195,\n",
       "             b'cross_how': 1,\n",
       "             b'how_to': 61,\n",
       "             b'contact': 31,\n",
       "             b'to_contact': 5,\n",
       "             b'contact_you': 1,\n",
       "             b'2': 632,\n",
       "             b'2_how': 1,\n",
       "             b'dare': 13,\n",
       "             b'how_dare': 5,\n",
       "             b'dare_you': 3,\n",
       "             b'you_have': 85,\n",
       "             b'have_the': 93,\n",
       "             b'audacity': 7,\n",
       "             b'the_audacity': 6,\n",
       "             b'audacity_to': 5,\n",
       "             b'think': 343,\n",
       "             b'to_think': 29,\n",
       "             b'think_you': 15,\n",
       "             b'understand': 87,\n",
       "             b'you_understand': 3,\n",
       "             b'understand_the': 13,\n",
       "             b'pain': 66,\n",
       "             b'the_pain': 21,\n",
       "             b'pain_when': 2,\n",
       "             b'most': 794,\n",
       "             b'when_most': 2,\n",
       "             b'countries': 924,\n",
       "             b'most_countries': 5,\n",
       "             b'countries_in': 43,\n",
       "             b'region': 194,\n",
       "             b'the_region': 106,\n",
       "             b'region_including': 2,\n",
       "             b'including_lebanon': 6,\n",
       "             b'have_an': 24,\n",
       "             b'exploitative': 24,\n",
       "             b'an_exploitative': 9,\n",
       "             b'system': 461,\n",
       "             b'exploitative_system': 15,\n",
       "             b'system_for': 13,\n",
       "             b'for_migrant': 2991,\n",
       "             b'kafala': 3207,\n",
       "             b'workers_kafala': 5,\n",
       "             b'3': 435,\n",
       "             b'kafala_3': 1,\n",
       "             b'3_lebanon': 3,\n",
       "             b'has_a': 114,\n",
       "             b'a_long': 57,\n",
       "             b'track': 9,\n",
       "             b'long_track': 1,\n",
       "             b'record': 40,\n",
       "             b'track_record': 2,\n",
       "             b'record_of': 3,\n",
       "             b'abuse': 155,\n",
       "             b'of_abuse': 17,\n",
       "             b'abuse_and': 18,\n",
       "             b'borderline': 7,\n",
       "             b'and_borderline': 7,\n",
       "             b'borderline_slavery': 1,\n",
       "             b'slavery_with': 1,\n",
       "             b'with_migrant': 12,\n",
       "             b'donate': 383,\n",
       "             b'donate_for': 5,\n",
       "             b'west': 179,\n",
       "             b'the_west': 98,\n",
       "             b'bank': 151,\n",
       "             b'west_bank': 72,\n",
       "             b'gaza': 205,\n",
       "             b'bank_gaza': 12,\n",
       "             b'strip': 14,\n",
       "             b'gaza_strip': 10,\n",
       "             b'strip_and': 4,\n",
       "             b'and_refugee': 88,\n",
       "             b'camps': 1289,\n",
       "             b'refugee_camps': 862,\n",
       "             b'camps_in': 469,\n",
       "             b'and_jordan': 219,\n",
       "             b'jordan_to': 13,\n",
       "             b'provide': 1206,\n",
       "             b'to_provide': 169,\n",
       "             b'urgent': 135,\n",
       "             b'provide_urgent': 5,\n",
       "             b'humanitarian': 434,\n",
       "             b'urgent_humanitarian': 7,\n",
       "             b'relief': 235,\n",
       "             b'humanitarian_relief': 8,\n",
       "             b'relief_for': 13,\n",
       "             b'for_at': 2,\n",
       "             b'risk': 120,\n",
       "             b'at_risk': 40,\n",
       "             b'communities': 417,\n",
       "             b'risk_communities': 1,\n",
       "             b'communities_and': 51,\n",
       "             b'hospitals': 77,\n",
       "             b'and_hospitals': 9,\n",
       "             b'hospitals_by': 1,\n",
       "             b'providing': 170,\n",
       "             b'by_providing': 12,\n",
       "             b'providing_them': 8,\n",
       "             b'food': 645,\n",
       "             b'them_food': 1,\n",
       "             b'infection': 10,\n",
       "             b'food_infection': 1,\n",
       "             b'control': 60,\n",
       "             b'infection_control': 1,\n",
       "             b'supplies': 98,\n",
       "             b'control_supplies': 1,\n",
       "             b'medical': 179,\n",
       "             b'supplies_medical': 1,\n",
       "             b'equipment': 17,\n",
       "             b'medical_equipment': 6,\n",
       "             b'equipment_and': 2,\n",
       "             b'building': 69,\n",
       "             b'and_building': 3,\n",
       "             b'building_new': 2,\n",
       "             b'departments': 2,\n",
       "             b'new_departments': 1,\n",
       "             b'study': 70,\n",
       "             b'finds': 42,\n",
       "             b'study_finds': 5,\n",
       "             b'ancient': 22,\n",
       "             b'finds_ancient': 2,\n",
       "             b'canaanites': 9,\n",
       "             b'ancient_canaanites': 3,\n",
       "             b'genetically': 2,\n",
       "             b'canaanites_genetically': 2,\n",
       "             b'linked': 28,\n",
       "             b'genetically_linked': 2,\n",
       "             b'linked_to': 25,\n",
       "             b'to_modern': 13,\n",
       "             b'populations': 66,\n",
       "             b'modern_populations': 2,\n",
       "             b'when_refugees': 4,\n",
       "             b'fear': 103,\n",
       "             b'refugees_fear': 1,\n",
       "             b'fear_racism': 1,\n",
       "             b'harassment': 18,\n",
       "             b'racism_harassment': 1,\n",
       "             b'harassment_and': 6,\n",
       "             b'and_the': 998,\n",
       "             b'possibility': 8,\n",
       "             b'the_possibility': 6,\n",
       "             b'possibility_of': 4,\n",
       "             b'deportation': 29,\n",
       "             b'of_deportation': 2,\n",
       "             b'deportation_they': 1,\n",
       "             b'less': 133,\n",
       "             b'are_less': 2,\n",
       "             ...})"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases#, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "#tweet_corpus_tokens = [tweet.split() for tweet in tweet_corpus]\n",
    "#tweet_corpus_tokens\n",
    "bigram = Phrases(tweet_corpus, min_count=20, threshold=20) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[tweet_corpus_tokens], threshold=20) \n",
    "\n",
    "\n",
    "trigram.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for params size=100,window=5\n",
      "[('refugee_camps', 0.7530064582824707), ('exceed', 0.6699388027191162), ('camps', 0.648147702217102), ('refugee', 0.6430160403251648), ('15_mil', 0.6423872113227844), ('89', 0.6361641883850098), ('taken_refuge', 0.6245043277740479), ('percentage', 0.6201708316802979), ('forcibly_exiled', 0.6079879999160767), ('15_million', 0.6018157005310059)]\n",
      "\n",
      "for params size=100,window=7\n",
      "[('refugee_camps', 0.7207822799682617), ('sheltered', 0.6638866662979126), ('1960s', 0.6482441425323486), ('15_mil', 0.6410447359085083), ('refugee', 0.6318071484565735), ('camps', 0.6107119917869568), ('not_beignored', 0.595453679561615), ('poverty', 0.5937148332595825), ('report_trapped', 0.5917030572891235), ('15_million', 0.590601921081543)]\n",
      "\n",
      "for params size=200,window=5\n",
      "[('refugee_camps', 0.7206195592880249), ('camps', 0.6818684339523315), ('15_mil', 0.6752337217330933), ('sheltered', 0.6639184355735779), ('refugee', 0.6545926928520203), ('15_million', 0.6437801122665405), ('currently_residing', 0.6205162405967712), ('jordan_amounted', 0.619218111038208), ('tented_settlements', 0.6150147318840027), ('jordan', 0.613725483417511)]\n",
      "\n",
      "for params size=200,window=7\n",
      "[('refugee_camps', 0.7383606433868408), ('sheltered', 0.6890142560005188), ('89', 0.6663094758987427), ('poverty', 0.6368516683578491), ('refugee', 0.6351296901702881), ('forcibly_exiled', 0.6195415258407593), ('as_well', 0.6101706027984619), ('camps', 0.6064353585243225), ('report_trapped', 0.5886334180831909), ('west_bank', 0.5786362886428833)]\n",
      "\n",
      "for params size=300,window=5\n",
      "[('refugee_camps', 0.7783056497573853), ('sheltered', 0.7397201061248779), ('not_beignored', 0.6963502764701843), ('camps', 0.6619561314582825), ('refugee', 0.636478066444397), ('15_mil', 0.6191922426223755), ('as_well', 0.612144947052002), ('poverty', 0.6085872054100037), ('second_class', 0.5913479328155518), ('extreme_poverty', 0.5910264253616333)]\n",
      "\n",
      "for params size=300,window=7\n",
      "[('refugee_camps', 0.7638648748397827), ('taken_refuge', 0.667212188243866), ('15_mil', 0.621514081954956), ('refugee', 0.620859682559967), ('neighbouring_countries', 0.6157338619232178), ('extreme_poverty', 0.608931839466095), ('15_million', 0.6072452068328857), ('90_percent', 0.6010550260543823), ('arab_countries', 0.5997676253318787), ('percentage', 0.5979204177856445)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "model_phrases = [trigram[tweet.split()] for tweet in tweet_corpus]\n",
    "#print(model_phrases[:10])\n",
    "\n",
    "#negatives = [5,10,20]\n",
    "sizes = [100,200,300]\n",
    "sgs=[0,1]\n",
    "windows =[5,7] \n",
    "#cbow_means = [0,1]\n",
    "#iters=[10]\n",
    "\n",
    "\n",
    "\n",
    "for size in sizes:\n",
    "        for window in windows:\n",
    "            #print(f'\\nfor params size={size},negative={neg},sg={sg},hs={hs},window={window},cbow_mean={cbow},iter={it}')\n",
    "            print(f'\\nfor params size={size},window={window}')\n",
    "            model = Word2Vec(model_phrases,size=size,window=window)\n",
    "            print(model.wv.most_similar('refugees'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('refugee_camps', 0.7634189128875732),\n",
       " ('sheltered', 0.7147684097290039),\n",
       " ('camps', 0.6536272764205933),\n",
       " ('difficult_conditions', 0.6474202871322632),\n",
       " ('refugee', 0.628537654876709),\n",
       " ('poverty', 0.6162228584289551),\n",
       " ('forcibly_exiled', 0.6024370193481445),\n",
       " ('15_million', 0.5982092618942261),\n",
       " ('percentage', 0.5728586912155151),\n",
       " ('arab_countries', 0.571622908115387)]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(model_phrases,size=300,window=7)\n",
    "model.wv.most_similar('refugees')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. We instantiate stanza english language module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-11 16:51:49 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-04-11 16:51:49 INFO: Use device: cpu\n",
      "2021-04-11 16:51:49 INFO: Loading: tokenize\n",
      "2021-04-11 16:51:49 INFO: Loading: pos\n",
      "2021-04-11 16:51:50 INFO: Loading: lemma\n",
      "2021-04-11 16:51:50 INFO: Loading: depparse\n",
      "2021-04-11 16:51:51 INFO: Loading: sentiment\n",
      "2021-04-11 16:51:52 INFO: Loading: ner\n",
      "2021-04-11 16:51:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ needed when running first time ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "\n",
    "#stanza.download(\"en\")\n",
    "\n",
    "#stanza.install_corenlp()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# loading the pipeline\n",
    "en_nlp = stanza.Pipeline(\"en\", tokenize_pretokenized=True, ner_batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle_file(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event_data(event_name):\n",
    "    assert event_name in ['moria','tigray','channel','all','beirut'], f\"Oh no! We do not analyze {event_name} event\"\n",
    "    \n",
    "    print(f'Loading {event_name} data...')\n",
    "    try:\n",
    "        #sample = 2000\n",
    "        event_np_list = load_pickle(event_name + '_np_list')#[1000:sample]\n",
    "        event_crf_list = load_pickle(event_name + '_crf_list')#[1000:sample]\n",
    "        event_tagged_tweets = load_pickle(event_name + '_tagged_tweets')#[1000:sample]\n",
    "        \n",
    "        return event_np_list,event_crf_list,event_tagged_tweets\n",
    "    except:\n",
    "        print(f'The {event_name} files not found! Run candidate_extraction.py file on the {eventname}_df')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pipeline(event_name):\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 1. LOAD THE DATA ~~~~~~~~~~~~~~~~~~~~~\n",
    "    event_np_list,event_crf_list,event_tagged_tweets = load_event_data(event_name)\n",
    "    \n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 2. GET POS AND NER TAGS ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # get easily accessible list of tuples (POS-tags of each word, NER-tags of each named entity) \n",
    "    tweet_tags = cand_prep.get_tweet_tags(event_tagged_tweets) \n",
    "    \n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 3. PREPROCESS CANDIDATES ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ~~~~~~~~~~~~ processing of noun phrases ~~~~~~~~~~~~~~~~~~~~~\n",
    "    print(f'Processing {event_name} noun phrase candidates...')\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    # remove NP candidates longer than threshold and remove all child NPs of parent NPs\n",
    "    event_np_list = cand_prep.remove_long_nps(event_np_list)\n",
    "    event_np_list = cand_prep.remove_child_nps(event_np_list) \n",
    "    #event_np_list = remove_weird_chars(event_np_list)\n",
    "    event_np_list = cand_prep.remove_char(event_np_list,'@')\n",
    "\n",
    "    event_np_list = [['no_candidate'] if len(noun_ps)==0 or noun_ps ==' ' else noun_ps for noun_ps in event_np_list ]\n",
    "    \n",
    "    #print(event_np_list)\n",
    "    print(f'Tagging {event_name} noun phrase candidates...')\n",
    "    #tag all tweets and save them in a list    \n",
    "\n",
    "    #tagged_np_cands = batched_np_list.progress_apply(en_nlp)\n",
    "    tagged_np_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_np_list)]\n",
    "    #tagged_np_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_np_list, en_nlp, batch_size=6000))]\n",
    "\n",
    "    np_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tagged_np_cands]\n",
    "    #print(np_cand_heads)\n",
    "    \n",
    "    np_and_cand_list = cand_prep.get_cand_type(event_np_list,np_cand_heads, tweet_tags)\n",
    "    #print(event_np_list)\n",
    "          \n",
    "    # ~~~~~~~~~~~~ processing of coref candidates ~~~~~~~~~~~~~~~~~~~~~\n",
    "    print(f'Processing {event_name} coreference candidates...')    \n",
    "    \n",
    "    #extract only the representative mentions as representative phrases of candidates\n",
    "    event_crf_list = [[coref_group[0][coref_group[1]] for coref_group in tweet_corefs] for tweet_corefs in event_crf_list]\n",
    "    \n",
    "    #event_crf_list = remove_weird_chars(event_crf_list)\n",
    "    event_crf_list = cand_prep.remove_char(event_crf_list,'@')\n",
    "\n",
    "    event_crf_list = [['no_candidate'] if len(crf_ps)==0 else crf_ps for crf_ps in event_crf_list ]\n",
    "    \n",
    "    print(f'Tagging {event_name} coreference candidates...')       \n",
    "    #tag all tweets and save them in a list    \n",
    "    #batched_coref_list = cand_prep.prep_candlist_for_batching(event_crf_list)\n",
    "    #print(batched_coref_list)\n",
    "    tagged_coref_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_crf_list)]\n",
    "    #tagged_coref_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_coref_list, en_nlp, batch_size=6000))] \n",
    "    #print(tagged_coref_cands)\n",
    "        \n",
    "    coref_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tagged_coref_cands]\n",
    "          \n",
    "    coref_and_cand_list = cand_prep.get_cand_type(event_crf_list, coref_cand_heads, tweet_tags)\n",
    "          \n",
    "    # ~~~~~~~~~~~~~~~~~~~~ combining candidate lists ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #concatenate corefs and noun phrase lists\n",
    "    nps_cands = [cand for cands in np_and_cand_list for cand in cands]\n",
    "    crf_cands = [cand for cands in coref_and_cand_list for cand in cands]\n",
    "    #candidate_list = coref_and_cand_list + np_and_cand_list\n",
    "    #print(f'Len = {len(candidate_list)} should be 2x amount of tweets')\n",
    "    #print(len(nps_cands), len(crf_cands))\n",
    "    #unpack list of lists into one list\n",
    "    candidate_list = nps_cands + crf_cands\n",
    "    print(f'The amount of all candidates is {len(candidate_list)} -  nps: {len(nps_cands)}, crfs:{len(crf_cands)}')\n",
    "          \n",
    "    nps_tagged = [sent for tagged_cand in tagged_np_cands for sent in tagged_cand.sentences ]\n",
    "    crf_tagged = [sent for tagged_cand in tagged_coref_cands for sent in tagged_cand.sentences ]\n",
    "    print(len(nps_tagged), len(crf_tagged))\n",
    "    all_cands_tagged = nps_tagged + crf_tagged\n",
    "\n",
    "        \n",
    "    #print(len(candidate_list),'vs', len(all_cands_tagged))\n",
    "    cand_df = pd.DataFrame(\n",
    "        {'candidates': candidate_list,\n",
    "         'cand_tags': all_cands_tagged\n",
    "        })\n",
    "\n",
    "    cand_df['cand_text'] = cand_df.candidates.apply(lambda x: x[0])\n",
    "    cand_df['cand_len'] = cand_df.cand_text.apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "    count_cands = Counter(cand_df['cand_text'])\n",
    "    cand_df['cand_freq'] = cand_df[\"cand_text\"].map(count_cands)\n",
    "    \n",
    "    #count_cands[cand_df['cand_text']]\n",
    "    #count_sorted = sorted(count_cands.items(),key=lambda x: x[1],reverse=True)\n",
    "    cand_df.columns = cand_df.columns.str.strip()\n",
    "    \n",
    "          \n",
    "    # we sort the candidates by their length\n",
    "    cand_df.sort_values('cand_freq', ascending=False,inplace=True)\n",
    "\n",
    "    #cand_df = cand_df[cand_df.cand_text not in  ['no_candidate', 'candidate_to_be_removed']]\n",
    "\n",
    "    cand_df.reset_index(drop=True, inplace = True)\n",
    "    #remove dummy candidates that were used to avoid errors\n",
    "\n",
    "    print(len(cand_df))\n",
    "    cand_df = cand_df[cand_df.cand_text != 'candidate_to_be_removed']\n",
    "    cand_df = cand_df[cand_df.cand_text != 'no_candidate']\n",
    "    len(cand_df)\n",
    "    cand_df.reset_index(drop=True,inplace=True)\n",
    "          \n",
    "    return cand_df\n",
    "          \n",
    "          \n",
    "moria_cands = pipeline('moria')\n",
    "\n",
    "pickle_files('moria_cands_df', moria_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading beirut data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/24511 [01:00<?, ?it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24511/24511 [00:00<00:00, 29013.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing beirut noun phrase candidates...\n",
      "removing long candidates...\n",
      "Removed 0 candidates longer than 9 words!\n",
      "removing child NP candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/24511 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 child NP candidates!\n",
      "Tagging beirut noun phrase candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24511/24511 [4:34:45<00:00,  1.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 24511/24511 [12:30<00:00, 32.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207566\n"
     ]
    }
   ],
   "source": [
    "def pipeline2(event_name,np_list):\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 1. LOAD THE DATA ~~~~~~~~~~~~~~~~~~~~~\n",
    "    event_np_list,event_crf_list,event_tagged_tweets = load_event_data(event_name)\n",
    "    event_np_list = np_list\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 2. GET POS AND NER TAGS ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # get easily accessible list of tuples (POS-tags of each word, NER-tags of each named entity) \n",
    "    tweet_tags = cand_prep.get_tweet_tags(event_tagged_tweets) \n",
    "    \n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 3. PREPROCESS CANDIDATES ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ~~~~~~~~~~~~ processing of noun phrases ~~~~~~~~~~~~~~~~~~~~~\n",
    "    print(f'Processing {event_name} noun phrase candidates...')\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    # remove NP candidates longer than threshold and remove all child NPs of parent NPs\n",
    "    event_np_list = cand_prep.remove_long_nps(event_np_list)\n",
    "    event_np_list = cand_prep.remove_child_nps(event_np_list) \n",
    "    #event_np_list = remove_weird_chars(event_np_list)\n",
    "    event_np_list = cand_prep.remove_char(event_np_list,'@')\n",
    "\n",
    "    event_np_list = [['no_candidate'] if len(noun_ps)==0 or noun_ps ==' ' else noun_ps for noun_ps in event_np_list ]\n",
    "    \n",
    "    #print(event_np_list)\n",
    "    print(f'Tagging {event_name} noun phrase candidates...')\n",
    "    #tag all tweets and save them in a list    \n",
    "\n",
    "    #tagged_np_cands = batched_np_list.progress_apply(en_nlp)\n",
    "    tagged_np_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_np_list)]\n",
    "    #tagged_np_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_np_list, en_nlp, batch_size=6000))]\n",
    "\n",
    "    np_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tagged_np_cands]\n",
    "    #print(np_cand_heads)\n",
    "    \n",
    "    np_and_cand_list = cand_prep.get_cand_type(event_np_list,np_cand_heads, tweet_tags)\n",
    "    #print(event_np_list)\n",
    "          \n",
    "          \n",
    "    # ~~~~~~~~~~~~~~~~~~~~ combining candidate lists ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #concatenate corefs and noun phrase lists\n",
    "    nps_cands = [cand for cands in np_and_cand_list for cand in cands]\n",
    "    #candidate_list = coref_and_cand_list + np_and_cand_list\n",
    "    #print(f'Len = {len(candidate_list)} should be 2x amount of tweets')\n",
    "    #print(len(nps_cands), len(crf_cands))\n",
    "    #unpack list of lists into one list\n",
    "    candidate_list = nps_cands\n",
    "          \n",
    "    nps_tagged = [sent for tagged_cand in tagged_np_cands for sent in tagged_cand.sentences ]\n",
    "\n",
    "    all_cands_tagged = nps_tagged\n",
    "\n",
    "        \n",
    "    #print(len(candidate_list),'vs', len(all_cands_tagged))\n",
    "    cand_df = pd.DataFrame(\n",
    "        {'candidates': candidate_list,\n",
    "         'cand_tags': all_cands_tagged\n",
    "        })\n",
    "\n",
    "    cand_df['cand_text'] = cand_df.candidates.apply(lambda x: x[0])\n",
    "    cand_df['cand_len'] = cand_df.cand_text.apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "    count_cands = Counter(cand_df['cand_text'])\n",
    "    cand_df['cand_freq'] = cand_df[\"cand_text\"].map(count_cands)\n",
    "    \n",
    "    #count_cands[cand_df['cand_text']]\n",
    "    #count_sorted = sorted(count_cands.items(),key=lambda x: x[1],reverse=True)\n",
    "    cand_df.columns = cand_df.columns.str.strip()\n",
    "    \n",
    "          \n",
    "    # we sort the candidates by their length\n",
    "    cand_df.sort_values('cand_freq', ascending=False,inplace=True)\n",
    "\n",
    "    #cand_df = cand_df[cand_df.cand_text not in  ['no_candidate', 'candidate_to_be_removed']]\n",
    "\n",
    "    cand_df.reset_index(drop=True, inplace = True)\n",
    "    #remove dummy candidates that were used to avoid errors\n",
    "\n",
    "    \n",
    "    cand_df = cand_df[cand_df.cand_text != 'candidate_to_be_removed']\n",
    "    cand_df = cand_df[cand_df.cand_text != 'no_candidate']\n",
    "    print(len(cand_df))    \n",
    "    cand_df.reset_index(drop=True,inplace=True)\n",
    "          \n",
    "    return cand_df\n",
    "          \n",
    "          \n",
    "beirut_cands = pipeline2('beirut',np_list)\n",
    "\n",
    "#pickle_files('moria_cands_df', moria_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. We apply stanza module on the tweets to get NER and POS tags. We do it in batches to speed things up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. As initial WCL candidates, we extract noun phrases (NPs) and coreference chains.\n",
    "\n",
    "## We do so using CoreNLPClient wrapper\n",
    "\n",
    "### SOME PREPROCESSING NEEDED\n",
    "* remove links - check\n",
    "* remove # from hashtags? - check\n",
    "* remove/merge mentions? - check\n",
    "\n",
    "\n",
    "* remove recurring texts (signatures of news media) - any new spotted should be added in preprocessing file's '__remove_tweet_signatures__' function\n",
    "* remove posts of some accounts (refugee_list)\n",
    "* exclude NERs that tag numbers - should we mark phrase as NE if the head is not NE? - check\n",
    "* play around with candidate types\n",
    "* optimize code and make it neater\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. We keep only NPs shorter than 20 words and remove children of parent NPs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. We get the heads of noun phrases (in batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. We define candidate types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. We assign candidate types to noun phrase candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. We get coreference chains candidates from the tweet corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. We determine candidate's type for representative mentions of coref candidates (in batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. We combine the candidate lists for candidate merging\n",
    "\n",
    "We organize candidates in a list sorted by their number of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "moria_cands = load_pickle('moria_cands_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "beirut_whatmerged = load_pickle('beirut_whatmerged2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [I, all your books, professor, you, Ilili, thi...\n",
       "1        [a Lebanese immigrant, the second wave, Norman...\n",
       "2                      [migrant, workers, Lebanese racism]\n",
       "3        [eastern Lebanon, the coronavirus, the vulnera...\n",
       "4        [a lovely way, the week, inspiring art, the be...\n",
       "                               ...                        \n",
       "24506    [the CIA, a report, the (Beirut port explosion...\n",
       "24507    [I, a Syrian refugee, Lebanon, I, four childre...\n",
       "24508                                    [i, im, i family]\n",
       "24509    [me, a Palestinian auntie familys, my familys ...\n",
       "24510                  [we, a better year, we, each other]\n",
       "Length: 24511, dtype: object"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = beirut_cands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First merging step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# THIS IS THE FIRST MERGING STEP\n",
    "#\n",
    "\n",
    "        \n",
    "def merging_step1(candidate_list):\n",
    "    \"\"\"\n",
    "    In the first merging step, we merge two candidates if the head of each of their representative phrase \n",
    "     is identical by string comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    indices_to_remove = set()\n",
    "    for up_cand_id in tqdm(range(len(candidate_list))):   \n",
    "\n",
    "        if up_cand_id in indices_to_remove:\n",
    "            continue\n",
    "        up_cand = candidate_list[up_cand_id]    \n",
    "            \n",
    "        for low_cand_id in range(up_cand_id+1,len(candidate_list)):\n",
    "            low_cand = candidate_list[low_cand_id]\n",
    "\n",
    "            if up_cand[1].lower() == low_cand[1].lower():# and upper_cand[3] == lower_cand[3]:\n",
    "\n",
    "\n",
    "                indices_to_remove.add(low_cand_id)\n",
    "\n",
    "                \n",
    "    return indices_to_remove\n",
    "\n",
    "\n",
    "def merge_indices(cand_df,indices_to_remove):                \n",
    "\n",
    "    print(f'Initial amount of candidates: {len(cand_df)}')                \n",
    "    #print(len(sorted(indices_to_remove)))\n",
    "\n",
    "    #for index in reversed(sorted(indices_to_remove)):\n",
    "    cand_df.drop(indices_to_remove,inplace=True)\n",
    "        \n",
    "    cand_df.reset_index(drop=True,inplace=True)\n",
    "    print(f'Amount of candidates: {len(cand_df)}, after removing {len(sorted(indices_to_remove))} indices') \n",
    "    return cand_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in cand_df['candidates']:\n",
    "    print(cand[1], cand[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second merging step\n",
    "\n",
    "We merge 2 candidates if their sets of phrases heads are semantically similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "#load the GoogleNews 300dim model (fix path)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\nikodemicek\\Dropbox (CBS)\\Master thesis data\\GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "event_series = event_df['text'].apply(lambda x: x.split(' '))\n",
    "model_moria = Word2Vec(event_series,size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('refugees', 0.8041340112686157),\n",
       " ('asylum_seeker', 0.7111446857452393),\n",
       " ('asylum_seekers', 0.6694400906562805),\n",
       " ('Refugee', 0.6444518566131592),\n",
       " ('refugee_status', 0.6259119510650635),\n",
       " ('asylum', 0.6098962426185608),\n",
       " ('refugee_resettlement', 0.6018669605255127),\n",
       " ('UNHCR', 0.5838466882705688),\n",
       " ('displaced_persons', 0.5829589366912842),\n",
       " ('Refugees', 0.5805562734603882)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('refugee')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lesbos', 0.7489250302314758),\n",
       " ('Eleonas', 0.6656873226165771),\n",
       " ('La', 0.664945125579834),\n",
       " ('Mae', 0.6619389057159424),\n",
       " ('Moria.', 0.6439236402511597),\n",
       " ('#Lipa', 0.6374841332435608),\n",
       " ('Lipa', 0.633926510810852),\n",
       " ('Karatepe', 0.6262264847755432),\n",
       " ('#Jerusalem,', 0.610672116279602),\n",
       " ('Karen', 0.6037241220474243)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_moria.most_similar('moria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "def merging_step2(candidate_list):\n",
    "    \n",
    "    indices_to_remove = set()\n",
    "    \n",
    "    for upper_cand_id in tqdm(range(len(candidate_list))):     \n",
    "        upper_cand = candidate_list[upper_cand_id]\n",
    "        \n",
    "        up_cand_mean_vec = phrase_heads_avg_vector(upper_cand[2])\n",
    "        \n",
    "        for lower_cand_id in range(upper_cand_id+1,len(candidate_list)): \n",
    "            lower_cand = candidate_list[lower_cand_id]\n",
    "            #print(f'for index {candidate_list.index(longer_cand)} checking the index {candidate_list.index(cand)}')\n",
    "            #if candidate_list[longer_cand][1] == candidate_list[cand][1]:\n",
    "                #print(f'matching \"{longer_cand}\" with \"{cand}\"')\n",
    "            low_cand_mean_vec = phrase_heads_avg_vector(lower_cand[2])\n",
    "\n",
    "            if upper_cand[3] == lower_cand[3]:\n",
    "                #try:\n",
    "                    #print(1-cosine(long_cand_mean_vec,cand_mean_vec))\n",
    "                    #print(long_cand_mean_vec.reshape(-1,1).shape, cand_mean_vec.reshape(1,-1).shape)\n",
    "                    if 1-cosine(up_cand_mean_vec,low_cand_mean_vec) >= 0.7:\n",
    "                        #print(f'matching \"{longer_cand}\" with \"{cand}\"') \n",
    "                        indices_to_remove.add(lower_cand_id)\n",
    "                        what_merged2[upper_cand[0].lower()].append(lower_cand[0].lower())\n",
    "                        \n",
    "                #except AttributeError:\n",
    "                    #pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                if 1-cosine(up_cand_mean_vec,low_cand_mean_vec) >= 0.8:\n",
    "                    #print(f'matching \"{longer_cand}\" with \"{cand}\"') \n",
    "                    indices_to_remove.add(lower_cand_id)\n",
    "                    what_merged2[upper_cand[0].lower()].append(lower_cand[0].lower())\n",
    "\n",
    "\n",
    "\n",
    "    return indices_to_remove\n",
    "\n",
    "def phrase_heads_avg_vector(phrase_set):\n",
    "    phrase_head_vectors = []\n",
    "    for phrase_head in phrase_set:    \n",
    "        try:\n",
    "            phrase_head_vectors.append(model[phrase_head])\n",
    "        except KeyError:\n",
    "            phrase_head_vectors.append(np.NaN)\n",
    "    #phrase_head_vectors = [model[phrase_head] for phrase_head in phrase_set]\n",
    "    if len(phrase_head_vectors) != 0:\n",
    "        return np.mean(phrase_head_vectors,axis=0)\n",
    "    else: \n",
    "        return np.NaN\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(event_cands_merged['cand_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in cand_df['candidates']:\n",
    "    print(cand[1], cand[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third merging step representative labeling\n",
    "\n",
    "currently working on average cosine similarity of each phrase in the candidate - maybe not optimal, maybe it will be better with a different threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def merging_step3(cand_df):\n",
    "    phrases = []\n",
    "    indices_to_remove = set()\n",
    "    # 1. first we find adj-nn phrases within the candidate\n",
    "    for candidate in cand_df['cand_tags']:  \n",
    "        #the head of noun phrase is marked with value 0 for the word.head\n",
    "        cand_heads_pos = [(word.text, word.head, word.xpos) for word in candidate.words]\n",
    "        #np_pos_tags = {word.text: word.xpos for sent in doc.sentences for word in sent.words}\n",
    "        #print(np_heads_pos)\n",
    "        cand_labeling_phrases = []\n",
    "        for word, head, pos in cand_heads_pos:\n",
    "            #head-1 because the pointer to head does not use 0 index\n",
    "            if (pos == 'JJ' or pos=='VBN') and 'NN' in cand_heads_pos[head-1][2]:\n",
    "                cand_labeling_phrases.append(f'{word}_{cand_heads_pos[head-1][0]}')\n",
    "        phrases.append(cand_labeling_phrases)\n",
    "    \n",
    "    candidate_list = cand_df['candidates']\n",
    "    # 2. we compare the similarities of candidates' phrases\n",
    "    for up_cand_id in range(len(candidate_list)):     \n",
    "        up_cand = candidate_list[up_cand_id]\n",
    "        up_cand_vectors = phrases_vectors(phrases[up_cand_id])\n",
    "        if len(up_cand_vectors)==0:\n",
    "            pass\n",
    "        else:\n",
    "            for low_cand_id in range(up_cand_id+1,len(candidate_list)): \n",
    "                low_cand = candidate_list[low_cand_id]\n",
    "                low_cand_vectors = phrases_vectors(phrases[low_cand_id])\n",
    "                if len(low_cand_vectors)==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    sim_matrix = np.zeros((len(up_cand_vectors),len(low_cand_vectors)))\n",
    "                    #print(sim_matrix)\n",
    "                    for i in range(len(up_cand_vectors)):\n",
    "                        for j in range(len(low_cand_vectors)):\n",
    "\n",
    "                            sim_matrix[i][j] = 1-cosine(up_cand_vectors[i],low_cand_vectors[j])\n",
    "\n",
    "                    # can we compute matrix mean like this? \n",
    "                    #print(sim_matrix)\n",
    "                    if np.mean(sim_matrix) > 0.6:\n",
    "                        #print(f'{longer_cand} and {cand} are {numpy.mean(sim_matrix)} similar' )\n",
    "                        indices_to_remove.add(low_cand_id)\n",
    "                        what_merged3[up_cand[0].lower()].append(low_cand[0].lower())\n",
    "                    #else:\n",
    "                        #print(f'{numpy.mean(sim_matrix)} is not similar' )\n",
    "                    \n",
    "    return indices_to_remove\n",
    "                \n",
    "\n",
    "\n",
    "def phrases_vectors(cand_phrases):\n",
    "    \n",
    "#for cand_phrases in phrases:\n",
    "    #print(cand_phrases)\n",
    "    cand_phrase_vectors = []\n",
    "    for phrase in cand_phrases:\n",
    "        try:\n",
    "            cand_phrase_vectors.append(model[phrase])\n",
    "            #print(f'for existing phrase \"{phrase}\" the vector is {model[phrase][0]}')\n",
    "        except KeyError:\n",
    "            phrase_words = phrase.split('_')\n",
    "            #print(model[phrase_words[1]])\n",
    "            try:\n",
    "                phrase_vectors = [model[phrase_word] for phrase_word in phrase_words]\n",
    "                #print(f'for phrase \"{phrase}\" avg vector is \"{sum(phrase_vectors)/len(phrase_vectors)}') \n",
    "                cand_phrase_vectors.append(sum(phrase_vectors)/len(phrase_vectors))\n",
    "            except KeyError:\n",
    "                cand_phrase_vectors.append(np.NaN)\n",
    "    #print(len(cand_phrase_vectors))\n",
    "    return cand_phrase_vectors\n",
    "    \n",
    " \n",
    "#event_cands_merged = merge_indices(event_cands_merged, merging_step3(event_cands_merged))\n",
    "#print(indices_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_merged3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing the second method - we check for the lexical identity of specific stems in multiple candidates.\n",
    "\n",
    "def merging_step4(cand_df):\n",
    "    phrases = []\n",
    "    indices_to_remove = set()\n",
    "    # 1. first we find adj-nn phrases within the candidate\n",
    "    for candidate in cand_df['cand_tags']:\n",
    "\n",
    "        #the head of noun phrase is marked with value 0 for the word.head\n",
    "        cand_heads_pos = [(word.text, word.head, word.xpos) for word in candidate.words]\n",
    "\n",
    "        #print(np_heads_pos)\n",
    "        cand_compound_phrases = []\n",
    "        for word, head, pos in cand_heads_pos:\n",
    "            #i = np_heads_pos.index((word, head, pos))\n",
    "            #print(np_heads_pos)\n",
    "            #print(np_heads_pos[i])\n",
    "            #print(np_heads_pos[head-1])\n",
    "            #'NN' in np_heads_pos[head-1][2] and\n",
    "            try:\n",
    "                #if 'NN' in pos and 'NN' in cand_heads_pos[i+1][2] : \n",
    "                    #cand_compound_phrases.append(f'{word}_{cand_heads_pos[i+1][0]}')\n",
    "                if 'NN' in pos and 'NN' in cand_heads_pos[head-1][2]:\n",
    "                    cand_compound_phrases.append(f'{word}_{cand_heads_pos[head-1][0]}')\n",
    "            except IndexError:\n",
    "                pass\n",
    "        phrases.append(cand_compound_phrases)\n",
    "    \n",
    "    candidate_list = cand_df['candidates']\n",
    "    # 2. we compare the similarities of candidates' phrases\n",
    "    for up_cand_id in range(len(candidate_list)):     \n",
    "        up_cand = candidate_list[up_cand_id]\n",
    "        up_cand_vectors = phrases_vectors(phrases[up_cand_id])\n",
    "        if len(up_cand_vectors)==0:\n",
    "            pass\n",
    "        else:\n",
    "            for low_cand_id in range(up_cand_id+1,len(candidate_list)):\n",
    "                low_cand = candidate_list[low_cand_id]\n",
    "                low_cand_vectors = phrases_vectors(phrases[low_cand_id])\n",
    "                if len(low_cand_vectors)==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    sim_matrix = np.zeros((len(up_cand_vectors),len(low_cand_vectors)))\n",
    "                    #print(sim_matrix)\n",
    "                    for i in range(len(up_cand_vectors)):\n",
    "                        for j in range(len(low_cand_vectors)):\n",
    "                            #print(cosine_similarity(long_cand_vectors[i].reshape(1,-1),short_cand_vectors[j].reshape(1,-1)))\n",
    "                            sim_matrix[i][j] = 1-cosine(up_cand_vectors[i],low_cand_vectors[j])\n",
    "                            \"\"\"if cosine_similarity(long_cand_vectors[i].reshape(1,-1),short_cand_vectors[j].reshape(1,-1)) > 0.4:                \n",
    "                                sim_matrix[i][j] = 2\n",
    "                            elif cosine_similarity(long_cand_vectors[i].reshape(1,-1),short_cand_vectors[j].reshape(1,-1)) > 0.2:\n",
    "                                sim_matrix[i][j] = 1\n",
    "                            else:\n",
    "                                sim_matrix[i][j] = 0\"\"\"\n",
    "\n",
    "                    #print(sim_matrix, up_cand,low_cand)            \n",
    "                    if np.mean(sim_matrix) > 0.6:\n",
    "                        print(f'{up_cand_id} and {low_cand_id} are {np.mean(sim_matrix)} similar' )\n",
    "                        indices_to_remove.add(low_cand_id)\n",
    "                        what_merged4[up_cand[0].lower()].append(low_cand[0].lower())\n",
    "                    #else:\n",
    "                        #print(f'{numpy.mean(sim_matrix)} is not similar' )\n",
    "                    \n",
    "    return indices_to_remove\n",
    "\n",
    "\n",
    "#event_cands_merged = merge_indices(event_cands_merged, merging_step4(event_cands_merged))\n",
    "#print(merging_step4(candidate_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging step 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 207551/207551 [55:17<00:00, 62.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial amount of candidates: 207551\n",
      "Amount of candidates: 14465, after removing 193086 indices\n"
     ]
    }
   ],
   "source": [
    "what_merged1,what_merged2,what_merged3,what_merged4 = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "event_cands = beirut_cands\n",
    "\n",
    "event_cands_merged = merge_indices(event_cands, merging_step1(event_cands['candidates']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/14465 [00:00<?, ?it/s]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  5%|███▊                                                                        | 717/14465 [17:18<7:50:16,  2.05s/it]"
     ]
    }
   ],
   "source": [
    "event_cands_merged = merge_indices(event_cands_merged, merging_step2(event_cands_merged['candidates']))\n",
    "\n",
    "what_merged2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands_merged = merge_indices(event_cands_merged, merging_step3(event_cands_merged))\n",
    "\n",
    "what_merged3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands_merged = merge_indices(event_cands_merged, merging_step4(event_cands_merged))\n",
    "what_merged4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_files('moria_cands_merged',event_cands_merged)\n",
    "pickle_files('moria_whatmerged2',what_merged2)\n",
    "pickle_files('moria_whatmerged3',what_merged3)\n",
    "pickle_files('moria_whatmerged4',what_merged4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"frame_properties = {'affection':['affection','attachment', 'devotion', 'fondness','love','passion'],\n",
    "                    'refusal': ['refusal','declination','denial','disallowance','nay','no'],\n",
    "                    'trustworthiness':['trustworthiness','integrity','accuracy','credibility','authenticity','fairness'],\n",
    "                    'no trustworthiness':['falsehood','dishonesty','unfairness','deceit','corruption'],\n",
    "                    'reason': ['reason','logic','sense','rationale','argument','justification'],\n",
    "                    'unreason/irrationality': ['unreason','irrationality','fallaciousness','unsoundness'],\n",
    "                    'easiness': ['easiness','simplicity','obviousness','ease','comfort'],\n",
    "                    'difficulty': ['difficulty','adversity','hardship','crisis','obstacle','trouble' ],\n",
    "                    'honor': ['honor', 'dignity','esteem','reputation','praise'],\n",
    "                    'dishonor': ['disgrace','dishonor','reproach','opprobrium']}\"\"\" #from Hamborg's paper\n",
    "\n",
    "# from paper Shifting the refugee narratives? by Greussing & Boomgaarden (2015)\n",
    "frame_properties = {'settlement':['settlement','accomodation','permanent','temporary','barracks','accommodated','tent','camp', 'shelter'],\n",
    "                   'reception':['quota', 'distribution', 'limit', 'selection','reception','together','asylum','receive'],\n",
    "                    'security':['security', 'border','crossing','fence','control','flow'],\n",
    "                    'criminality':['officer','terror','suspicion','crime','offense','police','trafficking','suspect'],\n",
    "                    'economisation':['euro','economic','million','thousand','cost','money'],\n",
    "                    'humanitarian':['humane','voluntary','help','support','aid','care','solidarity'],\n",
    "                    'victimization':['fight','victim','war','dead','rescued','state'],\n",
    "                    'integration': ['labour','employed','unemployed','integration','positive'],\n",
    "                    \n",
    "                    #from hamborg\n",
    "                    'affection':['affection','attachment', 'devotion', 'fondness','love','passion'],\n",
    "                    'refusal': ['refusal','declination','denial','disallowance','nay','no'],\n",
    "                    'trustworthiness':['trustworthiness','integrity','accuracy','credibility','authenticity','fairness'],\n",
    "                    'no trustworthiness':['falsehood','dishonesty','unfairness','deceit','corruption'],\n",
    "                    'reason': ['reason','logic','sense','rationale','argument','justification'],\n",
    "                    'irrationality': ['unreason','irrationality','fallaciousness','unsoundness'],\n",
    "                    'easiness': ['easiness','simplicity','obviousness','ease','comfort'],\n",
    "                    'difficulty': ['difficulty','adversity','hardship','crisis','obstacle','trouble' ],\n",
    "                    'honor': ['honor', 'dignity','esteem','reputation','praise'],\n",
    "                    'dishonor': ['disgrace','dishonor','reproach','opprobrium']\n",
    "                   \n",
    "                   }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_merged2 = load_pickle('beirut_whatmerged3')\n",
    "what_merged2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import conceptnet_lite as cn\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(r\"C:/Users/niol19ac/Dropbox (CBS)/Master thesis data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "manual_cands = ['refugee','migrant','greece','turkey','syria','beirut','immigrant','aoun']\n",
    "\n",
    "\n",
    "# to run on the server we should use larger model according to the paper - \"conceptnet-numberbatch-17-06-300\"\n",
    "model = api.load(\"glove-twitter-200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print('preprocessing tweets...')\n",
    "tqdm.pandas()\n",
    "tweets_corpus = list(event_df['text'].progress_apply(preprocessing.preprocess_tweets))\n",
    "\n",
    "\n",
    "print('assigning frame properties to words from tweets...')\n",
    "word_properties = defaultdict(dict)\n",
    "for i in tqdm(tweets_corpus):\n",
    "    tweet_words = [word.lower() for word in i.split() if word not in stop_words and len(word)>1]\n",
    "    for word in tweet_words:\n",
    "        #print(word)\n",
    "        word = lemma.lemmatize(word)\n",
    "        property_list = []\n",
    "        #print(list(frame_properties.keys()))\n",
    "        for prop in list(frame_properties.keys()):\n",
    "            #print(frame_properties[prop])\n",
    "            \n",
    "            try:\n",
    "                #print(f'sim of {word}, {prop} is {model.similarity(word, prop)}')\n",
    "                weights = [model.similarity(word, seed) for seed in frame_properties[prop]]\n",
    "                #print(weights)\n",
    "                if max(weights)>0.4:\n",
    "                    word_properties[word][prop] = max(weights)\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "print(word_properties)\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tweets = load_pickle('beirut_tagged_tweets')\n",
    "coref_chains = load_pickle('moria_crf_list')\n",
    "\n",
    "#coref_chains[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet_id in range(len(tagged_tweets))[:10]:\n",
    "    print(coref_chains[tweet_id])\n",
    "    print(tagged_tweets[tweet_id].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus.reader.wordnet import NOUN\n",
    "import numpy as np\n",
    "  \n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "cand_frames = defaultdict(list)\n",
    "\n",
    "framed_words = pd.DataFrame(columns=['word','date',list(frame_properties.keys())])\n",
    "\n",
    "event_df[['date','time']] = event_df['created_at'].str.split(' ',expand=True)\n",
    "\n",
    "\n",
    "for tweet_id in tqdm(range(len(tagged_tweets))):\n",
    "    #print(tweet)\n",
    "    cand_words = [[word.id, word.text,word.head] for sent in tagged_tweets[tweet_id].sentences for word in sent.words]\n",
    "    #print(*[f'id: {word.id}\\tword: {word.text:<15}head id: {word.head:<5}head: {sent.words[word.head-1].text if word.head > 0 else \"root\":<10}deprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "    #print(len(cand_df['candidates']))\n",
    "    for cand in manual_cands:#event_merged_cands['candidates']:\n",
    "            # lemmatize representative head of candidate \n",
    "            rep_head = cand #lemma.lemmatize(cand[1].lower(),pos=NOUN)\n",
    "            \n",
    "            if rep_head in tagged_tweets[tweet_id].text.lower() and len(rep_head)>1:\n",
    "                #find all dependencies of the phrase head\n",
    "                for related in range(len(cand_words)):\n",
    "                    cand_word_lemma = lemma.lemmatize(cand_words[related][1].lower())\n",
    "                    #print(f'Yes it is, related = {lemma.lemmatize(cand_heads[related][1].lower(),pos=NOUN)}')\n",
    "                    #lemma.lemmatize(cand_heads[related][1].lower(),pos=NOUN)\n",
    "                    if rep_head == cand_word_lemma:\n",
    "                        related_word = lemma.lemmatize(cand_words[cand_words[related][2]-1][1].lower())\n",
    "                        cand_frames['word'].append(rep_head)\n",
    "                        cand_frames['date'].append(event_df['date'][tweet_id])\n",
    "                        #cand_frames['word'].append(phrase_head)\n",
    "                        for frame_property in list(frame_properties.keys()):\n",
    "                            #print(frame_property)\n",
    "\n",
    "                            try:\n",
    "                                #print(word_properties[phrase_head][frame_property])\n",
    "                                cand_frames[frame_property].append(word_properties[related_word][frame_property])\n",
    "\n",
    "                            except KeyError:\n",
    "                                #print('Error')\n",
    "                                #cand_frames[frame_property].append(word_properties['tent'][frame_property])\n",
    "                                cand_frames[frame_property].append(np.NaN)\n",
    "                                \n",
    "\n",
    "                    #print(len(cand_frames[frame_property]))\n",
    "                    \n",
    "                #print('\\n')\n",
    "                \n",
    "                \n",
    "                \"\"\"for frame_property in list(frame_properties.keys()):\n",
    "                    for seed_word in frame_properties[frame_property]:\n",
    "                        try:\n",
    "                            for related in range(len(np_heads)):\n",
    "                                #print(np_heads[related])\n",
    "                                #if cand[1] == np_heads[related][1]:\n",
    "                                if phrase_head == np_heads[related][1]:\n",
    "                                    #print(f'checking {seed_word} and {phrase_head}_{np_heads[np_heads[related][2]-1][1]}')\n",
    "                                    cand_frames[seed_word][phrase_head].append(model.similarity(seed_word,np_heads[np_heads[related][2]-1][1]))\n",
    "                            #[cand_frames[seed_word][cand].append(model.similarity(seed_word,np_heads[np_heads[related][2]+1][1])) if cand == np_heads[related][1] else print('') for related in range(len(np_heads))]\n",
    "                        except KeyError:\n",
    "                            pass\"\"\"\n",
    "                        #[cand_frames[seed_word][cand].append(model.similarity(print(f'{cand} is related to {np_heads[np_heads[related][2]+1][1]}') if cand == np_heads[related][1] else print('nej') for related in range(len(np_heads))]\n",
    "            #print(get_head(cand))\n",
    "            #print(np_heads[19][1])\n",
    "            #[f(x) if condition else g(x) for x in sequence]\n",
    "            #[print(np_heads[np_heads[related][2]-1]) if get_head(cand)==np_heads[related][1] else print('hi') for related in range(len(np_heads))]\n",
    "\n",
    "            \n",
    "#became ___ (vb and vbx)\n",
    "#(VP sit/VB (PP on/IN (NP the/DT mat/NN))))) \n",
    "\n",
    "#common phrases = migrant camp, covid case, covid test\n",
    "\n",
    "#cand_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cand_frames.keys())\n",
    "\n",
    "framed_words = pd.DataFrame.from_dict(cand_frames)\n",
    "\n",
    "#framed_words[framed_words['word']=='migrants'].tail(50)\n",
    "\n",
    "#framed_words = framed_words.dropna(subset=['settlement', 'reception', 'security', 'criminality', 'economisation', 'humanitarian', 'victimization', 'integration', 'affection', 'refusal', 'trustworthiness', 'no trustworthiness', 'reason', 'unreason/irrationality', 'easiness', 'difficulty', 'honor', 'dishonor'],how='all')\n",
    "\n",
    "framed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_word = 'refugee'\n",
    "\n",
    "merged_frames = framed_words.copy()\n",
    "\n",
    "#framed_words[framed_words['date']=='2020-09-04']\n",
    "\n",
    "merged_frames['no trustworthiness'] = - merged_frames['no trustworthiness']\n",
    "merged_frames['refusal'] = - merged_frames['refusal']\n",
    "merged_frames['unreason/irrationality'] = -merged_frames['unreason/irrationality']\n",
    "merged_frames['difficulty'] = -merged_frames['difficulty'] \n",
    "merged_frames['dishonor'] = -merged_frames['dishonor']\n",
    "\n",
    "\n",
    "trust = ['trustworthiness', 'no trustworthiness']\n",
    "honor = ['honor', 'dishonor']\n",
    "affection = ['affection','refusal']\n",
    "reason = ['reason','unreason/irrationality']\n",
    "easiness = ['easiness','difficulty']\n",
    "\n",
    "merged_frames = pd.lreshape(merged_frames,\n",
    "                        {'reason':reason, 'honor':honor,'affection':affection,'trust':trust,'easiness':easiness},\n",
    "                       dropna=False)\n",
    "\n",
    "\n",
    "merged_frames = merged_frames[merged_frames['word'] == the_word]\n",
    "\n",
    "aggr_frames = merged_frames.groupby(['word','date'],as_index=False).mean()\n",
    "frame_size = merged_frames.groupby(['word','date'],as_index=False).size()\n",
    "\n",
    "aggr_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "\n",
    "ax1 = px.line(aggr_frames, x=\"date\", y=['settlement'],render_mode='webgl')\n",
    "\n",
    "ax2 = px.line(frame_size, x=\"date\", y=['size'],render_mode='webgl')\n",
    "\n",
    "ax2.update_traces(yaxis='y2')\n",
    "fig.add_traces(ax1.data + ax2.data)\n",
    "\n",
    "fig.for_each_trace(lambda t: t.update(line=dict(color=t.marker.color)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(aggr_frames, x=\"date\", y=['difficulty','easiness'], title=f'Frame bias towards {the_word}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching the tweets speeds the model considerably and is enabled by splitting sentences using '\\n\\n' \n",
    "from stanza_batch import batch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# the sampled_df series should be converted to list and sentences separated with \"\\n\\n\"\n",
    "all_tweets_list = list(tweets_corpus)[:50] \n",
    "for tweet in range(len(all_tweets_list)):\n",
    "    tweet_sentokenized = sent_tokenize(all_tweets_list[tweet])\n",
    "    if tweet_sentokenized == []:\n",
    "        tweet_sentokenized.append('empty_tweet')\n",
    "        print(f'empty tweet at index {tweet}')\n",
    "    all_tweets_list[tweet] = \"\\n\\n\".join(tweet_sentokenized)\n",
    "\n",
    "\n",
    "#tag all tweets and save them in a list    \n",
    "tagged_tweets = [] \n",
    "for tweet in tqdm(batch(all_tweets_list, en_nlp, batch_size=1000)): # Default batch size is 32\n",
    "        tagged_tweets.append(tweet)\n",
    "\n",
    "# the tweet text can now be accessed using .text method        \n",
    "tagged_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for tweet in tqdm(range(len(tweets_corpus))):\n",
    "    print(tweets_corpus[tweet])\n",
    "    np_heads = [[word.id, word.text,word.head,word.deprel] for sent in tagged_tweets[tweet].sentences for word in sent.words]\n",
    "    print(*[f'id: {word.id}\\tword: {word.text:<15}head id: {word.head:<5}head: {sent.words[word.head-1].text if word.head > 0 else \"root\":<10}deprel: {word.deprel}' for sent in tagged_tweets[tweet].sentences for word in sent.words], sep='\\n')\n",
    "    #print(np_heads)\n",
    "    ph_ids = set([np_heads[i][2] for i in range(len(np_heads))])\n",
    "    ph_words = [np_heads[i-1][1] for i in ph_ids]\n",
    "\n",
    "    word_pairs = [(np_heads[word][1], np_heads[np_heads[word][2]-1][1]) for word in range(len(np_heads)) if np_heads[word][2] != 0]\n",
    "    #print(word_pairs)\n",
    "    \n",
    "    compounds = [[np_heads[i][1]+'_'+np_heads[np_heads[i][2]-1][1]] for i in range(len(np_heads)) if 'compound' in np_heads[i][3]]\n",
    "    print(compounds)\n",
    "    \n",
    "    advmods = [[np_heads[i][1]+'_'+np_heads[np_heads[i][2]-1][1]] for i in range(len(np_heads)) if np_heads[i][3]=='advmod']\n",
    "    print(advmods)\n",
    "    \n",
    "    amods = [[np_heads[i][1]+'_'+np_heads[np_heads[i][2]-1][1]] for i in range(len(np_heads)) if np_heads[i][3]=='amod']\n",
    "    print(amods)\n",
    "    for pair in word_pairs:\n",
    "        phrase = pair[0]+'_'+pair[1]\n",
    "\n",
    "    #print(model.most_similar('illegal_immigrant'))\n",
    "\n",
    "    \n",
    "    \"\"\"#print(len(cand_df['candidates']))\n",
    "    candidate_list = cand_df['candidates']\n",
    "    for cand in cand_df['candidates']:\n",
    "        #print(cand[2])\n",
    "        #print(get_head(str(cand)))\n",
    "        for phrase_head in cand[2]:\n",
    "            #print(phrase_head)\n",
    "            #if str(cand[1]) in str(tweet):\n",
    "            if str(phrase_head) in str(tweet) and len(phrase_head)>2:\n",
    "                #print(phrase_head)\n",
    "                ph_words = [np_heads[i-1][1] for i in phrase_heads]\n",
    "                #print(ph_words)\n",
    "                for related in range(len(np_heads)):\n",
    "                    if phrase_head == np_heads[related][1]:\n",
    "                        pass\n",
    "                        #print(f'checking {phrase_head}_{np_heads[np_heads[related][2]-1][1]}')\n",
    "                  for frame_property in list(frame_properties.keys()):\n",
    "                        for seed_word in frame_properties[frame_property]:\n",
    "                        try:\n",
    "                            for related in range(len(np_heads)):\n",
    "                                #print(np_heads[related])\n",
    "                                #if cand[1] == np_heads[related][1]:\n",
    "                                if phrase_head == np_heads[related][1]:\n",
    "                                    print(f'checking {seed_word} and {phrase_head}_{np_heads[np_heads[related][2]-1][1]}')\n",
    "                                    #cand_frames[seed_word][phrase_head].append(model.similarity(seed_word,np_heads[np_heads[related][2]-1][1]))\n",
    "                                #[cand_frames[seed_word][cand].append(model.similarity(seed_word,np_heads[np_heads[related][2]+1][1])) if cand == np_heads[related][1] else print('') for related in range(len(np_heads))]\n",
    "                                except KeyError:\n",
    "                                pass\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
