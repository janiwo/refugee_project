{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate merging and related preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages for the following parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "#python libraries\n",
    "import stanza\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# self written modules\n",
    "import preprocessing\n",
    "import candidate_processing as cand_prep\n",
    "import candidate_extraction as cand_ex\n",
    "\n",
    "import pickle\n",
    "\n",
    "def pickle_file(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. We import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 175900 tweets!\n"
     ]
    }
   ],
   "source": [
    "greece_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/greece_df_clean.csv\" # for Beirut\n",
    "moria_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_moria.csv\" # for Moria\n",
    "channel_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_channel.csv\"\n",
    "rohingya_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_rohingya.csv\"\n",
    "all_url = r\"Dropbox (CBS)/Master thesis data/df_tweets.csv\" # for all tweets\n",
    "\n",
    "def read_event_df(data_url):\n",
    "    directory_path = os.getcwd() + \"/../../../\" + data_url \n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df(greece_url)\n",
    "event_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First,  extracting noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code runs for around another 13h per 100k tweets\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "#use \"with\" so the client stops properly after finished\n",
    "with CoreNLPClient(annotators=[\"tokenize,ssplit,pos,parse\"], timeout=6000000, memory='8G') as client:\n",
    "        print('extracting noun phrases...')\n",
    "        tqdm.pandas()\n",
    "        # get noun phrases with tregex using get_noun_phrases function\n",
    "        event_df['noun_phrases'] = event_df['event_corefs_resolved'].progress_apply(cand_ex.get_noun_phrases,args=(client,\"tokenize,ssplit,pos,parse\"))\n",
    "\n",
    "np_list = list(event_df['noun_phrases'])\n",
    "len(np_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('greece_noun_phrases',np_list)\n",
    "\n",
    "#np_list = load_pickle(\"moria_short_noun_phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. instantiate stanza english language module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-17 14:52:56 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-05-17 14:52:56 INFO: Use device: cpu\n",
      "2021-05-17 14:52:56 INFO: Loading: tokenize\n",
      "2021-05-17 14:52:56 INFO: Loading: pos\n",
      "2021-05-17 14:52:57 INFO: Loading: lemma\n",
      "2021-05-17 14:52:57 INFO: Loading: depparse\n",
      "2021-05-17 14:52:58 INFO: Loading: sentiment\n",
      "2021-05-17 14:52:59 INFO: Loading: ner\n",
      "2021-05-17 14:53:01 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ needed when running first time ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "\n",
    "#stanza.download(\"en\")\n",
    "\n",
    "#stanza.install_corenlp()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# loading the pipeline\n",
    "en_nlp = stanza.Pipeline(\"en\", tokenize_pretokenized=True, ner_batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag tweets using stanza module to get NER and POS tags in tweets. We do it in batches to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 125791/125791 [33:02:57<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "event_tagged_tweets = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(list(event_df['text_clean']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\nikodemicek\\\\Documents\\\\GitHub\\\\refugee_project/../../../Dropbox (CBS)/Master thesis data/Candidate Data/rohingya/rohingya_tagged_tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-cd22c4288f43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rohingya_tagged_tweets'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mevent_tagged_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-22eb04f3af70>\u001b[0m in \u001b[0;36mpickle_file\u001b[1;34m(file_name, file_to_dump)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfolder_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirectory_path\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[1;34mfr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_to_dump\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\nikodemicek\\\\Documents\\\\GitHub\\\\refugee_project/../../../Dropbox (CBS)/Master thesis data/Candidate Data/rohingya/rohingya_tagged_tweets'"
     ]
    }
   ],
   "source": [
    "pickle_file('greece_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for candidate identification\n",
    "\n",
    "**Necessary files:**\n",
    " - event_np_list = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event_data(event_name):\n",
    "    assert event_name in ['greece','tigray','rohingya','moria'], f\"Oh no! We do not analyze {event_name} event\"\n",
    "    \n",
    "    print(f'Loading {event_name} data...')\n",
    "    try:\n",
    "        #sample = 2000\n",
    "        event_np_list = load_pickle(event_name + '_np_list')#[1000:sample]\n",
    "        event_tagged_tweets = load_pickle(event_name + '_tagged_tweets')#[1000:sample]\n",
    "        \n",
    "        return event_np_list,event_tagged_tweets\n",
    "    except:\n",
    "        print(f'The {event_name} files not found! Run candidate_extraction.py file on the {eventname}_df')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading moria data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 92806/92806 [08:40<00:00, 178.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing moria noun phrase candidates...\n",
      "removing long candidates...\n",
      "Removed 0 candidates longer than 9 words!\n",
      "removing child NP candidates...\n",
      "Removed 0 child NP candidates!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/92806 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging moria noun phrase candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 92806/92806 [11:17:03<00:00,  2.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 92806/92806 [16:39<00:00, 92.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751019\n"
     ]
    }
   ],
   "source": [
    "def pipeline(event_name,np_list):\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 1. LOAD THE DATA ~~~~~~~~~~~~~~~~~~~~~\n",
    "    event_np_list,event_tagged_tweets = load_event_data(event_name)\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 2. GET POS AND NER TAGS ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # get easily accessible list of tuples (POS-tags of each word, NER-tags of each named entity) \n",
    "    tweet_tags = cand_prep.get_tweet_tags(event_tagged_tweets) \n",
    "    \n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 3. PREPROCESS CANDIDATES ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ~~~~~~~~~~~~ processing of noun phrases ~~~~~~~~~~~~~~~~~~~~~\n",
    "    print(f'Processing {event_name} noun phrase candidates...')\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    # remove NP candidates longer than threshold and remove all child NPs of parent NPs\n",
    "    event_np_list = cand_prep.remove_long_nps(event_np_list)\n",
    "    event_np_list = cand_prep.remove_child_nps(event_np_list) \n",
    "    #event_np_list = remove_weird_chars(event_np_list)\n",
    "    event_np_list = cand_prep.remove_char(event_np_list,'@')\n",
    "\n",
    "    event_np_list = [['no_candidate'] if len(noun_ps)==0 or noun_ps ==' ' else noun_ps for noun_ps in event_np_list ]\n",
    "    \n",
    "    #print(event_np_list)\n",
    "    print(f'Tagging {event_name} noun phrase candidates...')\n",
    "    #tag all tweets and save them in a list    \n",
    "\n",
    "    #tagged_np_cands = batched_np_list.progress_apply(en_nlp)\n",
    "    tagged_np_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_np_list)]\n",
    "    #tagged_np_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_np_list, en_nlp, batch_size=6000))]\n",
    "\n",
    "    np_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tagged_np_cands]\n",
    "    #print(np_cand_heads)\n",
    "    \n",
    "    np_and_cand_list = cand_prep.get_cand_type(event_np_list,np_cand_heads, tweet_tags)\n",
    "    #print(event_np_list)\n",
    "          \n",
    "          \n",
    "    # ~~~~~~~~~~~~~~~~~~~~ combining candidate lists ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #concatenate corefs and noun phrase lists\n",
    "    nps_cands = [cand for cands in np_and_cand_list for cand in cands]\n",
    "    #candidate_list = coref_and_cand_list + np_and_cand_list\n",
    "\n",
    "    #unpack list of lists into one list\n",
    "    candidate_list = nps_cands\n",
    "          \n",
    "    nps_tagged = [sent for tagged_cand in tagged_np_cands for sent in tagged_cand.sentences ]\n",
    "\n",
    "    all_cands_tagged = nps_tagged\n",
    "\n",
    "        \n",
    "    #print(len(candidate_list),'vs', len(all_cands_tagged))\n",
    "    cand_df = pd.DataFrame(\n",
    "        {'candidates': candidate_list,\n",
    "         'cand_tags': all_cands_tagged\n",
    "        })\n",
    "\n",
    "    cand_df['cand_text'] = cand_df.candidates.apply(lambda x: x[0])\n",
    "    cand_df['cand_len'] = cand_df.cand_text.apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "    count_cands = Counter(cand_df['cand_text'])\n",
    "    cand_df['cand_freq'] = cand_df[\"cand_text\"].map(count_cands)\n",
    "    \n",
    "    #count_cands[cand_df['cand_text']]\n",
    "    #count_sorted = sorted(count_cands.items(),key=lambda x: x[1],reverse=True)\n",
    "    cand_df.columns = cand_df.columns.str.strip()\n",
    "    \n",
    "          \n",
    "    # we sort the candidates by their length\n",
    "    cand_df.sort_values('cand_freq', ascending=False,inplace=True)\n",
    "\n",
    "    #cand_df = cand_df[cand_df.cand_text not in  ['no_candidate', 'candidate_to_be_removed']]\n",
    "\n",
    "    cand_df.reset_index(drop=True, inplace = True)\n",
    "    #remove dummy candidates that were used to avoid errors\n",
    "\n",
    "    \n",
    "    cand_df = cand_df[cand_df.cand_text != 'candidate_to_be_removed']\n",
    "    cand_df = cand_df[cand_df.cand_text != 'no_candidate']\n",
    "    print(len(cand_df))    \n",
    "    cand_df.reset_index(drop=True,inplace=True)\n",
    "          \n",
    "    return cand_df\n",
    "          \n",
    "#import random\n",
    "\n",
    "#random.seed(42)\n",
    "#np_list_sample = random.sample(np_list,10000)\n",
    "event_cands = pipeline2('moria',np_list)\n",
    "\n",
    "#pickle_file('moria_cands_df', moria_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates as identified by stanza library still have a lot of noise to be removed. Cleaner candidates merge better and throwing away duplicate candidates or candidates without useful information speeds up merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally the candidates are cleaned before storing in a file prior to merging\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_cands(event_cands):\n",
    "    \"\"\"\n",
    "    Applying cleaning steps on candidates and engineering some features:\n",
    "     1. creating a column with length of the tweet (in chars)\n",
    "     2. lowercase the candidate information in the tuple with cand, candidate representative head and set of phrases heads\n",
    "     3. extract candidate text and keep only alphanumeric chars\n",
    "     4. remove candidates that are stopwords\n",
    "     5. remove candidates that are only numeric\n",
    "     6. remove candidates that are only 1 char long\n",
    "     \"\"\"\n",
    "    def clean_cand(cand):\n",
    "        cand = list(cand)\n",
    "        cand[0] = re.sub(r'[^A-Za-z0-9 ]+', '', cand[0].lower())\n",
    "        cand[1] = re.sub(r'[^A-Za-z0-9 ]+', '', cand[1].lower())\n",
    "        cand[2] = set([re.sub(r'[^A-Za-z0-9 ]+', '', phrase_word.lower()) for phrase_word in cand[2]])\n",
    "\n",
    "        return tuple(cand)\n",
    "\n",
    "    #stopwords\n",
    "    tqdm.pandas()\n",
    "    event_cands_clean = event_cands.copy()\n",
    "    \n",
    "    \n",
    "    event_cands_clean['candidates'] = event_cands_clean['candidates'].progress_apply(clean_cand)\n",
    "    \n",
    "    event_cands_clean['cand_text'] = event_cands_clean['cand_text'].progress_apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', '', x.lower()).strip())\n",
    "    event_cands_clean = event_cands_clean[~event_cands_clean['cand_text'].isin(stopwords.words('english'))]\n",
    "    event_cands_clean['pure_chars'] = event_cands_clean['cand_text'].progress_apply(lambda x: x.replace(' ', ''))\n",
    "    event_cands_clean = event_cands_clean[~event_cands_clean['pure_chars'].str.isnumeric()]\n",
    "    event_cands_clean.drop('pure_chars',axis=1,inplace=True)\n",
    "    \n",
    "    event_cands_clean['string_len'] = event_cands_clean['cand_text'].progress_apply(len)\n",
    "    event_cands_clean = event_cands_clean[event_cands_clean['string_len']>1]\n",
    "    event_cands_clean = event_cands_clean.drop_duplicates(subset = [\"cand_text\"])\n",
    "    event_cands_clean.reset_index(drop=True, inplace=True)\n",
    "    print(f'The event has  {len(event_cands_clean)} unique candidates after cleaning')\n",
    "    return event_cands_clean\n",
    "\n",
    "event_cands_clean = clean_cands(event_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('moria_short_cands', event_cands_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands = load_pickle('moria_short_cands')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
