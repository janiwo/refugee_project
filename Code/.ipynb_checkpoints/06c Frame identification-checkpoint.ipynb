{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame identification\n",
    "\n",
    "2 versions - By factor analysis and by using BERT embeddings and clustering (affinity propagation)\n",
    "https://www.datacamp.com/community/tutorials/introduction-factor-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(sentence_transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions pickle_file and load_pickle merely help with storing pickled files in the event folders on drive\n",
    "def pickle_file(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = os.getcwd() + \"/../../../../\" + r\"/Dropbox (CBS)/Master thesis data/\"\n",
    "event_url = file_url + r\"Event Dataframes/\"\n",
    "event_url_clean = event_url + r\"Clean/\"\n",
    "\n",
    "candidate_url = file_url + r\"Candidate Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_url_clean = event_url_clean + r\"df_tigray_clean.csv\" # location of clean Tigray dataset\n",
    "greece_url_clean = event_url_clean + r\"df_greece_clean.csv\" # location of clean Greece dataset\n",
    "rohingya_url_clean = event_url_clean + r\"df_rohingya_clean.csv\" # location clean of Rohingya dataset\n",
    "channel_url_clean = event_url_clean +r\"df_channel_clean.csv\" #Location of clean Channel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_url_fi = event_url_clean + r\"df_tigray_fi.csv\" # location of Tigray dataset for frame identification\n",
    "greece_url_fi = event_url_clean + r\"df_greece_fi.csv\" # location of Greece dataset for frame identification\n",
    "rohingya_url_fi = event_url_clean + r\"df_rohingya_fi.csv\" # location of Rohingya dataset for frame identification\n",
    "channel_url_fi = event_url_clean +r\"df_channel_fi.csv\" #Location of Channel dataset for frame identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_candidate_url = candidate_url + r\"tigray/tigray_ents\"\n",
    "greece_candidate_url = candidate_url + r\"greece/greece_ents\"\n",
    "rohingya_candidate_url = candidate_url + r\"rohingya/rohingya_ents\"\n",
    "channel_candidate_url = candidate_url + r\"channel/channel_ents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(greece_candidate_url,\"rb\") as input_file:\n",
    "    ents = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_list(url):\n",
    "    with open(url,\"rb\") as input_file:\n",
    "        ents = pickle.load(input_file)\n",
    "        ents = ents[ents[\"freq\"]>15]\n",
    "        \n",
    "    return list(ents[\"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_ents = get_entity_list(tigray_candidate_url)\n",
    "greece_ents = get_entity_list(greece_candidate_url)\n",
    "rohingya_ents = get_entity_list(rohingya_candidate_url)\n",
    "channel_ents = get_entity_list(channel_candidate_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_list = set(tigray_ents + greece_ents + rohingya_ents + channel_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_event_df(data_url):\n",
    "    # easy dataframe load\n",
    "    event_df = pd.read_csv(data_url, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    event_df = event_df[event_df['text_stm'].notna()]\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 42843 tweets!\n",
      "loaded 137418 tweets!\n",
      "loaded 29423 tweets!\n",
      "loaded 173615 tweets!\n"
     ]
    }
   ],
   "source": [
    "df_tigray = read_event_df(tigray_url_clean)\n",
    "df_greece = read_event_df(greece_url_clean)\n",
    "df_rohingya = read_event_df(rohingya_url_clean)\n",
    "df_channel = read_event_df(channel_url_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = list()\n",
    "\n",
    "for words in df_channel[\"text_stm\"]:\n",
    "    words_tok = word_tokenize(words)\n",
    "    for word in words_tok:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(all_words)\n",
    "counter.most_common() #counter_obj.most_common(n=10)\n",
    "\n",
    "most_frequent_words = [pair[0] for pair in counter.most_common(int(len(counter)*0.025))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1245"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_words(df_col, min_words):\n",
    "    \n",
    "    all_words = list()\n",
    "    \n",
    "    for words in df_col:\n",
    "        for word in words:\n",
    "            all_words.append(word)\n",
    "            \n",
    "    counter = Counter(all_words)\n",
    "    \n",
    "    return [pair[0] for pair in counter.most_common(int(len(counter)*min_words))] #0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(df_col):\n",
    "    \"\"\"\n",
    "    Takes a list with strings and returns a list with tokens\n",
    "    \"\"\"\n",
    "    print(\"Tokenizing tweets...\\n\")\n",
    "    return df_col.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "def remove_unfrequent_words(df_col):\n",
    "    print(\"Removing unfrequent words...\\n\")\n",
    "    most_frequent_words = get_most_frequent_words(df_col)\n",
    "    print(f\"(Removing words that are not among {len(most_frequent_words)} most frequent ones.)\\n\")\n",
    "    return df_col.apply(lambda x: [token for token in x if token in most_frequent_words])\n",
    "\n",
    "def remove_named_entities(df_col):\n",
    "    print(\"Removing named entities...\\n\")\n",
    "    return df_col.apply(lambda x: [token for token in x if token not in ne_list])\n",
    "\n",
    "def preprocessing(df_col, *steps):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe column with text and applies preprocessing steps given \n",
    "    in and returns a string.\n",
    "    \n",
    "    Input:\n",
    "    - df (dataframe): The dataframe containing the text column.\n",
    "    - steps (functions): Multiple functions for preprocessing can be given in.\n",
    "    \n",
    "    Output:\n",
    "    - List with strings.\n",
    "    \"\"\"\n",
    "    # copying over the column for preprocessing\n",
    "    temp = df_col.copy()\n",
    "    for func in steps:\n",
    "        temp = func(temp)\n",
    "    return temp.apply(lambda x: \" \".join([token for token in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(df_col, min_words):\n",
    "    \n",
    "    most_frequent_words = get_most_frequent_words(df_col, min_words)\n",
    "    print(f\"(Removing words that are not among {len(most_frequent_words)} most frequent ones.)\\n\")\n",
    "    \n",
    "    words_to_keep = [word for word in most_frequent_words if word not in ne_list]\n",
    "    \n",
    "    df_col =  df_col.apply(lambda x: [token for token in x if token in words_to_keep])\n",
    "    return df_col.apply(lambda x: \" \".join([token for token in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_greece[\"tok\"] = tokenization(df_greece[\"text_stm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Removing words that are not among 50 most frequent ones.)\n",
      "\n",
      "(Removing words that are not among 1251 most frequent ones.)\n",
      "\n",
      "(Removing words that are not among 2503 most frequent ones.)\n",
      "\n",
      "(Removing words that are not among 5006 most frequent ones.)\n",
      "\n",
      "(Removing words that are not among 25030 most frequent ones.)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-6a18079091a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_greece\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text_frame_identification_005\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_greece\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tok\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_greece\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text_frame_identification_01\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_greece\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tok\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_greece\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text_frame_identification_05\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_greece\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tok\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-193-8ecec0e57288>\u001b[0m in \u001b[0;36mremove_words\u001b[1;34m(df_col, min_words)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mwords_to_keep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmost_frequent_words\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mne_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf_col\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdf_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_to_keep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4138\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-193-8ecec0e57288>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mwords_to_keep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmost_frequent_words\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mne_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf_col\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdf_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_to_keep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-193-8ecec0e57288>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mwords_to_keep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmost_frequent_words\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mne_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf_col\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdf_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_to_keep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_greece[\"text_frame_identification_001\"] = remove_words(df_greece[\"tok\"],0.001)\n",
    "df_greece[\"text_frame_identification_0025\"] = remove_words(df_greece[\"tok\"],0.025)\n",
    "df_greece[\"text_frame_identification_005\"] = remove_words(df_greece[\"tok\"],0.05)\n",
    "df_greece[\"text_frame_identification_01\"] = remove_words(df_greece[\"tok\"],0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Removing unfrequent words...\n",
      "\n",
      "(Removing words that are not among 1251 most frequent ones.)\n",
      "\n",
      "Removing named entities...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_greece[\"text_frame_identification\"] = preprocessing(df_greece[\"text_stm\"],\n",
    "                                                  tokenization,\n",
    "                                                  remove_unfrequent_words,\n",
    "                                                  remove_named_entities)\n",
    "\n",
    "most_frequent_words = get_most_frequent_words(df_greece[\"text_stm\"].apply(lambda x: word_tokenize(x)))\n",
    "words_to_cluster_greece = [word for word in most_frequent_words if word not in ne_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Removing unfrequent words...\n",
      "\n",
      "(Removing words that are not among 649 most frequent ones.)\n",
      "\n",
      "Removing named entities...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tigray[\"text_frame_identification\"] = preprocessing(df_tigray[\"text_stm\"],\n",
    "                                                  tokenization,\n",
    "                                                  remove_unfrequent_words,\n",
    "                                                  remove_named_entities)\n",
    "\n",
    "most_frequent_words = get_most_frequent_words(df_tigray[\"text_stm\"].apply(lambda x: word_tokenize(x)))\n",
    "words_to_cluster_tigray = [word for word in most_frequent_words if word not in ne_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Removing unfrequent words...\n",
      "\n",
      "(Removing words that are not among 1245 most frequent ones.)\n",
      "\n",
      "Removing named entities...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_channel[\"text_frame_identification\"] = preprocessing(df_channel[\"text_stm\"],\n",
    "                                                  tokenization,\n",
    "                                                  remove_unfrequent_words,\n",
    "                                                  remove_named_entities)\n",
    "\n",
    "most_frequent_words = get_most_frequent_words(df_channel[\"text_stm\"].apply(lambda x: word_tokenize(x)))\n",
    "words_to_cluster_channel = [word for word in most_frequent_words if word not in ne_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Removing unfrequent words...\n",
      "\n",
      "(Removing words that are not among 486 most frequent ones.)\n",
      "\n",
      "Removing named entities...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rohingya[\"text_frame_identification\"] = preprocessing(df_rohingya[\"text_stm\"],\n",
    "                                                  tokenization,\n",
    "                                                  remove_unfrequent_words,\n",
    "                                                  remove_named_entities)\n",
    "\n",
    "most_frequent_words = get_most_frequent_words(df_rohingya[\"text_stm\"].apply(lambda x: word_tokenize(x)))\n",
    "words_to_cluster_rohingya = [word for word in most_frequent_words if word not in ne_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source', 'text', 'lang', 'id', 'created_at', 'author_id',\n",
       "       'retweet_count', 'reply_count', 'like_count', 'quote_count',\n",
       "       'withheld.scope', 'hashtags', 'mentions', 'annotations', 'text_clean',\n",
       "       'year', 'calendar_week', 'year_month', 'year_calendar_week', 'refugee',\n",
       "       'migrant', 'immigrant', 'asylum_seeker', 'other', 'date',\n",
       "       'text_coherent', 'retweet_count_sum', 'count', 'text_alphanum',\n",
       "       'text_stm', 'text_frame_identification', 'tok',\n",
       "       'text_frame_identification_0025', 'text_frame_identification_001',\n",
       "       'text_frame_identification_005', 'text_frame_identification_01'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_greece.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_greece_test = df_greece[[\"text\",\"text_coherent\",\"text_frame_identification\",\"date\",'text_frame_identification_0025', 'text_frame_identification_001',\n",
    "       'text_frame_identification_005', 'text_frame_identification_01']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_greece_test.to_csv('C:\\\\Users\\\\jawo19ad\\\\Documents\\\\GitHub\\\\refugee_project\\\\Code/../../../..//Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_greece_fi_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_greece_frame = df_greece[[\"text\",\"text_coherent\",\"text_frame_identification\",\"date\"]]\n",
    "df_tigray_frame = df_tigray[[\"text\",\"text_coherent\",\"text_frame_identification\",\"date\"]]\n",
    "df_rohingya_frame = df_rohingya[[\"text\",\"text_coherent\",\"text_frame_identification\",\"date\"]]\n",
    "df_channel_frame = df_channel[[\"text\",\"text_coherent\",\"text_frame_identification\",\"date\"]]\n",
    "\n",
    "df_greece_frame.to_csv(greece_url_fi)\n",
    "df_tigray_frame.to_csv(tigray_url_fi)\n",
    "df_rohingya_frame.to_csv(rohingya_url_fi)\n",
    "df_channel_frame.to_csv(channel_url_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = vectorizer.fit_transform(df_greece[\"frame_identification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dense = embeddings.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bartlett’s test of sphericity (want to have p-value of 0)\n",
    "chi_square_value, p_value=calculate_bartlett_sphericity(embeddings_dense)\n",
    "chi_square_value, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo_all,kmo_model=calculate_kmo(embeddings_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(kmo_all > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?FactorAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FactorAnalyzer(rotation=\"varimax\")\n",
    "fa.fit(embeddings_dense)\n",
    "print(\"Fit finished\")\n",
    "# Check Eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scree plot using matplotlib\n",
    "plt.scatter(range(1,embeddings_dense.shape[1]+1),ev)\n",
    "plt.plot(range(1,embeddings_dense.shape[1]+1),ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?fa.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FactorAnalyzer(9, rotation=\"varimax\")\n",
    "fa.fit(embeddings_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa.loadings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_dict =dict()\n",
    "for word,factor in zip(words_to_cluster,fa.loadings_):\n",
    "    factor_dict[word] = factor\n",
    "    \n",
    "factor_df = pd.DataFrame.from_dict(factor_dict)\n",
    "factor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df[\"criminal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df_transposed[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df_transposed = factor_df.T \n",
    "factor_df_transposed[factor_df_transposed[0]>0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df_transposed[factor_df_transposed[8]>0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict =dict()\n",
    "for word,vector in zip(words_to_cluster,embeddings):\n",
    "    vector_dict[word] = vector\n",
    "    \n",
    "vector_df = pd.DataFrame.from_dict(vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_embedding(df, words_to_cluster):\n",
    "    \n",
    "    tweet_sentences = [sent for tweet in df['text_alphanum'] for sent in sent_tokenize(tweet)]\n",
    "    sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "    bert_corpus = tweet_sentences + words_to_cluster\n",
    "\n",
    "    print(len(bert_corpus))\n",
    "    t0 = time()\n",
    "    document_embeddings = sbert_model.encode(bert_corpus)\n",
    "    print(f'Training embeddings took {time()-t0} seconds')\n",
    "    \n",
    "    return document_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357611\n",
      "Training embeddings took 17201.172611951828 seconds\n"
     ]
    }
   ],
   "source": [
    "greece_bert = create_bert_embedding(df_greece, words_to_cluster_greece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108610\n",
      "Training embeddings took 6313.1803567409515 seconds\n"
     ]
    }
   ],
   "source": [
    "tigray_bert = create_bert_embedding(df_tigray, words_to_cluster_tigray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68993\n",
      "Training embeddings took 3697.816387653351 seconds\n"
     ]
    }
   ],
   "source": [
    "rohingya_bert = create_bert_embedding(df_rohingya, words_to_cluster_rohingya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484502\n",
      "Training embeddings took 23848.87373805046 seconds\n"
     ]
    }
   ],
   "source": [
    "channel_bert = create_bert_embedding(df_channel, words_to_cluster_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('greece_frame_embeddings', greece_bert)\n",
    "pickle_file('tigray_frame_embeddings', tigray_bert)\n",
    "pickle_file('rohingya_frame_embeddings', rohingya_bert)\n",
    "pickle_file('channel_frame_embeddings', channel_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('greece_words_to_cluster', words_to_cluster_greece)\n",
    "pickle_file('tigray_words_to_cluster', words_to_cluster_tigray)\n",
    "pickle_file('rohingya_words_to_cluster', words_to_cluster_rohingya)\n",
    "pickle_file('channel_words_to_cluster', words_to_cluster_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "bert_corpus = tweet_sentences + words_to_cluster\n",
    "\n",
    "print(len(bert_corpus))\n",
    "t0 = time()\n",
    "document_embeddings = sbert_model.encode(bert_corpus)\n",
    "print(f'Training embeddings took {time()-t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-9f704284ad9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'document_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "words_embeddings = document_embeddings[len(tweet_sentences):]\n",
    "len(words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(vectorizer.get_feature_names(), embeddings.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentences_greece = [sent for tweet in df_greece['text_alphanum'] for sent in sent_tokenize(tweet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_greece = greece_bert[len(tweet_sentences_greece):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stopwords = [word for word in vectorizer.get_feature_names() if word not in words_to_cluster]\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords)\n",
    "\n",
    "embeddings = vectorizer.fit_transform(unique_tweets_df['text_alphanum'])\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tweet_sentences = [sent for tweet in unique_tweets_df['text_alphanum'] for sent in sent_tokenize(tweet)]\n",
    "len(tweet_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "bert_corpus = tweet_sentences + words_to_cluster\n",
    "\n",
    "print(len(bert_corpus))\n",
    "t0 = time()\n",
    "document_embeddings = sbert_model.encode(bert_corpus)\n",
    "print(f'Training embeddings took {time()-t0} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeddings = document_embeddings[len(tweet_sentences):]\n",
    "len(words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(vectorizer.get_feature_names(), embeddings.toarray()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeddings = load_pickle('greece_bert_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeddings = word_embeddings_greece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict =dict()\n",
    "for word,vector in zip(words_to_cluster_greece,words_embeddings):\n",
    "    vector_dict[word] = vector\n",
    "    \n",
    "vector_df = pd.DataFrame.from_dict(vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\factor_analyzer\\factor_analyzer.py:118: RuntimeWarning: divide by zero encountered in log\n",
      "  statistic = -np.log(corr_det) * (n - 1 - (2 * p + 5) / 6)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:1266: RuntimeWarning: invalid value encountered in subtract\n",
      "  return sc.xlogy(df/2.-1, x) - x/2. - sc.gammaln(df/2.) - (np.log(2)*df)/2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(inf, nan)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_square_value, p_value=calculate_bartlett_sphericity(vector_df)\n",
    "chi_square_value, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(vector_df)\n",
    "kmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmo_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.98170822e+02,  6.15643496e+01,  2.74392388e+01, ...,\n",
       "       -6.80665489e-15, -7.23782721e-15, -1.31274389e-14])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(vector_df)\n",
    "eigen_values, vectors = fa.get_eigenvalues()\n",
    "ev, v = fa.get_eigenvalues()\n",
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5591498 ,  0.26232961,  0.1421388 , ...,  0.52135674,\n",
       "         0.1564291 , -0.08579294],\n",
       "       [ 0.80202759,  0.2402963 ,  0.42277575, ...,  0.10855167,\n",
       "         0.14933181,  0.00148477],\n",
       "       [ 0.45446674,  0.75851588,  0.1387505 , ...,  0.17084086,\n",
       "         0.00321452,  0.17205969],\n",
       "       ...,\n",
       "       [ 0.26801312,  0.4440313 ,  0.03012718, ...,  0.13391058,\n",
       "         0.17683644, -0.04121899],\n",
       "       [ 0.65853832,  0.3494775 ,  0.23577034, ...,  0.09016037,\n",
       "         0.05169822, -0.01761102],\n",
       "       [-0.01549855,  0.60395185,  0.2936716 , ...,  0.23852288,\n",
       "        -0.16654519,  0.03617725]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa = FactorAnalyzer(8,rotation='varimax')\n",
    "fa.fit(vector_df)\n",
    "fa.loadings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>million</th>\n",
       "      <th>right</th>\n",
       "      <th>crisis</th>\n",
       "      <th>stop</th>\n",
       "      <th>help</th>\n",
       "      <th>camp</th>\n",
       "      <th>child</th>\n",
       "      <th>year</th>\n",
       "      <th>back</th>\n",
       "      <th>illegal</th>\n",
       "      <th>...</th>\n",
       "      <th>funded</th>\n",
       "      <th>leverage</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>weaponized</th>\n",
       "      <th>nine</th>\n",
       "      <th>naked</th>\n",
       "      <th>secure</th>\n",
       "      <th>alien</th>\n",
       "      <th>vessel</th>\n",
       "      <th>homeless</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.559150</td>\n",
       "      <td>0.802028</td>\n",
       "      <td>0.454467</td>\n",
       "      <td>0.467937</td>\n",
       "      <td>0.696474</td>\n",
       "      <td>0.428724</td>\n",
       "      <td>0.491324</td>\n",
       "      <td>0.567960</td>\n",
       "      <td>0.750028</td>\n",
       "      <td>0.189122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642741</td>\n",
       "      <td>0.566048</td>\n",
       "      <td>0.533897</td>\n",
       "      <td>0.480772</td>\n",
       "      <td>0.556380</td>\n",
       "      <td>0.089049</td>\n",
       "      <td>0.424473</td>\n",
       "      <td>0.268013</td>\n",
       "      <td>0.658538</td>\n",
       "      <td>-0.015499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.262330</td>\n",
       "      <td>0.240296</td>\n",
       "      <td>0.758516</td>\n",
       "      <td>0.678734</td>\n",
       "      <td>0.310521</td>\n",
       "      <td>0.164318</td>\n",
       "      <td>0.286453</td>\n",
       "      <td>0.203065</td>\n",
       "      <td>0.441203</td>\n",
       "      <td>0.788052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262760</td>\n",
       "      <td>0.480219</td>\n",
       "      <td>0.319887</td>\n",
       "      <td>0.464563</td>\n",
       "      <td>0.261044</td>\n",
       "      <td>0.587798</td>\n",
       "      <td>0.128603</td>\n",
       "      <td>0.444031</td>\n",
       "      <td>0.349477</td>\n",
       "      <td>0.603952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142139</td>\n",
       "      <td>0.422776</td>\n",
       "      <td>0.138751</td>\n",
       "      <td>0.087723</td>\n",
       "      <td>0.414351</td>\n",
       "      <td>0.225711</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.172113</td>\n",
       "      <td>0.153584</td>\n",
       "      <td>0.274067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525228</td>\n",
       "      <td>0.346378</td>\n",
       "      <td>0.236265</td>\n",
       "      <td>0.290491</td>\n",
       "      <td>0.242364</td>\n",
       "      <td>0.206138</td>\n",
       "      <td>0.764889</td>\n",
       "      <td>0.030127</td>\n",
       "      <td>0.235770</td>\n",
       "      <td>0.293672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.069198</td>\n",
       "      <td>-0.052656</td>\n",
       "      <td>0.168720</td>\n",
       "      <td>0.083585</td>\n",
       "      <td>0.260511</td>\n",
       "      <td>0.472473</td>\n",
       "      <td>0.369387</td>\n",
       "      <td>0.206922</td>\n",
       "      <td>0.133085</td>\n",
       "      <td>0.063767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172444</td>\n",
       "      <td>0.295814</td>\n",
       "      <td>0.491304</td>\n",
       "      <td>0.322113</td>\n",
       "      <td>0.078327</td>\n",
       "      <td>-0.024997</td>\n",
       "      <td>0.162073</td>\n",
       "      <td>0.170827</td>\n",
       "      <td>0.318039</td>\n",
       "      <td>0.304332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.095338</td>\n",
       "      <td>0.126932</td>\n",
       "      <td>0.067339</td>\n",
       "      <td>0.414472</td>\n",
       "      <td>0.019238</td>\n",
       "      <td>0.135389</td>\n",
       "      <td>0.062252</td>\n",
       "      <td>0.128646</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>0.299587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048517</td>\n",
       "      <td>-0.094704</td>\n",
       "      <td>0.089619</td>\n",
       "      <td>0.049355</td>\n",
       "      <td>0.266255</td>\n",
       "      <td>0.614700</td>\n",
       "      <td>0.171752</td>\n",
       "      <td>0.246698</td>\n",
       "      <td>0.080963</td>\n",
       "      <td>0.083757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.521357</td>\n",
       "      <td>0.108552</td>\n",
       "      <td>0.170841</td>\n",
       "      <td>0.084425</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.199444</td>\n",
       "      <td>0.027368</td>\n",
       "      <td>0.525760</td>\n",
       "      <td>0.230387</td>\n",
       "      <td>0.050137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>0.052107</td>\n",
       "      <td>0.029504</td>\n",
       "      <td>0.028232</td>\n",
       "      <td>0.367207</td>\n",
       "      <td>0.040128</td>\n",
       "      <td>0.020652</td>\n",
       "      <td>0.133911</td>\n",
       "      <td>0.090160</td>\n",
       "      <td>0.238523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.156429</td>\n",
       "      <td>0.149332</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>-0.013610</td>\n",
       "      <td>0.021649</td>\n",
       "      <td>-0.128033</td>\n",
       "      <td>0.090705</td>\n",
       "      <td>-0.035445</td>\n",
       "      <td>0.049393</td>\n",
       "      <td>-0.015276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070770</td>\n",
       "      <td>0.148605</td>\n",
       "      <td>0.219462</td>\n",
       "      <td>0.274342</td>\n",
       "      <td>0.112562</td>\n",
       "      <td>0.057057</td>\n",
       "      <td>0.180282</td>\n",
       "      <td>0.176836</td>\n",
       "      <td>0.051698</td>\n",
       "      <td>-0.166545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.085793</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.172060</td>\n",
       "      <td>-0.127447</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>-0.028697</td>\n",
       "      <td>0.183926</td>\n",
       "      <td>0.091577</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.123880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>0.215381</td>\n",
       "      <td>0.072823</td>\n",
       "      <td>0.150118</td>\n",
       "      <td>0.047483</td>\n",
       "      <td>-0.140126</td>\n",
       "      <td>0.061309</td>\n",
       "      <td>-0.041219</td>\n",
       "      <td>-0.017611</td>\n",
       "      <td>0.036177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1077 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    million     right    crisis      stop      help      camp     child  \\\n",
       "0  0.559150  0.802028  0.454467  0.467937  0.696474  0.428724  0.491324   \n",
       "1  0.262330  0.240296  0.758516  0.678734  0.310521  0.164318  0.286453   \n",
       "2  0.142139  0.422776  0.138751  0.087723  0.414351  0.225711  0.302220   \n",
       "3  0.069198 -0.052656  0.168720  0.083585  0.260511  0.472473  0.369387   \n",
       "4  0.095338  0.126932  0.067339  0.414472  0.019238  0.135389  0.062252   \n",
       "5  0.521357  0.108552  0.170841  0.084425  0.001089  0.199444  0.027368   \n",
       "6  0.156429  0.149332  0.003215 -0.013610  0.021649 -0.128033  0.090705   \n",
       "7 -0.085793  0.001485  0.172060 -0.127447  0.109134 -0.028697  0.183926   \n",
       "\n",
       "       year      back   illegal  ...    funded  leverage   vehicle  \\\n",
       "0  0.567960  0.750028  0.189122  ...  0.642741  0.566048  0.533897   \n",
       "1  0.203065  0.441203  0.788052  ...  0.262760  0.480219  0.319887   \n",
       "2  0.172113  0.153584  0.274067  ...  0.525228  0.346378  0.236265   \n",
       "3  0.206922  0.133085  0.063767  ...  0.172444  0.295814  0.491304   \n",
       "4  0.128646  0.203300  0.299587  ... -0.048517 -0.094704  0.089619   \n",
       "5  0.525760  0.230387  0.050137  ...  0.227349  0.052107  0.029504   \n",
       "6 -0.035445  0.049393 -0.015276  ...  0.070770  0.148605  0.219462   \n",
       "7  0.091577  0.016447  0.123880  ...  0.010893  0.215381  0.072823   \n",
       "\n",
       "   weaponized      nine     naked    secure     alien    vessel  homeless  \n",
       "0    0.480772  0.556380  0.089049  0.424473  0.268013  0.658538 -0.015499  \n",
       "1    0.464563  0.261044  0.587798  0.128603  0.444031  0.349477  0.603952  \n",
       "2    0.290491  0.242364  0.206138  0.764889  0.030127  0.235770  0.293672  \n",
       "3    0.322113  0.078327 -0.024997  0.162073  0.170827  0.318039  0.304332  \n",
       "4    0.049355  0.266255  0.614700  0.171752  0.246698  0.080963  0.083757  \n",
       "5    0.028232  0.367207  0.040128  0.020652  0.133911  0.090160  0.238523  \n",
       "6    0.274342  0.112562  0.057057  0.180282  0.176836  0.051698 -0.166545  \n",
       "7    0.150118  0.047483 -0.140126  0.061309 -0.041219 -0.017611  0.036177  \n",
       "\n",
       "[8 rows x 1077 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_dict =dict()\n",
    "for word,factor in zip(words_to_cluster,fa.loadings_):\n",
    "    factor_dict[word] = factor\n",
    "    \n",
    "factor_df = pd.DataFrame.from_dict(factor_dict)\n",
    "factor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>0.802028</td>\n",
       "      <td>0.240296</td>\n",
       "      <td>0.422776</td>\n",
       "      <td>-0.052656</td>\n",
       "      <td>0.126932</td>\n",
       "      <td>0.108552</td>\n",
       "      <td>0.149332</td>\n",
       "      <td>0.001485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>0.696474</td>\n",
       "      <td>0.310521</td>\n",
       "      <td>0.414351</td>\n",
       "      <td>0.260511</td>\n",
       "      <td>0.019238</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.021649</td>\n",
       "      <td>0.109134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back</th>\n",
       "      <td>0.750028</td>\n",
       "      <td>0.441203</td>\n",
       "      <td>0.153584</td>\n",
       "      <td>0.133085</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>0.230387</td>\n",
       "      <td>0.049393</td>\n",
       "      <td>0.016447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>0.744733</td>\n",
       "      <td>0.195558</td>\n",
       "      <td>0.384544</td>\n",
       "      <td>0.104071</td>\n",
       "      <td>0.117539</td>\n",
       "      <td>0.029671</td>\n",
       "      <td>0.262515</td>\n",
       "      <td>-0.139846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.613226</td>\n",
       "      <td>0.363217</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.173573</td>\n",
       "      <td>-0.027101</td>\n",
       "      <td>0.243353</td>\n",
       "      <td>0.040545</td>\n",
       "      <td>-0.013139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>couple</th>\n",
       "      <td>0.611650</td>\n",
       "      <td>0.216561</td>\n",
       "      <td>0.399804</td>\n",
       "      <td>0.270981</td>\n",
       "      <td>0.043119</td>\n",
       "      <td>0.145314</td>\n",
       "      <td>-0.047949</td>\n",
       "      <td>0.045120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gather</th>\n",
       "      <td>0.787129</td>\n",
       "      <td>0.264692</td>\n",
       "      <td>0.233996</td>\n",
       "      <td>0.273640</td>\n",
       "      <td>0.108069</td>\n",
       "      <td>0.098242</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>-0.055289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grow</th>\n",
       "      <td>0.684127</td>\n",
       "      <td>0.225808</td>\n",
       "      <td>0.238009</td>\n",
       "      <td>0.271731</td>\n",
       "      <td>0.086286</td>\n",
       "      <td>0.230837</td>\n",
       "      <td>0.218085</td>\n",
       "      <td>-0.006097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funded</th>\n",
       "      <td>0.642741</td>\n",
       "      <td>0.262760</td>\n",
       "      <td>0.525228</td>\n",
       "      <td>0.172444</td>\n",
       "      <td>-0.048517</td>\n",
       "      <td>0.227349</td>\n",
       "      <td>0.070770</td>\n",
       "      <td>0.010893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vessel</th>\n",
       "      <td>0.658538</td>\n",
       "      <td>0.349477</td>\n",
       "      <td>0.235770</td>\n",
       "      <td>0.318039</td>\n",
       "      <td>0.080963</td>\n",
       "      <td>0.090160</td>\n",
       "      <td>0.051698</td>\n",
       "      <td>-0.017611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "right   0.802028  0.240296  0.422776 -0.052656  0.126932  0.108552  0.149332   \n",
       "help    0.696474  0.310521  0.414351  0.260511  0.019238  0.001089  0.021649   \n",
       "back    0.750028  0.441203  0.153584  0.133085  0.203300  0.230387  0.049393   \n",
       "open    0.744733  0.195558  0.384544  0.104071  0.117539  0.029671  0.262515   \n",
       "news    0.613226  0.363217  0.261500  0.173573 -0.027101  0.243353  0.040545   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "couple  0.611650  0.216561  0.399804  0.270981  0.043119  0.145314 -0.047949   \n",
       "gather  0.787129  0.264692  0.233996  0.273640  0.108069  0.098242  0.100004   \n",
       "grow    0.684127  0.225808  0.238009  0.271731  0.086286  0.230837  0.218085   \n",
       "funded  0.642741  0.262760  0.525228  0.172444 -0.048517  0.227349  0.070770   \n",
       "vessel  0.658538  0.349477  0.235770  0.318039  0.080963  0.090160  0.051698   \n",
       "\n",
       "               7  \n",
       "right   0.001485  \n",
       "help    0.109134  \n",
       "back    0.016447  \n",
       "open   -0.139846  \n",
       "news   -0.013139  \n",
       "...          ...  \n",
       "couple  0.045120  \n",
       "gather -0.055289  \n",
       "grow   -0.006097  \n",
       "funded  0.010893  \n",
       "vessel -0.017611  \n",
       "\n",
       "[588 rows x 8 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_df_transposed = factor_df.T \n",
    "factor_df_transposed[factor_df_transposed[0]>0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering frame modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import umap.umap_ as umap\n",
    "start = time()\n",
    "reducer = umap.UMAP(random_state=42,n_components=3)\n",
    "reduced_embedding = reducer.fit_transform(words_embeddings)\n",
    "print(f'Duration: {time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "start = time()\n",
    "cluster_labels = DBSCAN(min_samples=6).fit_predict(reduced_embedding)\n",
    "print(f'Duration: {time() - start} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "start = time()\n",
    "cluster_labels = AffinityPropagation().fit_predict(reduced_embedding)\n",
    "print(f'Duration: {time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_clusters = KMeans(n_clusters=8, random_state=42).fit_predict(reduced_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "seaborn.scatterplot(x = reducer.embedding_[:, 0],\n",
    "                y = reducer.embedding_[:, 1],\n",
    "                hue = cluster_labels, palette =\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "x = reducer.embedding_[:, 0]\n",
    "y = reducer.embedding_[:, 1]\n",
    "z = reducer.embedding_[:, 2]\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "\n",
    "ax.scatter(x, y, z, c = cluster_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labeled_tweets = pd.DataFrame({'word': words_to_cluster,'label':cluster_labels})\n",
    "\n",
    "# Create documents per label\n",
    "docs_per_class = labeled_tweets.groupby(['label'], as_index=False).agg({'word': ' '.join})\n",
    "\n",
    "words_per_class = dict()\n",
    "for label,word in zip(docs_per_class['label'],docs_per_class['word']):\n",
    "    words_per_class[label] = word.split(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in words_per_class.items() ])).fillna('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "def kmean_test_n_clusters(data, n_clusters):\n",
    "    \"\"\"\n",
    "    Takes the document vectors and the maximum amount of clusters to look for. \n",
    "    Performs KMeans algorithm on the dataset for each amount of clusters. \n",
    "    Calculates silhouette score and interias for each amount of clusters. \n",
    "    Plots the scores as a function of the amount of clusters.\n",
    "    \n",
    "    Arguments: \n",
    "    data -- document vectors as numpy matrices\n",
    "    n_clusters -- integer that determines the maximum amount of clusters to test\n",
    "    \n",
    "    Returns: \n",
    "    Prints the scores as functions of the clusters in range 1, n_clusters\n",
    "    \"\"\"\n",
    "    n_clusters += 1\n",
    "    kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(data) for k in tqdm(range(1, n_clusters))]\n",
    "    print(\"clusters done\")\n",
    "    inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "    print(\"inertias done\")\n",
    "    silhouette_scores = [silhouette_score(data, model.labels_)\n",
    "                         for model in tqdm(kmeans_per_k[1:])]\n",
    "    print(\"silhouettes done\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, figsize=(8, 3.5))\n",
    "\n",
    "    ax1.plot(range(1, n_clusters), inertias, \"bo-\")\n",
    "    ax1.set_xlabel(\"$k$\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Inertia\", fontsize=14)\n",
    "    #ax1.annotate('Elbow',\n",
    "    #             xy=(4, inertias[3]),\n",
    "    #             xytext=(0.55, 0.55),\n",
    "    #             textcoords='figure fraction',\n",
    "    #             fontsize=16,\n",
    "    #             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "    #            )\n",
    "    ax2.plot(range(2, n_clusters), silhouette_scores, \"bo-\")\n",
    "    ax2.set_xlabel(\"$k$\", fontsize=14)\n",
    "    ax2.set_ylabel(\"Silhouette score\", fontsize=14)\n",
    "    #plt.axis([2, 8, 0.3, 0.475])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_test_n_clusters(reduced_embedding, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train this model only after the first merging step to save both memory and time\n",
    "words_to_cluster =  [\"accept\" ,        \"ally\"     ,      \"army\"     ,      \"attack\"     ,    \"attacking\"  ,    \"authority\"   ,   \"benefit\"       \n",
    ",\"billion\"    ,    \"blackmail\"    ,  \"block\"   ,       \"boat\"    ,       \"bomb\"     ,      \"bombing\"  ,      \"border\"        \n",
    ",\"break\"       ,   \"build\"   ,       \"burden\"    ,     \"camp\" ,          \"care\"      ,     \"child\",          \"citizen\"       \n",
    ",\"city\"       ,    \"civil\"    ,      \"civilian\" ,      \"clash\" ,         \"closed\"         ,\"coast\" ,         \"community\"     \n",
    ",\"conflict\"  ,     \"control\"   ,     \"creating\",       \"crime\"  ,        \"criminal\"      , \"cross\"  ,        \"crossing\"      \n",
    ",\"dead\"     ,      \"death\"      ,    \"defend\" ,        \"desperate\",      \"dictator\"     ,  \"displaced\",      \"door\"          \n",
    ",\"economic\",       \"economy\"     ,   \"entering\"       ,\"entry\"     ,     \"family\"      ,   \"fear\"      ,     \"fence\"         \n",
    ",\"fight\"          ,\"fighting\"     ,  \"fire\"          , \"fled\"       ,    \"flee\"       ,    \"fleeing\"    ,    \"flood\"         \n",
    ", \"flow\"         ,  \"food\"         ,  \"force\"       ,   \"forced\"     ,    \"foreign\"  ,      \"friend\"     ,    \"game\"          \n",
    " ,\"gate\"        ,   \"government\"    , \"guard\"      ,    \"health\"      ,   \"help\"    ,       \"helping\"     ,   \"history\"       \n",
    ", \"hold\"       ,    \"hope\"           ,\"host\"      ,     \"hosting\"      ,  \"house\"  ,        \"human\"        ,  \"humanitarian\"  \n",
    ",\"humanity\"   ,    \"hundred\"  ,      \"illegal\"   ,     \"illegally\"      ,\"influx\" ,        \"innocent\"       ,\"invade\"        \n",
    ", \"invader\"  ,      \"invading\" ,      \"invasion\",       \"islamic\"        ,\"jihadist\"      , \"kid\" ,           \"kill\"          \n",
    ",\"killed\"   ,      \"killing\"    ,    \"leaving\" ,       \"legal\"   ,       \"march\"         , \"mass\"  ,         \"military\"      \n",
    ", \"million\"       , \"minister\"   ,    \"money\"         , \"movement\",       \"national\"    ,   \"number\",         \"official\"      \n",
    ", \"opening\"      ,  \"order\"       ,   \"peace\"        ,  \"picture\"  ,      \"police\"     ,    \"policy\" ,        \"population\"    \n",
    ", \"power\"       ,   \"pressure\"     ,  \"prevent\"     ,   \"protect\"   ,     \"protection\",     \"pushing\" ,       \"refuge\"        \n",
    ",\"regime\"      ,   \"region\"         ,\"respect\"     ,   \"responsibility\", \"return\"    ,     \"risk\"      ,     \"rule\"          \n",
    ",\"safety\"     ,    \"school\"    ,     \"security\"   ,    \"shelter\" ,       \"shooting\" ,      \"shot\"       ,    \"social\"        \n",
    ",\"soldier\"      ,  \"solidarity\" ,    \"solution\"  ,     \"suffering\",      \"support\" ,       \"supporting\"  ,   \"tension\"       \n",
    ", \"territory\"  ,    \"terrorist\"  ,    \"thousand\",       \"threat\"   ,      \"travel\",         \"troop\"       ,   \"violence\"      \n",
    ", \"war\"       ,     \"wave\"        ,   \"weapon\" ,        \"woman\"     ,     \"work\" ,          \"worker\"       ,  \"working\"       \n",
    ", \"zone\"]\n",
    "\n",
    "print(unique_tweets_df['text_alphanum'].shape)\n",
    "len(words_to_cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
