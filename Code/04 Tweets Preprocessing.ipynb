{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b74034",
   "metadata": {},
   "source": [
    "# Tweets Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8539ecec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import preprocessor\n",
    "from ekphrasis.classes.tokenizer import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer \n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "#from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "\n",
    "# ekphrasis tokenizer gives more freedom to adjust the way the tokens are split\n",
    "social_pipeline = [\"TAG\", \"EMAIL\", \"USER\", \"HASHTAG\", \"CASHTAG\", \"PHONE\", \"PERCENT\", \"NUMBER\",\"WORD\"]\n",
    "tokenizer = Tokenizer(pipeline = social_pipeline, lowercase=False).tokenize\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "#spell_cor = SpellCorrector(corpus=\"english\") #spell correction did not perform well \n",
    "seg_eng = Segmenter(corpus=\"english\") \n",
    "\n",
    "# preprocessor setting to remove emojis and urls in the tweets\n",
    "preprocessor.set_options(preprocessor.OPT.URL, preprocessor.OPT.EMOJI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8525232b",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "Loading the event dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2a836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = os.getcwd() + \"/../../../../\" + r\"Dropbox (CBS)/Master thesis data/\"\n",
    "event_url = file_url + r\"Event Dataframes/\"\n",
    "event_url_raw = event_url + r\"Raw/\"\n",
    "event_url_clean = event_url + r\"Clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d7da343",
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_url_raw = event_url_raw + r\"df_tigray.csv\" # location of Tigray dataset\n",
    "greece_url_raw = event_url_raw + r\"df_greece.csv\" # location of Greece dataset\n",
    "rohingya_url_raw = event_url_raw + r\"df_rohingya.csv\" # location of Rohingya dataset\n",
    "channel_url_raw = event_url_raw + r\"df_channel.csv\" # Location of channel dataset\n",
    "afghan_url_raw = event_url_raw + r\"df_afghanistan.csv\" # Location of afghanistan dataset\n",
    "\n",
    "tigray_url_clean = event_url_clean + r\"df_tigray_clean.csv\" # location of clean Tigray dataset\n",
    "greece_url_clean = event_url_clean + r\"df_greece_clean.csv\" # location of clean Greece dataset\n",
    "rohingya_url_clean = event_url_clean + r\"df_rohingya_clean.csv\" # location clean of Rohingya dataset\n",
    "channel_url_clean = event_url_clean +r\"df_channel_clean.csv\" #Location of clean Channel dataset\n",
    "afghan_url_clean = event_url_clean + r\"df_afghanistan_clean.csv\" # Location of afghanistan dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d4a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 283643 tweets!\n"
     ]
    }
   ],
   "source": [
    "def read_event_df(data_url):\n",
    "    # easy dataframe load\n",
    "    event_df = pd.read_csv(data_url, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df(afghan_url_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47497473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading users dataframe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3166: DtypeWarning: Columns (13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# loading the dataframe with users to replace the mentions with twitter names\n",
    "\n",
    "users_url = file_url + \"/df_users.csv\"\n",
    "\n",
    "# Read the users csv\n",
    "print(\"loading users dataframe...\")\n",
    "df_users = pd.read_csv(users_url)\n",
    "\n",
    "# Drop unnecessary index column\n",
    "df_users.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_users.head()\n",
    "\n",
    "# Create dict that maps usernames to actual names\n",
    "mapping = dict(df_users[[\"username\",\"name\"]].values)\n",
    "mapping = {f'@{key}': value for key, value in mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4012d",
   "metadata": {},
   "source": [
    "## 2. Parsing corpus: Removing non-syntactic information to obtain more coherent sentences\n",
    "The process consists of:\n",
    "1. cleaning the tweet\n",
    "2. mapping the mentions to twitter names\n",
    "3. removing duplicate rows based on the clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2268fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    The goal of the function is to yield coherent sentences from raw tweets (without hashtags, URLs, emojis) \n",
    "    '''\n",
    "    #remove emojis, links \n",
    "    tweet = preprocessor.clean(tweet)\n",
    "    \n",
    "    # using social tokenizer from ekphrasis due to potentially improper text structure\n",
    "    tweet = tokenizer(tweet)\n",
    "    \n",
    "    #removing the irrelevant hashtags and mention using the heuristic that mentions in the beginning of the tweet \n",
    "    # and at least 2 consecutive hashtags at the end of the tweet carry no valuable information\n",
    "    try:\n",
    "        while tweet[0].startswith('@'):\n",
    "            tweet.remove(tweet[0])\n",
    "\n",
    "        if tweet[-1].startswith('@') and tweet[-2].startswith('@'):\n",
    "            while tweet[-1].startswith('@'):\n",
    "                tweet.remove(tweet[-1])\n",
    "\n",
    "        if tweet[-1].startswith('#') and tweet[-2].startswith('#'):\n",
    "            while tweet[-1].startswith('#'):\n",
    "                tweet.remove(tweet[-1])\n",
    "                \n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    #for hashtags that may carry information, we remove the # and split the word into more if applicable\n",
    "    for word in range(len(tweet)):\n",
    "        if tweet[word].startswith('#'):\n",
    "            tweet[word] = tweet[word].replace('#','')\n",
    "            tweet[word] = seg_eng.segment(tweet[word])\n",
    "\n",
    "        # potentially correct spelling - but it is not working very well - corrects numbers to weird words\n",
    "        #tweet[word] = spell_cor.correct(tweet[word])\n",
    "\n",
    "    # instead of .join we use detokenizer in order to reconstruct the cleaned sentence in a better way\n",
    "    #sample_df[twt] =  \" \".join(tweet) \n",
    "    tweet = detokenizer.detokenize(tweet)\n",
    "    \n",
    "    \n",
    "    #  tweets that end up being empty after preprocessing will cause problems when batching, replace empty tweet with 'no_tweet_text' which we can ignore later\n",
    "    tweet = 'no_tweet_text' if len(tweet)==0 else tweet\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b32e522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 283643/283643 [01:44<00:00, 2723.12it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas() # allowing progress bar on .apply method (== .progress_apply)\n",
    "event_df['parsing_corpus'] = event_df['text'].progress_apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b3e38",
   "metadata": {},
   "source": [
    "### Replace @username by screen name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddcd4304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 283643/283643 [00:02<00:00, 118478.30it/s]\n"
     ]
    }
   ],
   "source": [
    "def resolve_username_to_name(text):\n",
    "    new_text = text\n",
    "    for word in text.split(\" \"):\n",
    "        if word in mapping:\n",
    "            new_text = new_text.replace(word,mapping[word])\n",
    "    return new_text\n",
    "\n",
    "#tqdm.pandas()\n",
    "event_df['parsing_corpus'] = event_df['parsing_corpus'].progress_apply(resolve_username_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e65bb",
   "metadata": {},
   "source": [
    "### Remove Duplicate Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585735ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Priti Patel: Call on the UK government to resettle 20,000 Afghan refugees - Sign the Petition! via Change.org UK                                                                                                                             2477\n",
       "please expedite Special Immigrant Visas and evacuate applicants and their families . Translators, aid workers, and more, are suffering - we must act IMMEDIATELY.                                                                            1288\n",
       "I' ve emailed my MP to let them know we need new safe routes for Afghan refugees . safe routes save lives                                                                                                                                     704\n",
       "Britain promised a better life for the people of Afghanistan now it must help them to escape from the Taliban takeover . refugees welcome Sign the petition:                                                                                  416\n",
       "As Kabul fell to the Taliban, the EU started to worry about migrants flooding the EU! Trying to give another life to tpl fisa terrorist group will destabilize the HoA and result in the same thing! FRANCE 24 English The New York Times     377\n",
       "Name: parsing_corpus, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can see that the dataset contains a lot of duplicate tweets\n",
    "event_df[\"parsing_corpus\"].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7bb3409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsing_corpus</th>\n",
       "      <th>retweet_count_sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! Chartering direct evacuation flights, as the...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>! Department of State President Biden The Whit...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! My country is in chaos, thousand of innocent...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>! Now you' ve inadvertantly put yourself in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!! Afghan Refugees (UNHRC) European Migrant Cr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      parsing_corpus  retweet_count_sum  count\n",
       "0  ! Chartering direct evacuation flights, as the...                  0      1\n",
       "1  ! Department of State President Biden The Whit...                  4      1\n",
       "2  ! My country is in chaos, thousand of innocent...                  4      5\n",
       "3  ! Now you' ve inadvertantly put yourself in th...                  0      1\n",
       "4  !! Afghan Refugees (UNHRC) European Migrant Cr...                  0      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Grouping by tweet text, keep count of retweets\n",
    "event_df_grouped = event_df[[\"parsing_corpus\",\"retweet_count\"]].groupby(\"parsing_corpus\").agg({\"retweet_count\":[\"sum\",\"count\"]}).reset_index()\n",
    "event_df_grouped.columns = list(map(''.join, event_df_grouped.columns.values))\n",
    "event_df_grouped = event_df_grouped.rename(columns={\"retweet_countsum\":\"retweet_count_sum\",\"retweet_countcount\":\"count\"})\n",
    "event_df_grouped[\"retweet_count_sum\"] = event_df_grouped[\"retweet_count_sum\"] + event_df_grouped[\"count\"] - 1 #take into account that only the retweets of a similar tweet but also the tweet iself is supposed to be treated as a retweet\n",
    "event_df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c529063",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicate tweets\n",
    "event_df_sorted = event_df.sort_values(\"created_at\") #df should be sorted by default but this step ensures that sorting is there\n",
    "event_df_no_dups = event_df_sorted.drop_duplicates(\"parsing_corpus\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6c746f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge grouped data together\n",
    "event_df_no_dups1 = pd.merge(left = event_df_no_dups,\n",
    "                             right = event_df_grouped,\n",
    "                             left_on = \"parsing_corpus\",\n",
    "                             right_on = \"parsing_corpus\",\n",
    "                             how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d2d9889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if numbers add up\n",
    "event_df_no_dups1[\"count\"].sum() == event_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043c317",
   "metadata": {},
   "source": [
    "## 3. Frame identification corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49403432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikodemicek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikodemicek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikodemicek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def tokenization(df_col):\n",
    "    \"\"\"\n",
    "    Takes a list with strings and returns a list with tokens\n",
    "    \"\"\"\n",
    "    print(\"Tokenizing tweets...\\n\")\n",
    "    return df_col.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "\n",
    "def lowercase(df_col):\n",
    "    \"\"\"\n",
    "    #Returns all tokens in lowercase.\n",
    "    \"\"\"\n",
    "    print(\"Making all words lowercase...\\n\")\n",
    "    return df_col.apply(lambda x: [token.lower() for token in x])\n",
    "\n",
    "\n",
    "def only_alphabetic(df_col):\n",
    "    \"\"\"\n",
    "    Keeps only tokens which are alphabetic or an underscore and returns them.\n",
    "    \"\"\"\n",
    "    print(\"Removing all non-alphabetic words...\\n\")\n",
    "    return df_col.apply(lambda x: [token for token in x if re.match(\"^[a-zA-Z0_]*$\", token)])\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\"\"\"stop_words.update([\"refugee\",\"refugees\",\"migrant\",\"migrants\",\"immigrant\",\"immigrants\",\n",
    "                   \"like\", \"would\",\"want\",\"take\",\"must\",\"well\",\"could\",\"even\",\"since\",\n",
    "                   \"also\",\"know\"])\"\"\"\n",
    "\n",
    "def stopword_removal(df_col):\n",
    "    \"\"\"\n",
    "    Removes all words considered as stopwords and all words that have a length of three or less.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Removing Stopwords...\\n\")\n",
    "\n",
    "    return df_col.apply(lambda x: [token for token in x if token not in stop_words and len(token) > 3])\n",
    "\n",
    "\n",
    "def lemmatization(df_col):\n",
    "    \"\"\"\n",
    "    Applies lemmatization to all tokens and returns them afterwards.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Lemmatizing words...\\n\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return df_col.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "\n",
    "def preprocessing(df_col, *steps):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe column with text and applies preprocessing steps given \n",
    "    in and returns a string.\n",
    "    \n",
    "    Input:\n",
    "    - df (dataframe): The dataframe containing the text column.\n",
    "    - steps (functions): Multiple functions for preprocessing can be given in.\n",
    "    \n",
    "    Output:\n",
    "    - List with strings.\n",
    "    \"\"\"\n",
    "    # copying over the column for preprocessing\n",
    "    temp = df_col.copy()\n",
    "    for func in steps:\n",
    "        temp = func(temp)\n",
    "    return temp.apply(lambda x: \" \".join([token for token in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29be1f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Making all words lowercase...\n",
      "\n",
      "Removing all non-alphabetic words...\n",
      "\n",
      "Removing Stopwords...\n",
      "\n",
      "Lemmatizing words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_df[\"frame_identification_corpus\"] = preprocessing(event_df[\"parsing_corpus\"],\n",
    "                                            tokenization,\n",
    "                                            lowercase,\n",
    "                                            only_alphabetic,\n",
    "                                            stopword_removal,\n",
    "                                            lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "773cf12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year_calendar_week</th>\n",
       "      <th>date</th>\n",
       "      <th>refugee</th>\n",
       "      <th>migrant</th>\n",
       "      <th>immigrant</th>\n",
       "      <th>asylum_seeker</th>\n",
       "      <th>other</th>\n",
       "      <th>parsing_corpus</th>\n",
       "      <th>frame_identification_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1419447696558108674</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>2021-07-26 00:01:17+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>50626909</td>\n",
       "      <td>Biden authorizes up to $100M for Afghan refuge...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_7</td>\n",
       "      <td>2021_30</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Biden authorizes up to $100M for Afghan refugees</td>\n",
       "      <td>biden authorizes afghan refugee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419449166904713220</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>2021-07-26 00:07:08+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1372157932943503363</td>\n",
       "      <td>@faheem2430 @USAmbKabul @StateDeputySpox gandu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_7</td>\n",
       "      <td>2021_30</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>gandu afghani . . . . . maderchood u in refuge...</td>\n",
       "      <td>gandu afghani maderchood refugee camp iran int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1419449634179534850</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2021-07-26 00:08:59+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>159060632</td>\n",
       "      <td>Hazara refugees urge Australian government to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_7</td>\n",
       "      <td>2021_30</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Hazara refugees urge Australian government to ...</td>\n",
       "      <td>hazara refugee urge australian government help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1419450730063224833</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>2021-07-26 00:13:20+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1144443713994678273</td>\n",
       "      <td>\"US has a moral obligation to Afghan allies..c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_7</td>\n",
       "      <td>2021_30</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>\" US has a moral obligation to Afghan allies ....</td>\n",
       "      <td>moral obligation afghan ally conflict future c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1419453607284510721</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>2021-07-26 00:24:46+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>162114001</td>\n",
       "      <td>Fleeing fighting and hoping to head toward som...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021_7</td>\n",
       "      <td>2021_30</td>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Fleeing fighting and hoping to head toward som...</td>\n",
       "      <td>fleeing fighting hoping head toward something ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id               source                 created_at lang  \\\n",
       "0  1419447696558108674      Twitter Web App  2021-07-26 00:01:17+00:00   en   \n",
       "1  1419449166904713220  Twitter for Android  2021-07-26 00:07:08+00:00   en   \n",
       "2  1419449634179534850   Twitter for iPhone  2021-07-26 00:08:59+00:00   en   \n",
       "3  1419450730063224833  Twitter for Android  2021-07-26 00:13:20+00:00   en   \n",
       "4  1419453607284510721            TweetDeck  2021-07-26 00:24:46+00:00   en   \n",
       "\n",
       "             author_id                                               text  \\\n",
       "0             50626909  Biden authorizes up to $100M for Afghan refuge...   \n",
       "1  1372157932943503363  @faheem2430 @USAmbKabul @StateDeputySpox gandu...   \n",
       "2            159060632  Hazara refugees urge Australian government to ...   \n",
       "3  1144443713994678273  \"US has a moral obligation to Afghan allies..c...   \n",
       "4            162114001  Fleeing fighting and hoping to head toward som...   \n",
       "\n",
       "   retweet_count  reply_count  like_count  quote_count  ...  year_month  \\\n",
       "0              1            0           0            0  ...      2021_7   \n",
       "1              0            0           0            0  ...      2021_7   \n",
       "2              1            0           0            0  ...      2021_7   \n",
       "3              0            0           1            0  ...      2021_7   \n",
       "4              2            1           8            0  ...      2021_7   \n",
       "\n",
       "  year_calendar_week        date refugee migrant  immigrant  asylum_seeker  \\\n",
       "0            2021_30  2021-07-26    True   False      False          False   \n",
       "1            2021_30  2021-07-26    True   False      False          False   \n",
       "2            2021_30  2021-07-26    True   False      False          False   \n",
       "3            2021_30  2021-07-26   False   False      False          False   \n",
       "4            2021_30  2021-07-26    True   False      False          False   \n",
       "\n",
       "   other                                     parsing_corpus  \\\n",
       "0  False   Biden authorizes up to $100M for Afghan refugees   \n",
       "1  False  gandu afghani . . . . . maderchood u in refuge...   \n",
       "2  False  Hazara refugees urge Australian government to ...   \n",
       "3  False  \" US has a moral obligation to Afghan allies ....   \n",
       "4  False  Fleeing fighting and hoping to head toward som...   \n",
       "\n",
       "                         frame_identification_corpus  \n",
       "0                    biden authorizes afghan refugee  \n",
       "1  gandu afghani maderchood refugee camp iran int...  \n",
       "2  hazara refugee urge australian government help...  \n",
       "3  moral obligation afghan ally conflict future c...  \n",
       "4  fleeing fighting hoping head toward something ...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31a655bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tedcruz Thanks to the Ttump administration, Miller et al, they royally screwed up the visa process after releasing 5000 Taliban fighters. \n",
      "That was their plan all along. Screw the Allies.\n",
      "https://t.co/2MLjwDy76L\n",
      "---\n",
      "Thanks to the Ttump administration, Miller et al, they royally screwed up the visa process after releasing 5000 Taliban fighters . That was their plan all along . Screw the Allies.\n",
      "---\n",
      "thanks ttump administration miller royally screwed visa process releasing taliban fighter plan along screw ally\n",
      "-----------------------------------\n",
      "@MrsOhSehun already accommodating 3 million afghan refugees with meager resources, apart from that my country is equally as miserable as yours.\n",
      "---\n",
      "already accommodating 3 million afghan refugees with meager resources, apart from that my country is equally as miserable as yours.\n",
      "---\n",
      "already accommodating million afghan refugee meager resource apart country equally miserable\n",
      "-----------------------------------\n",
      "Let’s get some facts straight. \n",
      "\n",
      "Yes there are a considerable amount of Afghan refugees in Pakistan.\n",
      "\n",
      "But have any of you who keep using this line, even recognised how they are treated? Denied basic human rights. \n",
      "\n",
      "That’s not even me saying it, read here: https://t.co/c1VyZZ1MTz\n",
      "---\n",
      "Lets get some facts straight . Yes there are a considerable amount of Afghan refugees in Pakistan . But have any of you who keep using this line, even recognised how they are treated? Denied basic human rights . Thats not even me saying it, read here:\n",
      "---\n",
      "let fact straight considerable amount afghan refugee pakistan keep using line even recognised treated denied basic human right thats even saying read\n",
      "-----------------------------------\n",
      "@davoszkey @JacquiHeinrich Therese…the terrorist threat is here. It’s from all the people involved in the insurrection on 1/6 and all the Republicans who support them and are STILL lying about the election. THOSE are the terrorists. Not Afghan refugees.\n",
      "---\n",
      "Theresethe terrorist threat is here . Its from all the people involved in the insurrection on 1 / 6 and all the Republicans who support them and are STILL lying about the election . THOSE are the terrorists . Not Afghan refugees.\n",
      "---\n",
      "theresethe terrorist threat people involved insurrection republican support still lying election terrorist afghan refugee\n",
      "-----------------------------------\n",
      "@lancesalyers @SpencerJCox @AbbyPalmerCox Europe took in Afghan refugees, and discovered that some are not refugees but terrorists looking to infiltrate. Will UT vet these refugees? How?\n",
      "---\n",
      "Europe took in Afghan refugees, and discovered that some are not refugees but terrorists looking to infiltrate . Will UT vet these refugees? How?\n",
      "---\n",
      "europe took afghan refugee discovered refugee terrorist looking infiltrate refugee\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,line in event_df[[\"text\",\"parsing_corpus\",\"frame_identification_corpus\"]].sample(5).iterrows():\n",
    "    print(line[\"text\"])\n",
    "    print(\"---\")\n",
    "    print(line[\"parsing_corpus\"])\n",
    "    print(\"---\")\n",
    "    print(line[\"frame_identification_corpus\"])\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe912c6",
   "metadata": {},
   "source": [
    "## Save the dataframe with clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "582643cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df.to_csv(afghan_url_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e75722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
