{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b74034",
   "metadata": {},
   "source": [
    "# Tweets Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8539ecec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import preprocessor\n",
    "from ekphrasis.classes.tokenizer import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer \n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "#from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "\n",
    "# ekphrasis tokenizer gives more freedom to adjust the way the tokens are split\n",
    "social_pipeline = [\"TAG\", \"EMAIL\", \"USER\", \"HASHTAG\", \"CASHTAG\", \"PHONE\", \"PERCENT\", \"NUMBER\",\"WORD\"]\n",
    "tokenizer = Tokenizer(pipeline = social_pipeline, lowercase=False).tokenize\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "#spell_cor = SpellCorrector(corpus=\"english\") #spell correction did not perform well \n",
    "seg_eng = Segmenter(corpus=\"english\") \n",
    "\n",
    "# preprocessor setting to remove emojis and urls in the tweets\n",
    "preprocessor.set_options(preprocessor.OPT.URL, preprocessor.OPT.EMOJI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8525232b",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "Loading the event dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2a836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = os.getcwd() + \"/../../../../\" + r\"/Dropbox (CBS)/Master thesis data/\"\n",
    "event_url = file_url + r\"Event Dataframes/\"\n",
    "event_url_raw = event_url + r\"Raw/\"\n",
    "event_url_clean = event_url + r\"Clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7da343",
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_url_raw = event_url_raw + r\"df_tigray.csv\" # location of Tigray dataset\n",
    "greece_url_raw = event_url_raw + r\"df_greece.csv\" # location of Greece dataset\n",
    "rohingya_url_raw = event_url_raw + r\"df_rohingya.csv\" # location of Rohingya dataset\n",
    "channel_url_raw = event_url_raw + r\"df_channel.csv\" # Location of channel dataset\n",
    "\n",
    "tigray_url_clean = event_url_clean + r\"df_tigray_clean.csv\" # location of clean Tigray dataset\n",
    "greece_url_clean = event_url_clean + r\"df_greece_clean.csv\" # location of clean Greece dataset\n",
    "rohingya_url_clean = event_url_clean + r\"df_rohingya_clean.csv\" # location clean of Rohingya dataset\n",
    "channel_url_clean = event_url_clean +r\"df_channel_clean.csv\" #Location of clean Channel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d4a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 175900 tweets!\n"
     ]
    }
   ],
   "source": [
    "def read_event_df(data_url):\n",
    "    # easy dataframe load\n",
    "    event_df = pd.read_csv(data_url, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df(greece_url_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47497473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading users dataframe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3166: DtypeWarning: Columns (13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# loading the dataframe with users to replace the mentions with twitter names\n",
    "\n",
    "users_url = file_url + \"/df_users.csv\"\n",
    "\n",
    "# Read the users csv\n",
    "print(\"loading users dataframe...\")\n",
    "df_users = pd.read_csv(users_url)\n",
    "\n",
    "# Drop unnecessary index column\n",
    "df_users.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_users.head()\n",
    "\n",
    "# Create dict that maps usernames to actual names\n",
    "mapping = dict(df_users[[\"username\",\"name\"]].values)\n",
    "mapping = {f'@{key}': value for key, value in mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4012d",
   "metadata": {},
   "source": [
    "## 2. Parsing corpus: Removing non-syntactic information to obtain more coherent sentences\n",
    "The process consists of:\n",
    "1. cleaning the tweet\n",
    "2. mapping the mentions to twitter names\n",
    "3. removing duplicate rows based on the clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2268fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    The goal of the function is to yield coherent sentences from raw tweets (without hashtags, URLs, emojis) \n",
    "    '''\n",
    "    #remove emojis, links \n",
    "    tweet = preprocessor.clean(tweet)\n",
    "    \n",
    "    # using social tokenizer from ekphrasis due to potentially improper text structure\n",
    "    tweet = tokenizer(tweet)\n",
    "    \n",
    "    #removing the irrelevant hashtags and mention using the heuristic that mentions in the beginning of the tweet \n",
    "    # and at least 2 consecutive hashtags at the end of the tweet carry no valuable information\n",
    "    try:\n",
    "        while tweet[0].startswith('@'):\n",
    "            tweet.remove(tweet[0])\n",
    "\n",
    "        if tweet[-1].startswith('@') and tweet[-2].startswith('@'):\n",
    "            while tweet[-1].startswith('@'):\n",
    "                tweet.remove(tweet[-1])\n",
    "\n",
    "        if tweet[-1].startswith('#') and tweet[-2].startswith('#'):\n",
    "            while tweet[-1].startswith('#'):\n",
    "                tweet.remove(tweet[-1])\n",
    "                \n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    #for hashtags that may carry information, we remove the # and split the word into more if applicable\n",
    "    for word in range(len(tweet)):\n",
    "        if tweet[word].startswith('#'):\n",
    "            tweet[word] = tweet[word].replace('#','')\n",
    "            tweet[word] = seg_eng.segment(tweet[word])\n",
    "\n",
    "        # potentially correct spelling - but it is not working very well - corrects numbers to weird words\n",
    "        #tweet[word] = spell_cor.correct(tweet[word])\n",
    "\n",
    "    # instead of .join we use detokenizer in order to reconstruct the cleaned sentence in a better way\n",
    "    #sample_df[twt] =  \" \".join(tweet) \n",
    "    tweet = detokenizer.detokenize(tweet)\n",
    "    \n",
    "    \n",
    "    #  tweets that end up being empty after preprocessing will cause problems when batching, replace empty tweet with 'no_tweet_text' which we can ignore later\n",
    "    tweet = 'no_tweet_text' if len(tweet)==0 else tweet\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b32e522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175900/175900 [01:18<00:00, 2237.35it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas() # allowing progress bar on .apply method (== .progress_apply)\n",
    "event_df['parsing_corpus'] = event_df['text'].progress_apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b3e38",
   "metadata": {},
   "source": [
    "### Replace @username by screen name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddcd4304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175900/175900 [00:01<00:00, 114609.69it/s]\n"
     ]
    }
   ],
   "source": [
    "def resolve_username_to_name(text):\n",
    "    new_text = text\n",
    "    for word in text.split(\" \"):\n",
    "        if word in mapping:\n",
    "            new_text = new_text.replace(word,mapping[word])\n",
    "    return new_text\n",
    "\n",
    "#tqdm.pandas()\n",
    "event_df['parsing_corpus'] = event_df['parsing_corpus'].progress_apply(resolve_username_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e65bb",
   "metadata": {},
   "source": [
    "### Remove Duplicate Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "585735ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I just signed a @theactionnet petition: Urgently resettle child refugees from Greek islands . . Sign here:                                                                                                                                                                                    879\n",
       "1 - Turkey has started a rightful operation against Syrian Regime to protect its borders after the vicious attacks by Syrian Regime against civilians in Turkey controlled zones . 2 - Turkey has opened its doors to Europe for 72 hours . All immigrants are free to pass!                  721\n",
       "Hi EUHomeAffairs @Place_Beauvau Bundesministerium des Innern, f√ºr Bau und Heimat @foreignoffice @Justitiedep @ministerieJenV thousands of refugees are at risk of covid 19 on Greek islands due to crowded unsanitary conditions . Will you act now to leave no one behind and save lives?    479\n",
       "Please . . We are Iraqi refugees in Turkey from 2014 to 2015 and so far we have not got a homeland . . Put yourselves in our place . . Our situation is bad and our children their future is unknown . . We want a homeland . . Please help us . stand with iraqi refugees in Turkey.         257\n",
       ". Secretary Pompeo the State Department must speak clearly and unequivocally to Turkey asymmetric warfare, weaponization of migrants, and hostage politics must be condemned by the United States!                                                                                            230\n",
       "Name: parsing_corpus, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can see that the dataset contains a lot of duplicate tweets\n",
    "event_df[\"parsing_corpus\"].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bb3409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsing_corpus</th>\n",
       "      <th>retweet_count_sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! \" Trump plans to use COVID - 19 pandemic as ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>! Attention! To All Syrian Refugees: If you op...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! The public &amp; the rest of the world isnt supp...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>! To the attention of the Assad regime support...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!! Turkey claims one migrant was killed by Gre...</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      parsing_corpus  retweet_count_sum  count\n",
       "0  ! \" Trump plans to use COVID - 19 pandemic as ...                  2      1\n",
       "1  ! Attention! To All Syrian Refugees: If you op...                  0      1\n",
       "2  ! The public & the rest of the world isnt supp...                  0      1\n",
       "3  ! To the attention of the Assad regime support...                  1      1\n",
       "4  !! Turkey claims one migrant was killed by Gre...                 59      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Grouping by tweet text, keep count of retweets\n",
    "event_df_grouped = event_df[[\"parsing_corpus\",\"retweet_count\"]].groupby(\"parsing_corpus\").agg({\"retweet_count\":[\"sum\",\"count\"]}).reset_index()\n",
    "event_df_grouped.columns = list(map(''.join, event_df_grouped.columns.values))\n",
    "event_df_grouped = event_df_grouped.rename(columns={\"retweet_countsum\":\"retweet_count_sum\",\"retweet_countcount\":\"count\"})\n",
    "event_df_grouped[\"retweet_count_sum\"] = event_df_grouped[\"retweet_count_sum\"] + event_df_grouped[\"count\"] - 1 #take into account that only the retweets of a similar tweet but also the tweet iself is supposed to be treated as a retweet\n",
    "event_df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c529063",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicate tweets\n",
    "event_df_sorted = event_df.sort_values(\"created_at\") #df should be sorted by default but this step ensures that sorting is there\n",
    "event_df_no_dups = event_df_sorted.drop_duplicates(\"parsing_corpus\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6c746f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge grouped data together\n",
    "event_df_no_dups1 = pd.merge(left = event_df_no_dups,\n",
    "                             right = event_df_grouped,\n",
    "                             left_on = \"parsing_corpus\",\n",
    "                             right_on = \"parsing_corpus\",\n",
    "                             how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d2d9889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if numbers add up\n",
    "event_df_no_dups1[\"count\"].sum() == event_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043c317",
   "metadata": {},
   "source": [
    "## 3. Frame identification corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49403432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikodemicek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikodemicek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikodemicek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def tokenization(df_col):\n",
    "    \"\"\"\n",
    "    Takes a list with strings and returns a list with tokens\n",
    "    \"\"\"\n",
    "    print(\"Tokenizing tweets...\\n\")\n",
    "    return df_col.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "\n",
    "def lowercase(df_col):\n",
    "    \"\"\"\n",
    "    #Returns all tokens in lowercase.\n",
    "    \"\"\"\n",
    "    print(\"Making all words lowercase...\\n\")\n",
    "    return df_col.apply(lambda x: [token.lower() for token in x])\n",
    "\n",
    "\n",
    "def only_alphabetic(df_col):\n",
    "    \"\"\"\n",
    "    Keeps only tokens which are alphabetic or an underscore and returns them.\n",
    "    \"\"\"\n",
    "    print(\"Removing all non-alphabetic words...\\n\")\n",
    "    return df_col.apply(lambda x: [token for token in x if re.match(\"^[a-zA-Z0_]*$\", token)])\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\"\"\"stop_words.update([\"refugee\",\"refugees\",\"migrant\",\"migrants\",\"immigrant\",\"immigrants\",\n",
    "                   \"like\", \"would\",\"want\",\"take\",\"must\",\"well\",\"could\",\"even\",\"since\",\n",
    "                   \"also\",\"know\"])\"\"\"\n",
    "\n",
    "def stopword_removal(df_col):\n",
    "    \"\"\"\n",
    "    Removes all words considered as stopwords and all words that have a length of three or less.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Removing Stopwords...\\n\")\n",
    "\n",
    "    return df_col.apply(lambda x: [token for token in x if token not in stop_words and len(token) > 3])\n",
    "\n",
    "\n",
    "def lemmatization(df_col):\n",
    "    \"\"\"\n",
    "    Applies lemmatization to all tokens and returns them afterwards.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Lemmatizing words...\\n\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return df_col.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "\n",
    "def preprocessing(df_col, *steps):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe column with text and applies preprocessing steps given \n",
    "    in and returns a string.\n",
    "    \n",
    "    Input:\n",
    "    - df (dataframe): The dataframe containing the text column.\n",
    "    - steps (functions): Multiple functions for preprocessing can be given in.\n",
    "    \n",
    "    Output:\n",
    "    - List with strings.\n",
    "    \"\"\"\n",
    "    # copying over the column for preprocessing\n",
    "    temp = df_col.copy()\n",
    "    for func in steps:\n",
    "        temp = func(temp)\n",
    "    return temp.apply(lambda x: \" \".join([token for token in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29be1f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Making all words lowercase...\n",
      "\n",
      "Removing all non-alphabetic words...\n",
      "\n",
      "Removing Stopwords...\n",
      "\n",
      "Lemmatizing words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_df[\"frame_identification_corpus\"] = preprocessing(event_df[\"parsing_corpus\"],\n",
    "                                            tokenization,\n",
    "                                            lowercase,\n",
    "                                            only_alphabetic,\n",
    "                                            stopword_removal,\n",
    "                                            lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "773cf12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year_calendar_week</th>\n",
       "      <th>refugee</th>\n",
       "      <th>migrant</th>\n",
       "      <th>immigrant</th>\n",
       "      <th>asylum_seeker</th>\n",
       "      <th>other</th>\n",
       "      <th>date</th>\n",
       "      <th>parsing_corpus</th>\n",
       "      <th>frame_identification_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u.fooo.ooo</td>\n",
       "      <td>[üî¥ NEWS] Greece plans floating sea border wall...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227019556167864321</td>\n",
       "      <td>2020-02-11 00:00:33+00:00</td>\n",
       "      <td>1052191553802854407</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_2</td>\n",
       "      <td>2020_06</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>[NEWS] Greece plans floating sea border wall t...</td>\n",
       "      <td>news greece plan floating border wall keep ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>It is not your own feet sinking in the mud, bo...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227022233484308481</td>\n",
       "      <td>2020-02-11 00:11:12+00:00</td>\n",
       "      <td>2729959018</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_2</td>\n",
       "      <td>2020_06</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>It is not your own feet sinking in the mud, boy.</td>\n",
       "      <td>foot sinking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IFTTT</td>\n",
       "      <td>‚ö†Ô∏è ALL Meteoalarm Severe #Weather Warnings for...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227027068132745217</td>\n",
       "      <td>2020-02-11 00:30:24+00:00</td>\n",
       "      <td>342772619</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_2</td>\n",
       "      <td>2020_06</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>ALL Meteoalarm Severe weather Warnings for eur...</td>\n",
       "      <td>meteoalarm severe weather warning europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kurdish News</td>\n",
       "      <td>Snowstorm kills 13 migrants crossing from #Ira...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227027865771876354</td>\n",
       "      <td>2020-02-11 00:33:34+00:00</td>\n",
       "      <td>248248441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_2</td>\n",
       "      <td>2020_06</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>Snowstorm kills 13 migrants crossing from iran...</td>\n",
       "      <td>snowstorm kill migrant crossing iran turkey in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>.#Erdogan ‚Äúwill need to choose 1 of 2 solution...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227027924311855105</td>\n",
       "      <td>2020-02-11 00:33:48+00:00</td>\n",
       "      <td>254373781</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020_2</td>\n",
       "      <td>2020_06</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>. erdogan will need to choose 1 of 2 solutions...</td>\n",
       "      <td>erdogan need choose solution syrian cross turk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                               text lang  \\\n",
       "0          u.fooo.ooo  [üî¥ NEWS] Greece plans floating sea border wall...   en   \n",
       "1  Twitter for iPhone  It is not your own feet sinking in the mud, bo...   en   \n",
       "2               IFTTT  ‚ö†Ô∏è ALL Meteoalarm Severe #Weather Warnings for...   en   \n",
       "3        Kurdish News  Snowstorm kills 13 migrants crossing from #Ira...   en   \n",
       "4  Twitter for iPhone  .#Erdogan ‚Äúwill need to choose 1 of 2 solution...   en   \n",
       "\n",
       "                    id                 created_at            author_id  \\\n",
       "0  1227019556167864321  2020-02-11 00:00:33+00:00  1052191553802854407   \n",
       "1  1227022233484308481  2020-02-11 00:11:12+00:00           2729959018   \n",
       "2  1227027068132745217  2020-02-11 00:30:24+00:00            342772619   \n",
       "3  1227027865771876354  2020-02-11 00:33:34+00:00            248248441   \n",
       "4  1227027924311855105  2020-02-11 00:33:48+00:00            254373781   \n",
       "\n",
       "   retweet_count  reply_count  like_count  quote_count  ...  year_month  \\\n",
       "0              0            0           0            0  ...      2020_2   \n",
       "1              9            1          29            1  ...      2020_2   \n",
       "2              0            0           0            0  ...      2020_2   \n",
       "3              0            0           0            0  ...      2020_2   \n",
       "4              6            0           6            0  ...      2020_2   \n",
       "\n",
       "  year_calendar_week refugee migrant immigrant  asylum_seeker  other  \\\n",
       "0            2020_06    True   False     False          False  False   \n",
       "1            2020_06   False   False     False          False  False   \n",
       "2            2020_06    True   False     False          False  False   \n",
       "3            2020_06   False    True     False          False  False   \n",
       "4            2020_06   False   False     False          False  False   \n",
       "\n",
       "         date                                     parsing_corpus  \\\n",
       "0  2020-02-11  [NEWS] Greece plans floating sea border wall t...   \n",
       "1  2020-02-11   It is not your own feet sinking in the mud, boy.   \n",
       "2  2020-02-11  ALL Meteoalarm Severe weather Warnings for eur...   \n",
       "3  2020-02-11  Snowstorm kills 13 migrants crossing from iran...   \n",
       "4  2020-02-11  . erdogan will need to choose 1 of 2 solutions...   \n",
       "\n",
       "                         frame_identification_corpus  \n",
       "0  news greece plan floating border wall keep ref...  \n",
       "1                                       foot sinking  \n",
       "2           meteoalarm severe weather warning europe  \n",
       "3  snowstorm kill migrant crossing iran turkey in...  \n",
       "4  erdogan need choose solution syrian cross turk...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a655bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@guardiannews Turkey has opened its doors to Europe for 72 hours. All immigrants are free to pass!\n",
      "---\n",
      "Turkey has opened its doors to Europe for 72 hours . All immigrants are free to pass!\n",
      "---\n",
      "turkey opened door europe hour immigrant free pas\n",
      "-----------------------------------\n",
      "Europeans unite.... Greek border  under attack for weeks... thank you to all countries helping but we need more help!!! Not peaceful refugees but Islamists! https://t.co/LkavoQLFTr\n",
      "---\n",
      "Europeans unite . . . . Greek border under attack for weeks . . . thank you to all countries helping but we need more help!!! Not peaceful refugees but Islamists!\n",
      "---\n",
      "european unite greek border attack week thank country helping need help peaceful refugee islamist\n",
      "-----------------------------------\n",
      "@SteliosPetsas You just have to be a human, once Greece refugees of WWII got warm welcomed in Syria and today Greece welcome Syrian refugees with tear gass and bullets. Oh humanity\n",
      "---\n",
      "You just have to be a human, once Greece refugees of WWII got warm welcomed in Syria and today Greece welcome Syrian refugees with tear gass and bullets . Oh humanity\n",
      "---\n",
      "human greece refugee wwii warm welcomed syria today greece welcome syrian refugee tear gas bullet humanity\n",
      "-----------------------------------\n",
      "@Handeyy_ Send them back then. EU doesn't need these economy refugees.\n",
      "---\n",
      "Send them back then . EU doesn' t need these economy refugees.\n",
      "---\n",
      "send back need economy refugee\n",
      "-----------------------------------\n",
      "#EnoughisEnoughAll european citizens  turkey opened the door bcz all migrants are not secure now . We belive urs √ßare for  their secure in #EuropeanUnion https://t.co/R34Upx5mie\n",
      "---\n",
      "enoughis enough all european citizens turkey opened the door bcz all migrants are not secure now . We belive urs are for their secure in european union\n",
      "---\n",
      "enoughis enough european citizen turkey opened door migrant secure belive secure european union\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,line in event_df[[\"text\",\"parsing_corpus\",\"frame_identification_corpus\"]].sample(5).iterrows():\n",
    "    print(line[\"text\"])\n",
    "    print(\"---\")\n",
    "    print(line[\"parsing_corpus\"])\n",
    "    print(\"---\")\n",
    "    print(line[\"frame_identification_corpus\"])\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe912c6",
   "metadata": {},
   "source": [
    "## Save the dataframe with clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "582643cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df.to_csv(greece_url_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e75722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
