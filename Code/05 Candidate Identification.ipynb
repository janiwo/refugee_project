{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate merging and related preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessary files:**\n",
    " - event_df = df\\_[event]\\_clean.csv file with event dataframes with unique tweets only\n",
    " \n",
    " _the goal of this notebook is to tag all tweets from event_df and extract all noun phrases. Noun phrases will serve as candidates and using pipeline function they are categorised and finally only unique (and cleaned) candidates will be saved into event_cands dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "#python libraries\n",
    "import stanza\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# self written modules\n",
    "import preprocessing\n",
    "import candidate_processing as cand_prep\n",
    "import candidate_extraction as cand_ex\n",
    "\n",
    "import pickle\n",
    "\n",
    "def pickle_file(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. We import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 22966 tweets!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_coherent</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>retweet_count_sum</th>\n",
       "      <th>text_alphanum</th>\n",
       "      <th>text_stm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For rohingya Survivors in Bangladesh, Artwork ...</td>\n",
       "      <td>1373792416126402560</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>2</td>\n",
       "      <td>for rohingya survivors in bangladesh artwork b...</td>\n",
       "      <td>rohingya survivor bangladesh artwork bear witn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AstraZeneca dispels Indonesian Muslim concerns...</td>\n",
       "      <td>1373800977778700288</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>1</td>\n",
       "      <td>astrazeneca dispels indonesian muslim concerns...</td>\n",
       "      <td>astrazeneca dispels indonesian muslim concern ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think u are one of the illegally migrant Roh...</td>\n",
       "      <td>1373802051524730880</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>0</td>\n",
       "      <td>i think u are one of the illegally migrant roh...</td>\n",
       "      <td>think illegally migrant rohingya bangladesh be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India seals Myanmar border amid strains over r...</td>\n",
       "      <td>1373802536579174401</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>0</td>\n",
       "      <td>india seals myanmar border amid strains over r...</td>\n",
       "      <td>india seal myanmar border amid strain refugee ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fleeing coup, Myanmar police refugees in India...</td>\n",
       "      <td>1373804367757807619</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>1</td>\n",
       "      <td>fleeing coup myanmar police refugees in india ...</td>\n",
       "      <td>fleeing coup myanmar police refugee india seek...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_coherent                   id  \\\n",
       "0  For rohingya Survivors in Bangladesh, Artwork ...  1373792416126402560   \n",
       "1  AstraZeneca dispels Indonesian Muslim concerns...  1373800977778700288   \n",
       "2  I think u are one of the illegally migrant Roh...  1373802051524730880   \n",
       "3  India seals Myanmar border amid strains over r...  1373802536579174401   \n",
       "4  Fleeing coup, Myanmar police refugees in India...  1373804367757807619   \n",
       "\n",
       "         date  retweet_count_sum  \\\n",
       "0  2021-03-22                  2   \n",
       "1  2021-03-22                  1   \n",
       "2  2021-03-22                  0   \n",
       "3  2021-03-22                  0   \n",
       "4  2021-03-22                  1   \n",
       "\n",
       "                                       text_alphanum  \\\n",
       "0  for rohingya survivors in bangladesh artwork b...   \n",
       "1  astrazeneca dispels indonesian muslim concerns...   \n",
       "2  i think u are one of the illegally migrant roh...   \n",
       "3  india seals myanmar border amid strains over r...   \n",
       "4  fleeing coup myanmar police refugees in india ...   \n",
       "\n",
       "                                            text_stm  \n",
       "0  rohingya survivor bangladesh artwork bear witn...  \n",
       "1  astrazeneca dispels indonesian muslim concern ...  \n",
       "2  think illegally migrant rohingya bangladesh be...  \n",
       "3  india seal myanmar border amid strain refugee ...  \n",
       "4  fleeing coup myanmar police refugee india seek...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_greece_clean.csv\" # for Greece\n",
    "tigray_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_tigray_clean.csv\" # for Tigray\n",
    "rohingya_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_rohingya_clean.csv\" # for Rohingya\n",
    "\n",
    "def read_event_df(data_url):\n",
    "    directory_path = os.getcwd() + \"/../../../../\" + data_url \n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df(rohingya_url)\n",
    "event_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First,  extracting noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code runs for around another 13h per 100k tweets\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "#use \"with\" so the client stops properly after finished\n",
    "with CoreNLPClient(annotators=[\"tokenize,ssplit,pos,parse\"], timeout=6000000, memory='8G') as client:\n",
    "        print('extracting noun phrases...')\n",
    "        tqdm.pandas()\n",
    "        # get noun phrases with tregex using get_noun_phrases function\n",
    "        event_df['noun_phrases'] = event_df['text_coherent'].progress_apply(cand_ex.get_noun_phrases,args=(client,\"tokenize,ssplit,pos,parse\"))\n",
    "\n",
    "np_list = list(event_df['noun_phrases'])\n",
    "len(np_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('rohingya_noun_phrases',np_list)\n",
    "\n",
    "#np_list = load_pickle(\"moria_short_noun_phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. instantiate stanza english language module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-01 10:31:57 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-06-01 10:31:57 INFO: Use device: cpu\n",
      "2021-06-01 10:31:57 INFO: Loading: tokenize\n",
      "2021-06-01 10:31:57 INFO: Loading: pos\n",
      "2021-06-01 10:31:58 INFO: Loading: lemma\n",
      "2021-06-01 10:31:58 INFO: Loading: depparse\n",
      "2021-06-01 10:31:59 INFO: Loading: sentiment\n",
      "2021-06-01 10:31:59 INFO: Loading: ner\n",
      "2021-06-01 10:32:01 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ needed when running first time ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "\n",
    "#stanza.download(\"en\")\n",
    "\n",
    "#stanza.install_corenlp()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# loading the pipeline\n",
    "en_nlp = stanza.Pipeline(\"en\", tokenize_pretokenized=True, ner_batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag tweets using stanza module to get NER and POS tags in tweets. We do it in batches to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 19912/19912 [5:40:59<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "event_tagged_tweets = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(list(event_df['text_coherent']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle_file('tigray_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 22966/22966 [4:58:17<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "event_tagged_tweets = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(list(event_df['text_coherent']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('rohingya_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for candidate identification\n",
    "\n",
    "**Necessary files:**\n",
    " - event_np_list = pickled file of list of noun phrases\n",
    " - event_tagged_tweets = pickled file with NER and POS tags for all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event_data(event_name):\n",
    "    assert event_name in ['greece','tigray','rohingya','moria'], f\"Oh no! We do not analyze {event_name} event\"\n",
    "    \n",
    "    print(f'Loading {event_name} data...')\n",
    "    try:\n",
    "        #sample = 2000\n",
    "        event_np_list = load_pickle(event_name + '_noun_phrases')#[1000:sample]\n",
    "        event_tagged_tweets = load_pickle(event_name + '_tagged_tweets')#[1000:sample]\n",
    "        return event_np_list,event_tagged_tweets\n",
    "    except:\n",
    "        print(f'The {event_name} files not found! First extract noun phrases and tag tweets of the {eventname}_df')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rohingya data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                           | 203/22966 [00:00<00:11, 2011.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did I get here?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 22966/22966 [00:07<00:00, 2990.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rohingya noun phrase candidates...\n",
      "removing long candidates...\n",
      "Removed 0 candidates longer than 9 words!\n",
      "removing child NP candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/22966 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 40002 child NP candidates!\n",
      "Tagging rohingya noun phrase candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 22966/22966 [3:13:14<00:00,  1.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 22966/22966 [04:57<00:00, 77.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183488\n"
     ]
    }
   ],
   "source": [
    "def pipeline(event_name):\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 1. LOAD THE DATA ~~~~~~~~~~~~~~~~~~~~~\n",
    "    event_np_list,event_tagged_tweets = load_event_data(event_name)\n",
    "    print('did I get here?')\n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 2. GET POS AND NER TAGS ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # get list of tuples (POS-tags of each word, NER-tags of each named entity) \n",
    "    tweet_tags = cand_prep.get_tweet_tags(event_tagged_tweets) \n",
    "    \n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 3. PREPROCESS CANDIDATES ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ~~~~~~~~~~~~ processing of noun phrases ~~~~~~~~~~~~~~~~~~~~~\n",
    "    print(f'Processing {event_name} noun phrase candidates...')\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    # remove NP candidates longer than threshold and remove all child NPs of parent NPs\n",
    "    event_np_list = cand_prep.remove_long_nps(event_np_list)\n",
    "    event_np_list = cand_prep.remove_child_nps(event_np_list) \n",
    "    event_np_list = cand_prep.remove_char(event_np_list,'@')\n",
    "\n",
    "    event_np_list = [['no_candidate'] if len(noun_ps)==0 or noun_ps ==' ' else noun_ps for noun_ps in event_np_list ]\n",
    "    \n",
    "    #print(event_np_list)\n",
    "    print(f'Tagging {event_name} noun phrase candidates...')\n",
    "    #tag all tweets and save them in a list    \n",
    "\n",
    "    #tagged_np_cands = batched_np_list.progress_apply(en_nlp)\n",
    "    tagged_np_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_np_list)]\n",
    "    #tagged_np_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_np_list, en_nlp, batch_size=6000))]\n",
    "\n",
    "    np_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tagged_np_cands]\n",
    "    #print(np_cand_heads)\n",
    "    \n",
    "    np_and_cand_list = cand_prep.get_cand_type(event_np_list,np_cand_heads, tweet_tags)\n",
    "    #print(event_np_list)\n",
    "          \n",
    "          \n",
    "    # ~~~~~~~~~~~~~~~~~~~~ combining candidate lists ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #concatenate corefs and noun phrase lists\n",
    "    nps_cands = [cand for cands in np_and_cand_list for cand in cands]\n",
    "    #candidate_list = coref_and_cand_list + np_and_cand_list\n",
    "\n",
    "    #unpack list of lists into one list\n",
    "    candidate_list = nps_cands\n",
    "          \n",
    "    nps_tagged = [sent for tagged_cand in tagged_np_cands for sent in tagged_cand.sentences ]\n",
    "\n",
    "    all_cands_tagged = nps_tagged\n",
    "\n",
    "        \n",
    "    #print(len(candidate_list),'vs', len(all_cands_tagged))\n",
    "    cand_df = pd.DataFrame(\n",
    "        {'candidates': candidate_list,\n",
    "         'cand_tags': all_cands_tagged\n",
    "        })\n",
    "\n",
    "    cand_df['cand_text'] = cand_df.candidates.apply(lambda x: x[0])\n",
    "    cand_df['cand_len'] = cand_df.cand_text.apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "    count_cands = Counter(cand_df['cand_text'])\n",
    "    cand_df['cand_freq'] = cand_df[\"cand_text\"].map(count_cands)\n",
    "    \n",
    "    #count_cands[cand_df['cand_text']]\n",
    "    #count_sorted = sorted(count_cands.items(),key=lambda x: x[1],reverse=True)\n",
    "    cand_df.columns = cand_df.columns.str.strip()\n",
    "    \n",
    "          \n",
    "    # we sort the candidates by their length\n",
    "    cand_df.sort_values('cand_freq', ascending=False,inplace=True)\n",
    "\n",
    "    #cand_df = cand_df[cand_df.cand_text not in  ['no_candidate', 'candidate_to_be_removed']]\n",
    "\n",
    "    cand_df.reset_index(drop=True, inplace = True)\n",
    "    #remove dummy candidates that were used to avoid errors\n",
    "\n",
    "    \n",
    "    cand_df = cand_df[cand_df.cand_text != 'candidate_to_be_removed']\n",
    "    cand_df = cand_df[cand_df.cand_text != 'no_candidate']\n",
    "    print(len(cand_df))    \n",
    "    cand_df.reset_index(drop=True,inplace=True)\n",
    "          \n",
    "    return cand_df\n",
    "          \n",
    "\n",
    "event_cands = pipeline('rohingya')\n",
    "\n",
    "#pickle_file('moria_cands_df', moria_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidates</th>\n",
       "      <th>cand_tags</th>\n",
       "      <th>cand_text</th>\n",
       "      <th>cand_len</th>\n",
       "      <th>cand_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(refugees, refugees, {refugees}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...</td>\n",
       "      <td>refugees</td>\n",
       "      <td>1</td>\n",
       "      <td>2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(refugees, refugees, {refugees}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...</td>\n",
       "      <td>refugees</td>\n",
       "      <td>1</td>\n",
       "      <td>2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(refugees, refugees, {refugees}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...</td>\n",
       "      <td>refugees</td>\n",
       "      <td>1</td>\n",
       "      <td>2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(refugees, refugees, {refugees}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...</td>\n",
       "      <td>refugees</td>\n",
       "      <td>1</td>\n",
       "      <td>2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(refugees, refugees, {refugees}, misc)</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...</td>\n",
       "      <td>refugees</td>\n",
       "      <td>1</td>\n",
       "      <td>2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183483</th>\n",
       "      <td>(Agency freya_cole, freya_cole, {freya_cole}, ...</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"Agency\",\\n ...</td>\n",
       "      <td>Agency freya_cole</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183484</th>\n",
       "      <td>(Injuries and death, Injuries, {Injuries, deat...</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"Injuries\",\\...</td>\n",
       "      <td>Injuries and death</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183485</th>\n",
       "      <td>(areas of ethnic groups, areas, {areas, groups...</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"areas\",\\n  ...</td>\n",
       "      <td>areas of ethnic groups</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183486</th>\n",
       "      <td>(the junta' s airstrikes, airstrikes, {airstri...</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"the\",\\n    ...</td>\n",
       "      <td>the junta' s airstrikes</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183487</th>\n",
       "      <td>(shelter and hundreds missing, shelter, {hundr...</td>\n",
       "      <td>[\\n  {\\n    \"id\": 1,\\n    \"text\": \"shelter\",\\n...</td>\n",
       "      <td>shelter and hundreds missing</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183488 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               candidates  \\\n",
       "0                  (refugees, refugees, {refugees}, misc)   \n",
       "1                  (refugees, refugees, {refugees}, misc)   \n",
       "2                  (refugees, refugees, {refugees}, misc)   \n",
       "3                  (refugees, refugees, {refugees}, misc)   \n",
       "4                  (refugees, refugees, {refugees}, misc)   \n",
       "...                                                   ...   \n",
       "183483  (Agency freya_cole, freya_cole, {freya_cole}, ...   \n",
       "183484  (Injuries and death, Injuries, {Injuries, deat...   \n",
       "183485  (areas of ethnic groups, areas, {areas, groups...   \n",
       "183486  (the junta' s airstrikes, airstrikes, {airstri...   \n",
       "183487  (shelter and hundreds missing, shelter, {hundr...   \n",
       "\n",
       "                                                cand_tags  \\\n",
       "0       [\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...   \n",
       "1       [\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...   \n",
       "2       [\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...   \n",
       "3       [\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...   \n",
       "4       [\\n  {\\n    \"id\": 1,\\n    \"text\": \"refugees\",\\...   \n",
       "...                                                   ...   \n",
       "183483  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"Agency\",\\n ...   \n",
       "183484  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"Injuries\",\\...   \n",
       "183485  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"areas\",\\n  ...   \n",
       "183486  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"the\",\\n    ...   \n",
       "183487  [\\n  {\\n    \"id\": 1,\\n    \"text\": \"shelter\",\\n...   \n",
       "\n",
       "                           cand_text  cand_len  cand_freq  \n",
       "0                           refugees         1       2483  \n",
       "1                           refugees         1       2483  \n",
       "2                           refugees         1       2483  \n",
       "3                           refugees         1       2483  \n",
       "4                           refugees         1       2483  \n",
       "...                              ...       ...        ...  \n",
       "183483             Agency freya_cole         2          1  \n",
       "183484            Injuries and death         3          1  \n",
       "183485        areas of ethnic groups         4          1  \n",
       "183486       the junta' s airstrikes         4          1  \n",
       "183487  shelter and hundreds missing         4          1  \n",
       "\n",
       "[183488 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('tigray_cands', event_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates as identified by stanza library still have a lot of noise to be removed. Cleaner candidates merge better and throwing away duplicate candidates or candidates without useful information speeds up merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 183488/183488 [00:01<00:00, 91926.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 183488/183488 [00:00<00:00, 259163.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 164787/164787 [00:00<00:00, 549295.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 162945/162945 [00:00<00:00, 673231.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The event has  50447 unique candidates after cleaning\n"
     ]
    }
   ],
   "source": [
    "#Finally the candidates are cleaned before storing in a file prior to merging\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_cands(event_cands):\n",
    "    \"\"\"\n",
    "    Applying cleaning steps on candidates and engineering some features:\n",
    "     1. creating a column with length of the tweet (in chars)\n",
    "     2. lowercase the candidate information in the tuple with cand, candidate representative head and set of phrases heads\n",
    "     3. extract candidate text and keep only alphanumeric chars\n",
    "     4. remove candidates that are stopwords\n",
    "     5. remove candidates that are only numeric\n",
    "     6. remove candidates that are only 1 char long\n",
    "     \"\"\"\n",
    "    def clean_cand(cand):\n",
    "        cand = list(cand)\n",
    "        cand[0] = re.sub(r'[^A-Za-z0-9 ]+', '', cand[0].lower())\n",
    "        cand[1] = re.sub(r'[^A-Za-z0-9 ]+', '', cand[1].lower())\n",
    "        cand[2] = set([re.sub(r'[^A-Za-z0-9 ]+', '', phrase_word.lower()) for phrase_word in cand[2]])\n",
    "\n",
    "        return tuple(cand)\n",
    "\n",
    "    #stopwords\n",
    "    tqdm.pandas()\n",
    "    event_cands_clean = event_cands.copy()\n",
    "    \n",
    "    \n",
    "    event_cands_clean['candidates'] = event_cands_clean['candidates'].progress_apply(clean_cand)\n",
    "    \n",
    "    event_cands_clean['cand_text'] = event_cands_clean['cand_text'].progress_apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', '', x.lower()).strip())\n",
    "    event_cands_clean = event_cands_clean[~event_cands_clean['cand_text'].isin(stopwords.words('english'))]\n",
    "    event_cands_clean['pure_chars'] = event_cands_clean['cand_text'].progress_apply(lambda x: x.replace(' ', ''))\n",
    "    event_cands_clean = event_cands_clean[~event_cands_clean['pure_chars'].str.isnumeric()]\n",
    "    event_cands_clean.drop('pure_chars',axis=1,inplace=True)\n",
    "    \n",
    "    event_cands_clean['string_len'] = event_cands_clean['cand_text'].progress_apply(len)\n",
    "    event_cands_clean = event_cands_clean[event_cands_clean['string_len']>1]\n",
    "    event_cands_clean = event_cands_clean.drop_duplicates(subset = [\"cand_text\"])\n",
    "    event_cands_clean.reset_index(drop=True, inplace=True)\n",
    "    print(f'The event has  {len(event_cands_clean)} unique candidates after cleaning')\n",
    "    return event_cands_clean\n",
    "\n",
    "event_cands_clean = clean_cands(event_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('rohingya_cands', event_cands_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cands = load_pickle('moria_short_cands')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
