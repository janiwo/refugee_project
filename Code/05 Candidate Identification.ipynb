{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate merging and related preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessary files:**\n",
    " - event_df = df\\_[event]\\_clean.csv file with event dataframes with unique tweets only\n",
    " \n",
    " _the goal of this notebook is to tag all tweets from event_df and extract all noun phrases. Noun phrases will serve as candidates and using pipeline function they are categorised and finally only unique (and cleaned) candidates will be saved into event_cands dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python libraries\n",
    "import stanza\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# self written modules\n",
    "import preprocessing\n",
    "import candidate_processing as cand_prep\n",
    "import candidate_extraction as cand_ex\n",
    "\n",
    "import pickle\n",
    "\n",
    "def pickle_file(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    #folder_name = re.sub(r'[12]', '', folder_name)\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 173758 tweets!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>refugee</th>\n",
       "      <th>migrant</th>\n",
       "      <th>immigrant</th>\n",
       "      <th>asylum_seeker</th>\n",
       "      <th>other</th>\n",
       "      <th>text_coherent</th>\n",
       "      <th>retweet_count_sum</th>\n",
       "      <th>count</th>\n",
       "      <th>text_alphanum</th>\n",
       "      <th>text_stm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WordPress.com</td>\n",
       "      <td>CHANNEL MIGRANT CRISIS – TODAYS VIDEOS FROM DO...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284639846930227200</td>\n",
       "      <td>2020-07-19 00:03:01+00:00</td>\n",
       "      <td>1039171425364520960</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>CHANNEL MIGRANT CRISIS TODAYS VIDEOS FROMDOVER.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>channel migrant crisis todays videos fromdover.</td>\n",
       "      <td>channel crisis today video fromdover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>“Chinese immorality [and] eccentricities … are...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284640070855729163</td>\n",
       "      <td>2020-07-19 00:03:55+00:00</td>\n",
       "      <td>153438157</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Chinese immorality [and] eccentricities are ab...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>chinese immorality and eccentricities are abho...</td>\n",
       "      <td>chinese immorality eccentricity abhorrent arya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@chrisgregson123 @VeuveK @CharlieHicks90 @Rudy...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284640230499328000</td>\n",
       "      <td>2020-07-19 00:04:33+00:00</td>\n",
       "      <td>503070765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>O / c Leavers voted for what they believed was...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>o  c leavers voted for what they believed was ...</td>\n",
       "      <td>leaver voted believed best england wale howeve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@SkyNews It never will if uk keeps bring in hu...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284640911788576770</td>\n",
       "      <td>2020-07-19 00:07:15+00:00</td>\n",
       "      <td>1276420769384402944</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>It never will if uk keeps bring in hundreds of...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>it never will if uk keeps bring in hundreds of...</td>\n",
       "      <td>never keep bring hundred asylum seeker giving ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>How many illegal immigrants this week in #Dove...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284641481576402945</td>\n",
       "      <td>2020-07-19 00:09:31+00:00</td>\n",
       "      <td>755084846783950848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>How many illegal immigrants this week in dover...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>how many illegal immigrants this week in dover...</td>\n",
       "      <td>many illegal week dover</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                               text lang  \\\n",
       "0       WordPress.com  CHANNEL MIGRANT CRISIS – TODAYS VIDEOS FROM DO...   en   \n",
       "1     Twitter Web App  “Chinese immorality [and] eccentricities … are...   en   \n",
       "2  Twitter for iPhone  @chrisgregson123 @VeuveK @CharlieHicks90 @Rudy...   en   \n",
       "3  Twitter for iPhone  @SkyNews It never will if uk keeps bring in hu...   en   \n",
       "4  Twitter for iPhone  How many illegal immigrants this week in #Dove...   en   \n",
       "\n",
       "                    id                 created_at            author_id  \\\n",
       "0  1284639846930227200  2020-07-19 00:03:01+00:00  1039171425364520960   \n",
       "1  1284640070855729163  2020-07-19 00:03:55+00:00            153438157   \n",
       "2  1284640230499328000  2020-07-19 00:04:33+00:00            503070765   \n",
       "3  1284640911788576770  2020-07-19 00:07:15+00:00  1276420769384402944   \n",
       "4  1284641481576402945  2020-07-19 00:09:31+00:00   755084846783950848   \n",
       "\n",
       "   retweet_count  reply_count  like_count  quote_count  ...  refugee migrant  \\\n",
       "0              0            0           0            0  ...    False    True   \n",
       "1             22            1          37            0  ...    False   False   \n",
       "2              0            0           0            1  ...    False   False   \n",
       "3              1            1           3            1  ...    False    True   \n",
       "4              0            0           0            0  ...    False   False   \n",
       "\n",
       "  immigrant asylum_seeker  other  \\\n",
       "0     False         False  False   \n",
       "1      True         False  False   \n",
       "2     False         False  False   \n",
       "3     False          True  False   \n",
       "4      True         False  False   \n",
       "\n",
       "                                       text_coherent  retweet_count_sum count  \\\n",
       "0    CHANNEL MIGRANT CRISIS TODAYS VIDEOS FROMDOVER.                  0     1   \n",
       "1  Chinese immorality [and] eccentricities are ab...                 22     1   \n",
       "2  O / c Leavers voted for what they believed was...                  0     1   \n",
       "3  It never will if uk keeps bring in hundreds of...                  1     1   \n",
       "4  How many illegal immigrants this week in dover...                  0     1   \n",
       "\n",
       "                                       text_alphanum  \\\n",
       "0    channel migrant crisis todays videos fromdover.   \n",
       "1  chinese immorality and eccentricities are abho...   \n",
       "2  o  c leavers voted for what they believed was ...   \n",
       "3  it never will if uk keeps bring in hundreds of...   \n",
       "4  how many illegal immigrants this week in dover...   \n",
       "\n",
       "                                            text_stm  \n",
       "0               channel crisis today video fromdover  \n",
       "1  chinese immorality eccentricity abhorrent arya...  \n",
       "2  leaver voted believed best england wale howeve...  \n",
       "3  never keep bring hundred asylum seeker giving ...  \n",
       "4                            many illegal week dover  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_greece_clean.csv\" # for Greece\n",
    "tigray_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_tigray_clean.csv\" # for Tigray\n",
    "rohingya_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_rohingya_clean.csv\" # for Rohingya\n",
    "channel_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_channel_clean.csv\" # for channel\n",
    "\n",
    "def read_event_df(data_url):\n",
    "    directory_path = os.getcwd() + \"/../../../../\" + data_url \n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df(channel_url)\n",
    "event_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First,  extracting noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-23 07:54:07 INFO: Writing properties to tmp file: corenlp_server-e0ed75ae247c4879.props\n",
      "2021-06-23 07:54:07 INFO: Starting server with command: java -Xmx8G -cp C:\\Users\\nikodemicek\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 6000000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-e0ed75ae247c4879.props -annotators tokenize,ssplit,pos,parse -preload -outputFormat serialized\n",
      "  0%|                                                                                        | 0/29432 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 29432 tweets!\n",
      "extracting noun phrases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 29432/29432 [2:39:32<00:00,  3.07it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d0d653c61fda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np_list' is not defined"
     ]
    }
   ],
   "source": [
    "# this code runs for around another 13h per 100k tweets\n",
    "event_df = read_event_df(rohingya_url)\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "#use \"with\" so the client stops properly after finished\n",
    "with CoreNLPClient(annotators=[\"tokenize,ssplit,pos,parse\"], timeout=6000000, memory='8G') as client:\n",
    "        print('extracting noun phrases...')\n",
    "        tqdm.pandas()\n",
    "        # get noun phrases with tregex using get_noun_phrases function\n",
    "        event_df['noun_phrases'] = event_df['text_coherent'].progress_apply(cand_ex.get_noun_phrases,args=(client,\"tokenize,ssplit,pos,parse\"))\n",
    "\n",
    "\n",
    "#len(np_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing child NP candidates...\n",
      "Removed 50798 child NP candidates!\n"
     ]
    }
   ],
   "source": [
    "np_list = list(event_df['noun_phrases'])\n",
    "np_list = cand_prep.remove_char(np_list,'@') # remove the @ sign from all mentions\n",
    "np_list = cand_prep.remove_child_nps(np_list) # remove the sub NPs if they were found in longer NPs in the same tree\n",
    "pickle_file('rohingya_noun_phrases',np_list)\n",
    "\n",
    "#np_list = load_pickle(\"moria_short_noun_phrases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-23 18:07:39 INFO: Writing properties to tmp file: corenlp_server-ea054eb6faf04dcb.props\n",
      "2021-06-23 18:07:39 INFO: Starting server with command: java -Xmx8G -cp C:\\Users\\nikodemicek\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 6000000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-ea054eb6faf04dcb.props -annotators tokenize,ssplit,pos,parse -preload -outputFormat serialized\n",
      "  0%|                                                                                        | 0/42853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 42853 tweets!\n",
      "extracting noun phrases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 42853/42853 [6:53:56<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# this code runs for around another 13h per 100k tweets\n",
    "event_df = read_event_df(tigray_url)\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "#use \"with\" so the client stops properly after finished\n",
    "with CoreNLPClient(annotators=[\"tokenize,ssplit,pos,parse\"], timeout=6000000, memory='8G') as client:\n",
    "        print('extracting noun phrases...')\n",
    "        tqdm.pandas()\n",
    "        # get noun phrases with tregex using get_noun_phrases function\n",
    "        event_df['noun_phrases'] = event_df['text_coherent'].progress_apply(cand_ex.get_noun_phrases,args=(client,\"tokenize,ssplit,pos,parse\"))\n",
    "\n",
    "\n",
    "#len(np_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing child NP candidates...\n",
      "Removed 78471 child NP candidates!\n"
     ]
    }
   ],
   "source": [
    "np_list = list(event_df['noun_phrases'])\n",
    "np_list = cand_prep.remove_char(np_list,'@') # remove the @ sign from all mentions\n",
    "np_list = cand_prep.remove_child_nps(np_list) # remove the sub NPs if they were found in longer NPs in the same tree\n",
    "pickle_file('tigray_noun_phrases',np_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag tweets using stanza module to get NER and POS tags in tweets. We do it in batches to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-20 22:20:33 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-06-20 22:20:33 INFO: Use device: cpu\n",
      "2021-06-20 22:20:33 INFO: Loading: tokenize\n",
      "2021-06-20 22:20:33 INFO: Loading: pos\n",
      "2021-06-20 22:20:33 INFO: Loading: lemma\n",
      "2021-06-20 22:20:33 INFO: Loading: depparse\n",
      "2021-06-20 22:20:34 INFO: Loading: ner\n",
      "2021-06-20 22:20:36 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ needed when running first time ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "#stanza.download(\"en\")\n",
    "#stanza.install_corenlp()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# loading the pipeline\n",
    "en_nlp = stanza.Pipeline(\"en\",  \n",
    "                         tokenize_pretokenized=False,\n",
    "                         ner_batch_size=4096,\n",
    "                         processors = \"tokenize,pos,lemma,depparse,ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_df = read_event_df(tigray_url)\n",
    "event_tagged_tweets = [en_nlp(tweet) for tweet in tqdm(list(event_df['text_coherent']))]\n",
    "pickle_file('tigray_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 22966/22966 [4:58:17<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "event_df = read_event_df(rohingya_url)\n",
    "event_tagged_tweets = [en_nlp(tweet_batch) for tweet_batch in tqdm(list(event_df['text_coherent']))]\n",
    "pickle_file('rohingya_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = read_event_df(greece_url)\n",
    "event_tagged_tweets = [en_nlp(tweet_batch) for tweet_batch in tqdm(list(event_df['text_coherent']))]\n",
    "pickle_file('greece_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = read_event_df(channel_url)\n",
    "event_tagged_tweets = [en_nlp(tweet_batch) for tweet_batch in tqdm(event_tweet_list)]\n",
    "pickle_file('channel_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel1 = load_pickle('channel_tagged_tweets2')\n",
    "channel2 = load_pickle('channel_tagged_tweets1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173758"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_tags = channel2 + channel1\n",
    "len(channel_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file('channel_tagged_tweets',channel_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for candidate identification\n",
    "\n",
    "**Necessary files:**\n",
    " - event_np_list = pickled file of list of noun phrases\n",
    " - event_tagged_tweets = pickled file with NER and POS tags for all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event_data(event_name):\n",
    "    assert event_name in ['greece','tigray','rohingya','moria','channel'], f\"Oh no! We do not analyze {event_name} event\"\n",
    "    \n",
    "    print(f'Loading {event_name} data...')\n",
    "    try:\n",
    "        #sample = 100\n",
    "        \n",
    "        event_np_list = load_pickle(event_name + '_noun_phrases')#[:sample]\n",
    "        event_tagged_tweets = load_pickle(event_name + '_tagged_tweets')#[:sample]\n",
    "        return event_np_list,event_tagged_tweets\n",
    "    except:\n",
    "        print(f'The {event_name} files not found! First extract noun phrases and tag tweets of the {event_name}_df')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pipeline(event_name):\n",
    "\n",
    "    ####~~~~~~~~~~~~~~~~~~~~~ 1. LOAD THE DATA ~~~~~~~~~~~~~~~~~~~~~\n",
    "    event_np_list,event_tagged_tweets = load_event_data(event_name)\n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 2. GET POS AND NER TAGS ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # get list of tuples (POS-tags of each word, NER-tags of each named entity)\n",
    "    tweet_tags = cand_prep.get_tweet_tags(event_tagged_tweets) \n",
    "\n",
    "    \n",
    "    ####  ~~~~~~~~~~~~~~~~~~~~~ 3. PREPROCESS CANDIDATES ~~~~~~~~~~~~~~~~~~~~~\n",
    "    # ~~~~~~~~~~~~ processing of noun phrases ~~~~~~~~~~~~~~~~~~~~~\n",
    "    print(f'Processing {event_name} noun phrase candidates...')\n",
    "    \n",
    "    event_np_list = [['no_candidate'] if len(noun_ps)==0 else noun_ps for noun_ps in event_np_list ]\n",
    "    \n",
    "    #print(event_np_list)\n",
    "    print(f'Tagging {event_name} noun phrase candidates...')\n",
    "    #tag all tweets and save them in a list    \n",
    "    \n",
    "    # loading the pipeline - for candidates use flag tokenize_pretokenized\n",
    "    en_nlp = stanza.Pipeline(\"en\",  \n",
    "                             tokenize_pretokenized=True,\n",
    "                             ner_batch_size=4096,\n",
    "                             processors = \"tokenize,pos,lemma,depparse,ner\",\n",
    "                             verbose=False)\n",
    "    \n",
    "    tagged_np_cands =load_pickle('channel_tagged_cands')\n",
    "    #tagged_np_cands = [en_nlp('\\n\\n'.join(tweet_batch)) for tweet_batch in tqdm(event_np_list)]\n",
    "    #tagged_np_cands = [tagged_cand for tagged_cand in tqdm(batch(batched_np_list, en_nlp, batch_size=6000))]\n",
    "    \n",
    "    np_cand_heads = [cand_prep.get_cand_heads(tweet_cands) for tweet_cands in tqdm(tagged_np_cands)]\n",
    "\n",
    "    np_and_cand_list = cand_prep.get_cand_type(event_np_list,np_cand_heads, tweet_tags)\n",
    "    #print(event_np_list)\n",
    "          \n",
    "          \n",
    "    # ~~~~~~~~~~~~~~~~~~~~ combining candidate lists ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #concatenate corefs and noun phrase lists\n",
    "    nps_cands = [cand for cands in np_and_cand_list for cand in cands]\n",
    "    #candidate_list = coref_and_cand_list + np_and_cand_list\n",
    "\n",
    "    #unpack list of lists into one list\n",
    "    candidate_list = nps_cands\n",
    "          \n",
    "    nps_tagged = [sent for tagged_cand in tagged_np_cands for sent in tagged_cand.sentences ]\n",
    "\n",
    "    all_cands_tagged = nps_tagged\n",
    "    \n",
    "        \n",
    "    #print(len(candidate_list),'vs', len(all_cands_tagged))\n",
    "    cand_df = pd.DataFrame(\n",
    "        {'candidates': candidate_list,\n",
    "         'cand_tags': all_cands_tagged\n",
    "        })\n",
    "    \n",
    "    \n",
    "    cand_df['cand_text'] = cand_df.candidates.apply(lambda x: x[0])\n",
    "    cand_df['cand_head'] = cand_df.candidates.apply(lambda x: x[1])\n",
    "    cand_df['cand_type'] = cand_df.candidates.apply(lambda x: x[2])\n",
    "    cand_df['cand_len'] = cand_df.cand_text.apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "    count_cands = Counter(cand_df['candidates'])\n",
    "    cand_df['cand_freq'] = cand_df[\"candidates\"].map(count_cands)\n",
    "    \n",
    "    #count_cands[cand_df['cand_text']]\n",
    "    #count_sorted = sorted(count_cands.items(),key=lambda x: x[1],reverse=True)\n",
    "    cand_df.columns = cand_df.columns.str.strip()\n",
    "    \n",
    "          \n",
    "    # we sort the candidates by their length\n",
    "    cand_df.sort_values('cand_freq', ascending=False,inplace=True)\n",
    "    cand_df.reset_index(drop=True, inplace = True)\n",
    "    #remove dummy candidates that were used to avoid errors\n",
    "\n",
    "    \n",
    "    cand_df = cand_df[cand_df.cand_text != 'candidate_to_be_removed']\n",
    "    cand_df = cand_df[cand_df.cand_text != 'no_candidate']\n",
    "    print(len(cand_df))    \n",
    "    cand_df.reset_index(drop=True,inplace=True)\n",
    "      \n",
    "    return cand_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates as identified by stanza library still have a lot of noise to be removed. Cleaner candidates merge better and throwing away duplicate candidates or candidates without useful information speeds up merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally the candidates are cleaned before storing in a file prior to merging\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_cands(event_cands):\n",
    "    \"\"\"\n",
    "    Applying cleaning steps on candidates and engineering some features:\n",
    "     1. creating a column with length of the tweet (in chars)\n",
    "     2. lowercase the candidate information in the tuple with cand, candidate representative head and set of phrases heads\n",
    "     3. extract candidate text and keep only alphanumeric chars\n",
    "     4. remove candidates that are stopwords\n",
    "     5. remove candidates that are only numeric\n",
    "     6. remove candidates that are only 1 char long\n",
    "     \"\"\"\n",
    "\n",
    "    #stopwords\n",
    "    tqdm.pandas()\n",
    "    event_cands_clean = event_cands.copy()\n",
    "        \n",
    "    event_cands_clean['cand_text'] = event_cands_clean['cand_text'].progress_apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', '', x.lower()).strip())\n",
    "    event_cands_clean['cand_head'] = event_cands_clean['cand_head'].progress_apply(lambda x:re.sub(r'[^A-Za-z0-9]+', '', x.lower()).strip())\n",
    "\n",
    "    event_cands_clean = event_cands_clean[~event_cands_clean['cand_text'].isin(stopwords.words('english'))]\n",
    "    event_cands_clean['pure_chars'] = event_cands_clean['cand_text'].progress_apply(lambda x: x.replace(' ', ''))\n",
    "    event_cands_clean = event_cands_clean[~event_cands_clean['pure_chars'].str.isnumeric()]\n",
    "    event_cands_clean.drop('pure_chars',axis=1,inplace=True)\n",
    "    \n",
    "    event_cands_clean['string_len'] = event_cands_clean['cand_text'].progress_apply(len)\n",
    "    event_cands_clean = event_cands_clean[event_cands_clean['string_len']>1]\n",
    "    event_cands_clean = event_cands_clean.drop_duplicates(subset = [\"cand_text\",\"cand_type\"])\n",
    "    event_cands_clean.reset_index(drop=True, inplace=True)\n",
    "    print(f'The event has  {len(event_cands_clean)} unique candidates after cleaning')\n",
    "    return event_cands_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tigray data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 42853/42853 [00:01<00:00, 26712.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tigray noun phrase candidates...\n",
      "Tagging tigray noun phrase candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 42853/42853 [4:42:02<00:00,  2.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 42853/42853 [00:02<00:00, 20607.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 42853/42853 [10:07<00:00, 70.51it/s]\n",
      "  0%|                                                                                       | 0/386607 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 386607/386607 [00:01<00:00, 343355.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 386607/386607 [00:01<00:00, 357068.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 340063/340063 [00:00<00:00, 815415.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 336875/336875 [00:00<00:00, 891096.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The event has  105841 unique candidates after cleaning\n"
     ]
    }
   ],
   "source": [
    "#run the pipeline for tigray\n",
    "event_cands = pipeline('tigray')\n",
    "event_cands_clean = clean_cands(event_cands)\n",
    "pickle_file('tigray_cands', event_cands_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rohingya data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 29432/29432 [00:00<00:00, 32846.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rohingya noun phrase candidates...\n",
      "Tagging rohingya noun phrase candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 29432/29432 [3:04:03<00:00,  2.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 29432/29432 [00:01<00:00, 29140.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 29432/29432 [05:18<00:00, 92.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 237318/237318 [00:00<00:00, 345833.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 237318/237318 [00:00<00:00, 346957.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 212903/212903 [00:00<00:00, 791350.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 210592/210592 [00:00<00:00, 806872.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The event has  67008 unique candidates after cleaning\n"
     ]
    }
   ],
   "source": [
    "#run the pipeline for rohingya\n",
    "event_cands = pipeline('rohingya')\n",
    "event_cands_clean = clean_cands(event_cands)\n",
    "pickle_file('rohingya_cands', event_cands_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1191823/1191823 [00:05<00:00, 201997.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1191823/1191823 [00:04<00:00, 242652.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1004413/1004413 [00:02<00:00, 444368.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 995833/995833 [00:01<00:00, 787400.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The event has  291113 unique candidates after cleaning\n"
     ]
    }
   ],
   "source": [
    "#run the pipeline for greece\n",
    "event_cands = pipeline('greece')\n",
    "event_cands_clean = clean_cands(event_cands)\n",
    "pickle_file('greece_cands', event_cands_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading channel data...\n"
     ]
    }
   ],
   "source": [
    "#run the pipeline for channel\n",
    "event_cands = pipeline('channel')\n",
    "event_cands_clean = clean_cands(event_cands)\n",
    "pickle_file('channel_cands', event_cands_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
