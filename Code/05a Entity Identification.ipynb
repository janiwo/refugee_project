{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction and Alignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Necessary files:**\n",
    " - event_df = df\\_[event]\\_clean.csv file with event dataframes with clean unique tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python libraries\n",
    "import stanza\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# pickle functions for quick storage and loading of checkpoint files\n",
    "import pickle\n",
    "\n",
    "def pickle_file(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    #folder_name = re.sub(r'[12]', '', folder_name)\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 173758 tweets!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>refugee</th>\n",
       "      <th>migrant</th>\n",
       "      <th>immigrant</th>\n",
       "      <th>asylum_seeker</th>\n",
       "      <th>other</th>\n",
       "      <th>text_coherent</th>\n",
       "      <th>retweet_count_sum</th>\n",
       "      <th>count</th>\n",
       "      <th>text_alphanum</th>\n",
       "      <th>text_stm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WordPress.com</td>\n",
       "      <td>CHANNEL MIGRANT CRISIS – TODAYS VIDEOS FROM DO...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284639846930227200</td>\n",
       "      <td>2020-07-19 00:03:01+00:00</td>\n",
       "      <td>1039171425364520960</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>CHANNEL MIGRANT CRISIS TODAYS VIDEOS FROMDOVER.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>channel migrant crisis todays videos fromdover.</td>\n",
       "      <td>channel crisis today video fromdover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>“Chinese immorality [and] eccentricities … are...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284640070855729163</td>\n",
       "      <td>2020-07-19 00:03:55+00:00</td>\n",
       "      <td>153438157</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Chinese immorality [and] eccentricities are ab...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>chinese immorality and eccentricities are abho...</td>\n",
       "      <td>chinese immorality eccentricity abhorrent arya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@chrisgregson123 @VeuveK @CharlieHicks90 @Rudy...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284640230499328000</td>\n",
       "      <td>2020-07-19 00:04:33+00:00</td>\n",
       "      <td>503070765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>O / c Leavers voted for what they believed was...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>o c leavers voted for what they believed was b...</td>\n",
       "      <td>leaver voted believed best england wale howeve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@SkyNews It never will if uk keeps bring in hu...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284640911788576770</td>\n",
       "      <td>2020-07-19 00:07:15+00:00</td>\n",
       "      <td>1276420769384402944</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>It never will if uk keeps bring in hundreds of...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>it never will if uk keeps bring in hundreds of...</td>\n",
       "      <td>never keep bring hundred asylum seeker giving ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>How many illegal immigrants this week in #Dove...</td>\n",
       "      <td>en</td>\n",
       "      <td>1284641481576402945</td>\n",
       "      <td>2020-07-19 00:09:31+00:00</td>\n",
       "      <td>755084846783950848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>How many illegal immigrants this week in dover...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>how many illegal immigrants this week in dover...</td>\n",
       "      <td>many illegal week dover</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                               text lang  \\\n",
       "0       WordPress.com  CHANNEL MIGRANT CRISIS – TODAYS VIDEOS FROM DO...   en   \n",
       "1     Twitter Web App  “Chinese immorality [and] eccentricities … are...   en   \n",
       "2  Twitter for iPhone  @chrisgregson123 @VeuveK @CharlieHicks90 @Rudy...   en   \n",
       "3  Twitter for iPhone  @SkyNews It never will if uk keeps bring in hu...   en   \n",
       "4  Twitter for iPhone  How many illegal immigrants this week in #Dove...   en   \n",
       "\n",
       "                    id                 created_at            author_id  \\\n",
       "0  1284639846930227200  2020-07-19 00:03:01+00:00  1039171425364520960   \n",
       "1  1284640070855729163  2020-07-19 00:03:55+00:00            153438157   \n",
       "2  1284640230499328000  2020-07-19 00:04:33+00:00            503070765   \n",
       "3  1284640911788576770  2020-07-19 00:07:15+00:00  1276420769384402944   \n",
       "4  1284641481576402945  2020-07-19 00:09:31+00:00   755084846783950848   \n",
       "\n",
       "   retweet_count  reply_count  like_count  quote_count  ...  refugee migrant  \\\n",
       "0              0            0           0            0  ...    False    True   \n",
       "1             22            1          37            0  ...    False   False   \n",
       "2              0            0           0            1  ...    False   False   \n",
       "3              1            1           3            1  ...    False    True   \n",
       "4              0            0           0            0  ...    False   False   \n",
       "\n",
       "  immigrant asylum_seeker  other  \\\n",
       "0     False         False  False   \n",
       "1      True         False  False   \n",
       "2     False         False  False   \n",
       "3     False          True  False   \n",
       "4      True         False  False   \n",
       "\n",
       "                                       text_coherent  retweet_count_sum count  \\\n",
       "0    CHANNEL MIGRANT CRISIS TODAYS VIDEOS FROMDOVER.                  0     1   \n",
       "1  Chinese immorality [and] eccentricities are ab...                 22     1   \n",
       "2  O / c Leavers voted for what they believed was...                  0     1   \n",
       "3  It never will if uk keeps bring in hundreds of...                  1     1   \n",
       "4  How many illegal immigrants this week in dover...                  0     1   \n",
       "\n",
       "                                       text_alphanum  \\\n",
       "0    channel migrant crisis todays videos fromdover.   \n",
       "1  chinese immorality and eccentricities are abho...   \n",
       "2  o c leavers voted for what they believed was b...   \n",
       "3  it never will if uk keeps bring in hundreds of...   \n",
       "4  how many illegal immigrants this week in dover...   \n",
       "\n",
       "                                            text_stm  \n",
       "0               channel crisis today video fromdover  \n",
       "1  chinese immorality eccentricity abhorrent arya...  \n",
       "2  leaver voted believed best england wale howeve...  \n",
       "3  never keep bring hundred asylum seeker giving ...  \n",
       "4                            many illegal week dover  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_event_df(event_name):\n",
    "    # reading the clean event dataframes based on the event name\n",
    "    assert event_name in ['greece','channel','tigray','rohingya']\n",
    "    \n",
    "    event_path = fr\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_{event_name}_clean.csv\"\n",
    "    directory_path = os.getcwd() + \"/../../../../\" + event_path\n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df('channel')\n",
    "event_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Entity extraction: Tag tweets using stanza module to get NER and POS tags in tweets. \n",
    "Recommended to run on GPU to speed things up.\n",
    "\n",
    "*The function will also perform dependency parsing used at the end of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-20 22:20:33 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-06-20 22:20:33 INFO: Use device: cpu\n",
      "2021-06-20 22:20:33 INFO: Loading: tokenize\n",
      "2021-06-20 22:20:33 INFO: Loading: pos\n",
      "2021-06-20 22:20:33 INFO: Loading: lemma\n",
      "2021-06-20 22:20:33 INFO: Loading: depparse\n",
      "2021-06-20 22:20:34 INFO: Loading: ner\n",
      "2021-06-20 22:20:36 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ needed when running first time ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "#stanza.download(\"en\")\n",
    "#stanza.install_corenlp()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# loading the pipeline\n",
    "en_nlp = stanza.Pipeline(\"en\",  \n",
    "                         tokenize_pretokenized=False,\n",
    "                         ner_batch_size=4096,\n",
    "                         processors = \"tokenize,pos,lemma,depparse,ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the stanza pipeline on the parsing corpus for each event dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name = 'greece'\n",
    "event_df = read_event_df(event_name)\n",
    "event_tagged_tweets = [en_nlp(tweet_batch) for tweet_batch in tqdm(list(event_df['parsing_corpus']))]\n",
    "pickle_file(f'{event_name}_tagged_tweets',event_tagged_tweets)\n",
    "pickle_file(f'{event_name}_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name = 'channel'\n",
    "event_df = read_event_df(event_name)\n",
    "event_tagged_tweets = [en_nlp(tweet_batch) for tweet_batch in tqdm(list(event_df['parsing_corpus']))]\n",
    "pickle_file(f'{event_name}_tagged_tweets',event_tagged_tweets)\n",
    "pickle_file(f'{event_name}_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name = 'tigray'\n",
    "event_df = read_event_df(event_name)\n",
    "event_tagged_tweets = [en_nlp(tweet_batch) for tweet_batch in tqdm(list(event_df['parsing_corpus']))]\n",
    "pickle_file(f'{event_name}_tagged_tweets',event_tagged_tweets)\n",
    "pickle_file(f'{event_name}_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 22966/22966 [4:58:17<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "event_name = 'rohingya'\n",
    "event_df = read_event_df(event_name)\n",
    "event_tagged_tweets = [en_nlp(tweet_batch) for tweet_batch in tqdm(list(event_df['parsing_corpus']))]\n",
    "pickle_file(f'{event_name}_tagged_tweets',event_tagged_tweets)\n",
    "pickle_file(f'{event_name}_tagged_tweets',event_tagged_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entity alignment\n",
    "## 3.1. Merging step 1\n",
    "\n",
    "Pre-requisite files:\n",
    "\n",
    "    - event_tagged_tweets = python object with tagged tweets\n",
    "    \n",
    "The first merging step is an overlap between extraction and alignment. First, we extract the entities from the tagged tweets objects. Next, we process them, remove the duplicates by counting their instances and store them in a data frame. The dataframe is sorted by frequency of the entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_tags(tagged_tweets):\n",
    "    \"\"\"\n",
    "    Input: pre-tagged tweets\n",
    "    Output: list of lists containing named entities\n",
    "    \"\"\"\n",
    "    tweet_entities=[]\n",
    "    excluded_tags = ['CARDINAL', 'DATE', 'QUANTITY', 'TIME', 'PERCENT', 'MONEY', 'ORDINAL']\n",
    "    for tweet in tqdm(range(len(tagged_tweets))):\n",
    "            tweet_ner= [ent.text for sent in tagged_tweets[tweet].sentences for ent in sent.ents if ent.type not in excluded_tags]\n",
    "            tweet_entities.append(tweet_ner)\n",
    "    return tweet_entities  \n",
    "\n",
    "def get_ent_head(entity):\n",
    "    if len(word_tokenize(entity))>1:\n",
    "        entity_doc = en_nlp(entity)\n",
    "        #the root of NP has value 0. Since head is only one and stored in a list, we pick item [0]\n",
    "        return [word.text for tweet_ent in entity_doc.sentences for word in tweet_ent.words if word.head == 0][0]\n",
    "    else:\n",
    "        return entity\n",
    "\n",
    "def process_entity(entity):\n",
    "    # re.sub removes all non alpanumeric characters (and lower() lowercases)\n",
    "    # \" \".join and split removes the consecutive whitespaces created by replacing characters\n",
    "    entity = re.sub('[^0-9a-zA-Z]+', ' ', entity.lower())\n",
    "    return \" \".join(entity.split())\n",
    "\n",
    "def create_entities_df(tagged_tweets):\n",
    "    \n",
    "    event_entities = get_tweet_tags(tagged_tweets)\n",
    "    \n",
    "    list_of_entities = list()\n",
    "    for tweet_entities in tqdm(event_entities):\n",
    "        list_of_entities.append(tweet_entities)\n",
    "\n",
    "    entity_list = [process_entity(entity) for tweet in list_of_entities for entity in tweet]\n",
    "    \n",
    "    #create the count dictionary which will be converted into a df\n",
    "    counted_entities = Counter(entity_list)\n",
    "    ent_df = pd.DataFrame(counted_entities.items(),columns=['entity','freq']).sort_values('freq',ascending=False)\n",
    "    #filter out entities shorter than 2 characters and frequency less than 5\n",
    "    ent_df = ent_df[ent_df.freq>=5]\n",
    "    ent_df['len'] = ent_df.entity.apply(lambda x: len(x))\n",
    "    ent_df = ent_df[ent_df['len']>=2]\n",
    "    #find head of the entity\n",
    "    tqdm.pandas()\n",
    "    ent_df['head'] = ent_df.entity.progress_apply(get_ent_head)\n",
    "    ent_df.reset_index(drop=True,inplace=True)\n",
    "    return ent_df[['entity','freq','head']]\n",
    "\n",
    "def merging_step1(event_name):\n",
    "    print(f'loading {event_name} tagged tweets...')\n",
    "    tagged_tweets = load_pickle(f'{event_name}_tagged_tweets')\n",
    "    ent_df = create_entities_df(tagged_tweets)\n",
    "    pickle_file(f'{event_name}_ents',ent_df)\n",
    "    return ent_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df = merging_step1('rohingya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df = merging_step1('tigray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df = merging_step1('greece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df = merging_step1('channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Merging step 2: Similarity using Sentence BERT embeddings\n",
    "\n",
    "### 3.2.1. Encode entities with S-BERT embeddings\n",
    "\n",
    "Pre-requisite files:\n",
    " \n",
    "    - event_ents = df with event entities after 1st merging step to be encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embeddings(event_ents):\n",
    "    from time import time\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    #sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    sbert_model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "    bert_corpus = list(event_ents['entity'])\n",
    "\n",
    "    print(f'there are {len(bert_corpus)} entities to be encoded')\n",
    "    t0 = time()\n",
    "    ent_embeddings = sbert_model.encode(bert_corpus)\n",
    "    print(f'Training embeddings took {time()-t0} seconds')\n",
    "    return ent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_ents = load_pickle('greece_ents')\n",
    "\n",
    "document_embeddings = train_embeddings(event_ents)\n",
    "\n",
    "pickle_file('greece_embeddings_ents', document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ents = load_pickle('rohingya_ents')\n",
    "\n",
    "document_embeddings = train_embeddings( event_ents)\n",
    "\n",
    "pickle_file('rohingya_embeddings_ents', document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ents = load_pickle('tigray_ents')\n",
    "\n",
    "document_embeddings = train_embeddings(event_ents)\n",
    "\n",
    "pickle_file('tigray_embeddings_ents', document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ents = load_pickle('channel_ents')\n",
    "\n",
    "document_embeddings = train_embeddings(event_ents)\n",
    "\n",
    "pickle_file('channel_embeddings_ents', document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "#sound warning after the code is done running\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Creating a similarity dataframe\n",
    "The similarity dataframe enables faster lookup and experimentation with optimal thresholds when aligning entities\n",
    "Pre-requisite files:\n",
    "\n",
    "    - [event_name]_ents\n",
    "    - [event_name]_embeddings_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def create_sim_df(event_name, sim_threshold=0.6):\n",
    "    \"\"\"\n",
    "    1. loads necessary files for the selected event \n",
    "    2. calculates pairwise cosine similarity of the entity embeddings \n",
    "    3. stores the pairs and their similarity in the similarity dataframe\"\"\"\n",
    "    event_entities = load_pickle(f'{event_name}_ents')\n",
    "    ent_embeddings = load_pickle(f'{event_name}_embeddings_ents')\n",
    "    rows_list = []\n",
    "    \n",
    "    #calculate pairwise similarities\n",
    "    sims = cosine_similarity(ent_embeddings)\n",
    "    \n",
    "    # check whether a pair similarity is above the given threshold, store in the sim_df if true\n",
    "    for up_ent_id in tqdm(range(len(event_entities))):\n",
    "        for low_ent_id in range(len(event_entities)):\n",
    "            dict1 = {}\n",
    "            # get input row in dictionary format\n",
    "            # key = col_name\n",
    "            if sims[up_ent_id][low_ent_id]>sim_threshold:\n",
    "                dict1.update({'text': event_entities['entity'][up_ent_id], \n",
    "                              'text_to_compare':event_entities['entity'][low_ent_id], \n",
    "                              'sim':sims[up_ent_id][low_ent_id]}) \n",
    "                rows_list.append(dict1)\n",
    "\n",
    "    sim_df = pd.DataFrame(rows_list)\n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = create_sim_df('greece')\n",
    "pickle_file('greece_sim_df_ents',sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = create_sim_df('rohingya')\n",
    "pickle_file('rohingya_sim_df_ents',sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = create_sim_df('tigray')\n",
    "pickle_file('tigray_sim_df_ents',sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = create_sim_df('channel')\n",
    "pickle_file('channel_sim_df_ents',sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_df enables lookup of the most similar terms \n",
    "sim_df = load_pickle('greece_sim_df_ents')\n",
    "sim_df[sim_df.text=='greece'].sort_values('sim',ascending=False)[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Aligning the entities\n",
    "\n",
    "Pre-requisite files:\n",
    "\n",
    "    - sim_df = df with all similar entity pairs)\n",
    "    - entities = df of all entities (including dissimilar) with frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_mixed_types):\n",
    "    # helper function to convert a list of mixed types (strings and lists) into a list\n",
    "    flat_list = []\n",
    "    for element in list_of_mixed_types:\n",
    "        if isinstance(element,list): \n",
    "            flat_list.extend(flatten(element))\n",
    "        else: \n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "\n",
    "def align_transitively(what_merged):\n",
    "    \"\"\"\n",
    "    function to transitively merge the entities\n",
    "    \"\"\"\n",
    "    merged_dict = defaultdict(list)\n",
    "    merged_ent = set()\n",
    "    \n",
    "    for key,value in tqdm(what_merged.items()):\n",
    "        merged_ent.update(what_merged[key])\n",
    "        for item in value:\n",
    "            merged_dict[key].extend(what_merged[key])\n",
    "            if item in what_merged.keys():\n",
    "                merged_dict[key].extend(what_merged[item])\n",
    "                merged_dict[key] = list(set(merged_dict[key]))\n",
    "\n",
    "        #print('-------loop------')\n",
    "        for ent,merged in what_merged.items():\n",
    "            for item2 in merged:\n",
    "                if item2 in what_merged[key] and item2 in what_merged[ent] and key!=ent:\n",
    "                    merged_dict[key].extend(what_merged[key] + what_merged[ent])\n",
    "                    merged_dict[key].append(ent)\n",
    "                    merged_ent.add(ent)\n",
    "        merged_dict[key] = list(set(merged_dict[key]))  \n",
    "        #merged_dict[key].pop(merged_dict[key].index(key))\n",
    "    return merged_dict\n",
    "\n",
    "def finalize_ents(merged_dict):\n",
    "    final_ents = defaultdict()\n",
    "    check_set = set()\n",
    "    for key,value in merged_dict.items():\n",
    "        if key not in check_set:\n",
    "            final_ents[key] = merged_dict[key]\n",
    "            check_set.update(merged_dict[key])\n",
    "    \n",
    "    for key in final_ents.keys():\n",
    "        if key in final_ents[key]:\n",
    "            final_ents[key].pop(final_ents[key].index(key))\n",
    "            \n",
    "    # add manual terms to the dict so (refugees and asyslum seekers) and (migrants and immigrants) are together\n",
    "    final_ents['refugees'] = ['refugee','asylum seeker','asylum seekers']\n",
    "    final_ents['migrants'] = ['migrant','immigrant','immigrants']\n",
    "    return final_ents\n",
    "\n",
    "\n",
    "def merging_step2(entities,sim_df,low_threshold=0.8, high_threshold =0.9, transitive=False): \n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    entities = entity df with entity frequencies\n",
    "    sim_df = similarity dataframe with all pairs of entities that are at least [threshold] similar\n",
    "    low_threshold = determines entity merge in the first merging round \n",
    "    high_threshold = determines entity merge in the second and subsequent merging rounds if transitive merge is enabled\n",
    "    transitive = True if enabled\n",
    "    returns:\n",
    "    what_merged = dictionary with core entities as keys and lists of their merged entities as values\n",
    "    \"\"\"\n",
    "    what_merged = defaultdict(list)\n",
    "    merged_ents = set()\n",
    "\n",
    "    #merge the dataframes so we have information about frequencies of entities in sim_df\n",
    "    # use outer join to include the entities that are not similar to any other entity (thus not in sim_df)\n",
    "    merged_df_ = pd.merge(sim_df,entities[entities.freq>=5], how='outer', left_on='text', right_on='entity')\n",
    "    merged_df = pd.merge(merged_df_,entities[entities.freq>=5], how='outer', left_on='text_to_compare', right_on='entity')\n",
    "    \n",
    "    #blank fields in sim_df's text are the entities that are dissimilar to the rest, pass the value from entity text\n",
    "    merged_df.entity_x.fillna(merged_df.entity_y, inplace=True) \n",
    "    merged_df.freq_x.fillna(merged_df.freq_y, inplace=True) \n",
    "    \n",
    "    merged_df.drop(['text','text_to_compare'],axis=1,inplace=True)\n",
    "\n",
    "    # and select only the rows above the lower threshold, so we do not have to filter by it in the loop\n",
    "    merged_df_small = merged_df[merged_df.sim>low_threshold]\n",
    "\n",
    "    print(f'finding merged entities...')\n",
    "    # create an object to iterate over unique entities\n",
    "    unique_merged_df = merged_df.groupby(by=['entity_x'], sort = False, as_index=False).agg({'freq_x':'max'}).sort_values('freq_x',ascending=False)\n",
    "\n",
    "    def get_merged_entities(entity,transitive = False):\n",
    "        \"\"\"\n",
    "        Store merged entities as list\n",
    "        \"\"\"\n",
    "        lookup_df = pd.DataFrame({'entity_x': [], 'entity_y':[]})\n",
    "\n",
    "        if entity not in merged_ents:\n",
    "            if transitive == False:\n",
    "                lookup_df = merged_df_small[merged_df_small.entity_x==entity]\n",
    "                merged_ents.update(list(lookup_df['entity_y']))\n",
    "            else:\n",
    "                lookup_df = merged_df_small[(merged_df_small.entity_x==entity) & (merged_df_small.sim>high_threshold)]\n",
    "        \n",
    "        #clean nans \n",
    "        lookup_df = lookup_df[~lookup_df['entity_y'].isnull()]\n",
    "        return list(lookup_df['entity_y'])\n",
    "    \n",
    "    tqdm.pandas() \n",
    "    #note that in the first round the transitiveness is False\n",
    "    unique_merged_df['merged'] =  unique_merged_df.entity_x.progress_apply(get_merged_entities)\n",
    "\n",
    "    #convert the dataframe into a dictionary\n",
    "    for entity,merged_list in tqdm(zip(unique_merged_df['entity_x'],unique_merged_df['merged'])):\n",
    "        what_merged[entity] = merged_list.copy()\n",
    "        if transitive:\n",
    "            for merged_ent in merged_list:\n",
    "                what_merged[entity].append(get_merged_entities(merged_ent,transitive=True))\n",
    "        what_merged[entity] = list(set(flatten(what_merged[entity]) ) ) \n",
    "    \n",
    "    return what_merged\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_merging_step2(event_name,entity_type='ents',low_threshold=0.8, high_threshold = 0.9, transitive=True):\n",
    "    sim_df = load_pickle(f'{event_name}_sim_df_{entity_type}')\n",
    "    entities = load_pickle(f'{event_name}_{entity_type}')\n",
    "\n",
    "    what_merged = merging_step2(entities,\n",
    "                                sim_df,\n",
    "                                low_threshold=low_threshold, \n",
    "                                high_threshold=high_threshold,\n",
    "                                transitive=transitive)\n",
    "    \n",
    "    if transitive:\n",
    "        what_merged = align_transitively(what_merged)\n",
    "    \n",
    "    if event_name == 'channel' or event_name == 'greece':\n",
    "        what_merged['germany'].pop(what_merged['germany'].index('austria'))\n",
    "        what_merged['germany'].pop(what_merged['germany'].index('austrian'))\n",
    "    \n",
    "    if event_name == 'greece':\n",
    "        what_merged['russia'].pop(what_merged['russia'].index('ukraine'))\n",
    "            \n",
    "    what_merged = finalize_ents(what_merged)\n",
    "    #pickle_file(f'{event_name}_final_{entity_type}',final_ents)\n",
    "    pickle_file(f'{event_name}_what_merged',what_merged)\n",
    "    return  what_merged #final_ents,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = run_merging_step2('tigray',low_threshold=0.7,high_threshold = 0.9, transitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = run_merging_step2('rohingya',low_threshold=0.7,high_threshold = 0.9,transitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding merged entities...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d172c5251541d09ebb49a1b9110274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2076 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f12c4308bf480e97816d7ece0e9d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_dict = run_merging_step2('greece',low_threshold=0.7, high_threshold = 0.9,transitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = run_merging_step2('channel',low_threshold=0.7,high_threshold=0.85, transitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['turkey', 'greece', 'eu', 'syria', 'erdogan', 'russia', 'idlib', 'uk', 'assad', 'germany', 'nato', 'us', 'un', 'muslim', 'lesbos', 'iran', 'afghanistan', 'italy', 'france', 'bulgaria']\n"
     ]
    }
   ],
   "source": [
    "print(list(merged_dict.keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether the similar pairs from sim_df correspond to entities in merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['north syrian',\n",
       " 'syrian war',\n",
       " 'syrias',\n",
       " 'syrians',\n",
       " 'russian syrian',\n",
       " 'northern syria',\n",
       " 'ne syria',\n",
       " 'the syrian arab',\n",
       " 'syria war',\n",
       " 'syrian',\n",
       " 'nw syria',\n",
       " 'the syrian war',\n",
       " 'syrian arab republic',\n",
       " 'damascus',\n",
       " 'turkish syrian',\n",
       " 'the syrian army',\n",
       " 'the syrian crisis',\n",
       " 'syrian coalition',\n",
       " 'the syrian arab republic',\n",
       " 'syrian army',\n",
       " 'northwest syria',\n",
       " 'syrian arab',\n",
       " 'north syria',\n",
       " 'syrian regime',\n",
       " 'the syrian civil war',\n",
       " 'n syria']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dict['syria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_to_compare</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>syria</td>\n",
       "      <td>syria</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrias</td>\n",
       "      <td>0.931801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>syria</td>\n",
       "      <td>n syria</td>\n",
       "      <td>0.913211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian</td>\n",
       "      <td>0.873278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrians</td>\n",
       "      <td>0.835501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian arab republic</td>\n",
       "      <td>0.828813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>syria</td>\n",
       "      <td>nw syria</td>\n",
       "      <td>0.822142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>syria</td>\n",
       "      <td>north syria</td>\n",
       "      <td>0.812446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>syria</td>\n",
       "      <td>ne syria</td>\n",
       "      <td>0.810139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>syria</td>\n",
       "      <td>northern syria</td>\n",
       "      <td>0.805941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>syria</td>\n",
       "      <td>northwest syria</td>\n",
       "      <td>0.805345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>syria</td>\n",
       "      <td>the syrian arab republic</td>\n",
       "      <td>0.797881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>syria</td>\n",
       "      <td>the syrian crisis</td>\n",
       "      <td>0.796544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>syria</td>\n",
       "      <td>the syrian war</td>\n",
       "      <td>0.777072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>syria</td>\n",
       "      <td>syria war</td>\n",
       "      <td>0.761021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>syria</td>\n",
       "      <td>north syrian</td>\n",
       "      <td>0.760741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian war</td>\n",
       "      <td>0.758014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian arab</td>\n",
       "      <td>0.754937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>syria</td>\n",
       "      <td>damascus</td>\n",
       "      <td>0.749163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>syria</td>\n",
       "      <td>turkish syrian</td>\n",
       "      <td>0.741895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian army</td>\n",
       "      <td>0.741441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian regime</td>\n",
       "      <td>0.741288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>syria</td>\n",
       "      <td>the syrian arab</td>\n",
       "      <td>0.737891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>syria</td>\n",
       "      <td>the syrian army</td>\n",
       "      <td>0.734514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>syria</td>\n",
       "      <td>the syrian civil war</td>\n",
       "      <td>0.732967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian coalition</td>\n",
       "      <td>0.730658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>syria</td>\n",
       "      <td>russian syrian</td>\n",
       "      <td>0.701742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian muslims</td>\n",
       "      <td>0.693782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian kurds</td>\n",
       "      <td>0.687873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>syria</td>\n",
       "      <td>syrian muslim</td>\n",
       "      <td>0.686973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text           text_to_compare       sim\n",
       "241  syria                     syria  1.000000\n",
       "249  syria                    syrias  0.931801\n",
       "286  syria                   n syria  0.913211\n",
       "242  syria                    syrian  0.873278\n",
       "243  syria                   syrians  0.835501\n",
       "265  syria      syrian arab republic  0.828813\n",
       "266  syria                  nw syria  0.822142\n",
       "254  syria               north syria  0.812446\n",
       "274  syria                  ne syria  0.810139\n",
       "250  syria            northern syria  0.805941\n",
       "261  syria           northwest syria  0.805345\n",
       "247  syria  the syrian arab republic  0.797881\n",
       "271  syria         the syrian crisis  0.796544\n",
       "251  syria            the syrian war  0.777072\n",
       "279  syria                 syria war  0.761021\n",
       "282  syria              north syrian  0.760741\n",
       "263  syria                syrian war  0.758014\n",
       "276  syria               syrian arab  0.754937\n",
       "246  syria                  damascus  0.749163\n",
       "273  syria            turkish syrian  0.741895\n",
       "258  syria               syrian army  0.741441\n",
       "248  syria             syrian regime  0.741288\n",
       "252  syria           the syrian arab  0.737891\n",
       "259  syria           the syrian army  0.734514\n",
       "253  syria      the syrian civil war  0.732967\n",
       "284  syria          syrian coalition  0.730658\n",
       "260  syria            russian syrian  0.701742\n",
       "268  syria            syrian muslims  0.693782\n",
       "283  syria              syrian kurds  0.687873\n",
       "272  syria             syrian muslim  0.686973"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df[sim_df.text=='syria'].sort_values('sim',ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Find entity frequencies\n",
    "\n",
    "Pre-requisite files:\n",
    "    \n",
    "    - [event_name]_what_merged = dictionary of merged entities\n",
    "    - event_df = event dataset\n",
    "    \n",
    "This part serves solely for illustrative purposes of how much have been the counts of analyzable entities increased thanks to entity alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def get_frequencies(what_merged, event_df):\n",
    "    \"\"\"\n",
    "    in part 1: get frequencies of each item in what_merged dictionary from the corpus\n",
    "    \n",
    "    in part 2: correct frequencies for repeating terms\n",
    "    \"\"\"\n",
    "    #~~~~ part 1 ~~~~\n",
    "    freq_dict = defaultdict(list)\n",
    "    for key in tqdm(what_merged.keys()):\n",
    "        freq_dict2 = dict()\n",
    "        c = sum(event_df['text_alphanum'].apply(lambda x: x.count(f' {key} ')))\n",
    "        freq_dict[key].append(c)\n",
    "        for item in what_merged[key]:\n",
    "            c2 = sum(event_df['text_alphanum'].apply(lambda x: x.count(f' {item} ')))\n",
    "            freq_dict2[item]=c2\n",
    "        freq_dict[key].append(freq_dict2)\n",
    "    \n",
    "    #~~~~ part 2 ~~~~\n",
    "    \n",
    "    # Instantiate dict to store individual count for entity and merged count\n",
    "    freq_counts = defaultdict(dict)\n",
    "    \n",
    "    # Iterate over entities\n",
    "    for ent in freq_dict.keys():\n",
    "        \n",
    "        #print(f\"Ent: {ent}\")\n",
    "        adj_freq = 0\n",
    "        \n",
    "        # Store number of occurences of root entity\n",
    "        root_freq = freq_dict[ent][0]\n",
    "        \n",
    "        # Instantiate empty list where entities are stored that have already been counted\n",
    "        roots = []\n",
    "        \n",
    "        # Integrate main entity into dict of merged entities\n",
    "        ent_dict = freq_dict[ent][1]\n",
    "        ent_dict[ent] = freq_dict[ent][0]\n",
    "        \n",
    "        # Iterate over merged entities based on their length (in term of tokens) in ascending order\n",
    "        for merged_ent in sorted(ent_dict.keys(), key = len):\n",
    "            #print(f\"Merged Ent: {merged_ent}\")\n",
    "            \n",
    "            # Create all possible sub-combinations of entity while keeping the order constant\n",
    "            # For example: \"Greece Turkey Border\" yields Greece, Turkey, Border, Greece Turkey, Turkey Border, Greece Turkey Border\n",
    "            merged_ent_list = merged_ent.split(\" \")\n",
    "            all_slices = [merged_ent_list[s:e] for s, e in combinations(range(len(merged_ent_list)+1), 2)]\n",
    "            all_slices_strings = [\" \".join(l) for l in all_slices]\n",
    "            \n",
    "            # Iterate over all entities that have already been counted\n",
    "            for root in roots:\n",
    "                \n",
    "                # Check if any of the substring of current entity has already been counted, if yes don't count it again\n",
    "                if root in all_slices:\n",
    "                    #print(f\"{merged_ent} was removed to prevent double count. {root} has already been counted\")\n",
    "                    break\n",
    "            \n",
    "            # If entity has not been counted, count it and add entity to counted entities\n",
    "            else:\n",
    "                roots.append([merged_ent])\n",
    "                #print(roots)\n",
    "                adj_freq += ent_dict[merged_ent]\n",
    "                #print(f\"Frequency incremented by {ent_dict[merged_ent]}\")\n",
    "        \n",
    "        # Update frequency dict\n",
    "        freq_counts[ent][\"Root\"] = root_freq\n",
    "        freq_counts[ent][\"Overall\"] = adj_freq\n",
    "\n",
    "\n",
    "    return freq_dict,freq_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = read_event_df(rohingya_url)\n",
    "what_merged = load_pickle('rohingya_what_merged')\n",
    "\n",
    "freq_dict,total_freq = get_frequencies(what_merged, event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = read_event_df(tigray_url)\n",
    "what_merged = load_pickle('tigray_what_merged')\n",
    "\n",
    "freq_dict,total_freq = get_frequencies(what_merged, event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = read_event_df(greece_url)\n",
    "what_merged = load_pickle('greece_what_merged')\n",
    "\n",
    "freq_dict,total_freq = get_frequencies(what_merged, event_df)\n",
    "#freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = read_event_df(channel_url)\n",
    "what_merged = load_pickle('channel_what_merged')\n",
    "\n",
    "freq_dict,total_freq = get_frequencies(what_merged, event_df)\n",
    "#freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(total_freq.keys())[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word= 'brexit'\n",
    "\n",
    "print(total_freq[word])\n",
    "print(list(dict(sorted(freq_dict[word][1].items(), key=lambda item: item[1],reverse=True)).keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(total_freq.items(), key=lambda item: item[1]['Overall'],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
