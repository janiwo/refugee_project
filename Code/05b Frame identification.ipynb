{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f1404c",
   "metadata": {},
   "source": [
    "# Frame identification\n",
    "\n",
    "2 versions - By factor analysis and by using BERT embeddings and clustering (affinity propagation)\n",
    "https://www.datacamp.com/community/tutorials/introduction-factor-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "5f435aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from time import time\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# functions pickle_file and load_pickle help with storing checkpoint files in the event folders on drive\n",
    "def pickle_file(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aab72c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 137462 tweets!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>migrant</th>\n",
       "      <th>immigrant</th>\n",
       "      <th>asylum_seeker</th>\n",
       "      <th>other</th>\n",
       "      <th>date</th>\n",
       "      <th>text_coherent</th>\n",
       "      <th>retweet_count_sum</th>\n",
       "      <th>count</th>\n",
       "      <th>text_alphanum</th>\n",
       "      <th>text_stm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hootsuite Inc.</td>\n",
       "      <td>PA Ambassador in Bosnia &amp;amp; Herzegovina says...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227019550912372737</td>\n",
       "      <td>2020-02-11 00:00:32+00:00</td>\n",
       "      <td>81136269</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>PA Ambassador in Bosnia &amp; Herzegovina says lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>pa ambassador in bosnia herzegovina says life ...</td>\n",
       "      <td>ambassador bosnia herzegovina say life europe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u.fooo.ooo</td>\n",
       "      <td>[ðŸ”´ NEWS] Greece plans floating sea border wall...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227019556167864321</td>\n",
       "      <td>2020-02-11 00:00:33+00:00</td>\n",
       "      <td>1052191553802854407</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>[ NEWS] Greece plans floating sea border wall ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>news greece plans floating sea border wall to...</td>\n",
       "      <td>news greece plan floating border wall keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>Latest Battle for Idlib Could Send Another Wav...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227021374780313601</td>\n",
       "      <td>2020-02-11 00:07:47+00:00</td>\n",
       "      <td>18570470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>Latest Battle for Idlib Could Send Another Wav...</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>latest battle for idlib could send another wav...</td>\n",
       "      <td>latest battle idlib send another wave europe w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tweepsmap</td>\n",
       "      <td>UNHCR calls for decisive action to end alarmin...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227021789525614594</td>\n",
       "      <td>2020-02-11 00:09:26+00:00</td>\n",
       "      <td>62632306</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>UNHCR calls for decisive action to end alarmin...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>unhcr calls for decisive action to end alarmin...</td>\n",
       "      <td>unhcr call decisive action alarming condition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>It is not your own feet sinking in the mud, bo...</td>\n",
       "      <td>en</td>\n",
       "      <td>1227022233484308481</td>\n",
       "      <td>2020-02-11 00:11:12+00:00</td>\n",
       "      <td>2729959018</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>It is not your own feet sinking in the mud, boy.</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>it is not your own feet sinking in the mud boy.</td>\n",
       "      <td>foot sinking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                               text lang  \\\n",
       "0      Hootsuite Inc.  PA Ambassador in Bosnia &amp; Herzegovina says...   en   \n",
       "1          u.fooo.ooo  [ðŸ”´ NEWS] Greece plans floating sea border wall...   en   \n",
       "2  Twitter Web Client  Latest Battle for Idlib Could Send Another Wav...   en   \n",
       "3           Tweepsmap  UNHCR calls for decisive action to end alarmin...   en   \n",
       "4  Twitter for iPhone  It is not your own feet sinking in the mud, bo...   en   \n",
       "\n",
       "                    id                 created_at            author_id  \\\n",
       "0  1227019550912372737  2020-02-11 00:00:32+00:00             81136269   \n",
       "1  1227019556167864321  2020-02-11 00:00:33+00:00  1052191553802854407   \n",
       "2  1227021374780313601  2020-02-11 00:07:47+00:00             18570470   \n",
       "3  1227021789525614594  2020-02-11 00:09:26+00:00             62632306   \n",
       "4  1227022233484308481  2020-02-11 00:11:12+00:00           2729959018   \n",
       "\n",
       "   retweet_count  reply_count  like_count  quote_count  ...  migrant  \\\n",
       "0              1            0           1            0  ...    False   \n",
       "1              0            0           0            0  ...    False   \n",
       "2              0            0           1            1  ...    False   \n",
       "3              0            0           0            0  ...    False   \n",
       "4              9            1          29            1  ...    False   \n",
       "\n",
       "  immigrant asylum_seeker  other        date  \\\n",
       "0     False         False  False  2020-02-11   \n",
       "1     False         False  False  2020-02-11   \n",
       "2     False         False  False  2020-02-11   \n",
       "3     False         False  False  2020-02-11   \n",
       "4     False         False  False  2020-02-11   \n",
       "\n",
       "                                       text_coherent  retweet_count_sum count  \\\n",
       "0  PA Ambassador in Bosnia & Herzegovina says lif...                  1     1   \n",
       "1  [ NEWS] Greece plans floating sea border wall ...                  0     1   \n",
       "2  Latest Battle for Idlib Could Send Another Wav...                  8     5   \n",
       "3  UNHCR calls for decisive action to end alarmin...                  9     3   \n",
       "4   It is not your own feet sinking in the mud, boy.                  9     1   \n",
       "\n",
       "                                       text_alphanum  \\\n",
       "0  pa ambassador in bosnia herzegovina says life ...   \n",
       "1   news greece plans floating sea border wall to...   \n",
       "2  latest battle for idlib could send another wav...   \n",
       "3  unhcr calls for decisive action to end alarmin...   \n",
       "4    it is not your own feet sinking in the mud boy.   \n",
       "\n",
       "                                            text_stm  \n",
       "0  ambassador bosnia herzegovina say life europe ...  \n",
       "1         news greece plan floating border wall keep  \n",
       "2  latest battle idlib send another wave europe w...  \n",
       "3  unhcr call decisive action alarming condition ...  \n",
       "4                                       foot sinking  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_event_df(event_name):\n",
    "    data_url = fr\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_{event_name}_clean.csv\"\n",
    "    directory_path = os.getcwd() + \"/../../../../\" + data_url \n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec09d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 137462 tweets!\n",
      "loaded 173758 tweets!\n",
      "loaded 42853 tweets!\n",
      "loaded 29432 tweets!\n"
     ]
    }
   ],
   "source": [
    "df_greece = read_event_df('greece')\n",
    "df_channel = read_event_df('channel')\n",
    "df_tigray = read_event_df('tigray')\n",
    "df_rohingya = read_event_df('rohingya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324f19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = os.getcwd() + \"/../../../../\" + r\"/Dropbox (CBS)/Master thesis data/\"\n",
    "event_url = file_url + r\"Event Dataframes/\"\n",
    "event_url_clean = event_url + r\"Clean/\"\n",
    "\n",
    "candidate_url = file_url + r\"Candidate Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ed2e51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_url_clean = event_url_clean + r\"df_tigray_clean.csv\" # location of clean Tigray dataset\n",
    "greece_url_clean = event_url_clean + r\"df_greece_clean.csv\" # location of clean Greece dataset\n",
    "rohingya_url_clean = event_url_clean + r\"df_rohingya_clean.csv\" # location clean of Rohingya dataset\n",
    "channel_url_clean = event_url_clean +r\"df_channel_clean.csv\" #Location of clean Channel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "345720e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_url_fi = event_url_clean + r\"df_tigray_fi.csv\" # location of Tigray dataset for frame identification\n",
    "greece_url_fi = event_url_clean + r\"df_greece_fi.csv\" # location of Greece dataset for frame identification\n",
    "rohingya_url_fi = event_url_clean + r\"df_rohingya_fi.csv\" # location of Rohingya dataset for frame identification\n",
    "channel_url_fi = event_url_clean +r\"df_channel_fi.csv\" #Location of Channel dataset for frame identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f98af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_candidate_url = candidate_url + r\"tigray/tigray_ents\"\n",
    "greece_candidate_url = candidate_url + r\"greece/greece_ents\"\n",
    "rohingya_candidate_url = candidate_url + r\"rohingya/rohingya_ents\"\n",
    "channel_candidate_url = candidate_url + r\"channel/channel_ents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8cfbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(greece_candidate_url,\"rb\") as input_file:\n",
    "    ents = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df22d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_list(url):\n",
    "    with open(url,\"rb\") as input_file:\n",
    "        ents = pickle.load(input_file)\n",
    "        ents = ents[ents[\"freq\"]>15]\n",
    "        \n",
    "    return list(ents[\"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc3f7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_ents = get_entity_list(tigray_candidate_url)\n",
    "greece_ents = get_entity_list(greece_candidate_url)\n",
    "rohingya_ents = get_entity_list(rohingya_candidate_url)\n",
    "channel_ents = get_entity_list(channel_candidate_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eb21c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_list = set(tigray_ents + greece_ents + rohingya_ents + channel_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e53bec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_list1 = str.split(\"abiy addis aegean afewerki afeworki afghan afghanistan africa african america amnesty ahmed american amhara andrew ankara antony arab asean asia assad aung bachelet balukhali bangladesh bangladeshi biden blinken boris borisjohnson borrell brexit brit britain british brussels bulgaria burma burmese calais canada channel china commission commissioner corona coronavirus council covid cox dover dublin edirne england english erdoan erdogan eritrea eritrean ethiopia ethiopian euro europe european evros farage filippo fontelles france french freya_cole garneau geneva german germany grandi greece greek guterres haavisto hamdayet harris hindu hitsats houthi houthis idlib idp india indian iran iraq isaias isayas isi israel italy jammu jazeera jerry johnson josep junta justin kachin kadra karen kayin kamala kent kenya kurd labour linda lindat_g lebanon lesbos lesvos libya london maikadra manipur marc merkel michelle mizoram moria muslim mutraw myanmar nation nations nationshumanrights nato nazi nigel november oromia oromo pakistan patel president priti putin reuters rohingya rohingyas russia russian samri secretary shimelba shire somalia spain state sudan sudanese syria syrian tegaru tigrai tigrayan tigrayans thai thailand tory tplf trudeau trump turk turkey turkish unhcr unicef union united unsc us yemen youtube\",\" \")\n",
    "ne_list2 = str.split(\"able absolutely access according account accountable across action actually affected agency agree agreement alive allegation allow allowed allowing almost alone along already also always ambassador amid among another answer anti anymore anyone anything area arent around arrived article attempt attention away back based basic become believe best better black blame blocked born breaking bring brother build call called calling came cant case caught cause centre change claim claiming clear clearly close come coming comment commited commiting completely concern concerned condemn condition confirmed  continue continues could country course cover created credible crisis currently daily day deal dear decade decision demand department despite didnt difference different dire doesnt done dont east eastern easy either effort else endf enough especially ethnic even ever every everyone everything evidence evil exactly expect extremely face facility facing fact failed fake false federal feel find first found four forget free fuck fucking full genuine getting given giving great ground group happy imagine including issue instead isnt give going good half hand happen happened happening hard head heading held high horn hour however huge idea image immediate immediately independent information inside internal internally interview investigate investigation issue join journalist keep kind know known lack land landing largest last latest leader least left let letting level lie like likely little live living load local long longer look looking lost made mail major majority make making many massive matter maybe mean medium member middle might migration mind month morning mostly move much must name national near nearly need needed neighboring never news next nobody north northern nothing obviously office official one ongoing operation others paid part party pas past perhaps place plan please point post press prevails prime probably problem process programme provide public push putting question quite rather reach read real realise reality really reason received recent record remains remember report reported reporting resident response responsible rest result right said satellite say saying second seek seeking seem seems seen send sending sent series service several shame share shit show side simple simply since single site situation small someone something soon sorry sort south source speak special spread stand star start started statement street still stop stopped stopping story stupid sure surely system take taken taking talk talking tell term testimony thank thanks thats there theyre thing think though thought three time today told took torn towards town tried true truth trying turn tweet understand unless urgent urgently used using video vice view virus visit voice wait waiting want wanted watch water week welcome well west western withdraw within without whats whilst white whole wing wish wonder wont word world worse wrong would year yesterday young youre\",\" \")\n",
    "ne_list = ne_list1 + ne_list2)\n",
    "len(ne_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6c931546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 42843 tweets!\n",
      "loaded 137418 tweets!\n",
      "loaded 29423 tweets!\n",
      "loaded 173615 tweets!\n"
     ]
    }
   ],
   "source": [
    "df_tigray = read_event_df(tigray_url_clean)\n",
    "df_greece = read_event_df(greece_url_clean)\n",
    "df_rohingya = read_event_df(rohingya_url_clean)\n",
    "df_channel = read_event_df(channel_url_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6981325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_words(df_col):\n",
    "    \n",
    "    all_words = list()\n",
    "    \n",
    "    for words in df_col:\n",
    "        for word in words:\n",
    "            all_words.append(word)\n",
    "            \n",
    "    counter = Counter(all_words)\n",
    "    \n",
    "    return [pair[0] for pair in counter.most_common(int(len(counter)*0.025))] #0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd6c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(df_col):\n",
    "    \"\"\"\n",
    "    Takes a list with strings and returns a list with tokens\n",
    "    \"\"\"\n",
    "    print(\"Tokenizing tweets...\\n\")\n",
    "    return df_col.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "def remove_unfrequent_words(df_col):\n",
    "    print(\"Removing unfrequent words...\\n\")\n",
    "    most_frequent_words = get_most_frequent_words(df_col)\n",
    "    print(f\"(Removing words that are not among {len(most_frequent_words)} most frequent ones.)\\n\")\n",
    "    return df_col.apply(lambda x: [token for token in x if token in most_frequent_words])\n",
    "\n",
    "def remove_named_entities(df_col):\n",
    "    print(\"Removing named entities...\\n\")\n",
    "    return df_col.apply(lambda x: [token for token in x if token not in ne_list])\n",
    "\n",
    "def preprocessing(df_col, *steps):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe column with text and applies preprocessing steps given \n",
    "    in and returns a string.\n",
    "    \n",
    "    Input:\n",
    "    - df (dataframe): The dataframe containing the text column.\n",
    "    - steps (functions): Multiple functions for preprocessing can be given in.\n",
    "    \n",
    "    Output:\n",
    "    - List with strings.\n",
    "    \"\"\"\n",
    "    # copying over the column for preprocessing\n",
    "    temp = df_col.copy()\n",
    "    for func in steps:\n",
    "        temp = func(temp)\n",
    "    return temp.apply(lambda x: \" \".join([token for token in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "068ff6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_words(df_col, min_words):\n",
    "#    \n",
    "#    most_frequent_words = get_most_frequent_words(df_col, min_words)\n",
    "#    print(f\"(Removing words that are not among {len(most_frequent_words)} most frequent ones.)\\n\")\n",
    "#    \n",
    "#    words_to_keep = [word for word in most_frequent_words if word not in ne_list]\n",
    "#    \n",
    "#    df_col =  df_col.apply(lambda x: [token for token in x if token in words_to_keep])\n",
    "#    return df_col.apply(lambda x: \" \".join([token for token in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "bf3688c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_greece[\"tok\"] = tokenization(df_greece[\"text_stm\"])\n",
    "\n",
    "#df_greece[\"text_frame_identification_001\"] = remove_words(df_greece[\"tok\"],0.001)\n",
    "#df_greece[\"text_frame_identification_0025\"] = remove_words(df_greece[\"tok\"],0.025)\n",
    "#df_greece[\"text_frame_identification_005\"] = remove_words(df_greece[\"tok\"],0.05)\n",
    "#df_greece[\"text_frame_identification_01\"] = remove_words(df_greece[\"tok\"],0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "73d28c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Removing unfrequent words...\n",
      "\n",
      "(Removing words that are not among 1251 most frequent ones.)\n",
      "\n",
      "Removing named entities...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_greece[\"text_frame_identification\"] = preprocessing(df_greece[\"frame_identification_corpus\"],\n",
    "                                                  tokenization,\n",
    "                                                  remove_unfrequent_words,\n",
    "                                                  remove_named_entities)\n",
    "\n",
    "most_frequent_words = get_most_frequent_words(df_greece[\"frame_identification_corpus\"].apply(lambda x: word_tokenize(x)))\n",
    "words_to_cluster_greece = [word for word in most_frequent_words if word not in ne_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1594e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Removing unfrequent words...\n",
      "\n",
      "(Removing words that are not among 649 most frequent ones.)\n",
      "\n",
      "Removing named entities...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tigray[\"text_frame_identification\"] = preprocessing(df_tigray[\"frame_identification_corpus\"],\n",
    "                                                  tokenization,\n",
    "                                                  remove_unfrequent_words,\n",
    "                                                  remove_named_entities)\n",
    "\n",
    "most_frequent_words = get_most_frequent_words(df_tigray[\"frame_identification_corpus\"].apply(lambda x: word_tokenize(x)))\n",
    "words_to_cluster_tigray = [word for word in most_frequent_words if word not in ne_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f21af54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Removing unfrequent words...\n",
      "\n",
      "(Removing words that are not among 1245 most frequent ones.)\n",
      "\n",
      "Removing named entities...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_channel[\"text_frame_identification\"] = preprocessing(df_channel[\"frame_identification_corpus\"],\n",
    "                                                  tokenization,\n",
    "                                                  remove_unfrequent_words,\n",
    "                                                  remove_named_entities)\n",
    "\n",
    "most_frequent_words = get_most_frequent_words(df_channel[\"frame_identification_corpus\"].apply(lambda x: word_tokenize(x)))\n",
    "words_to_cluster_channel = [word for word in most_frequent_words if word not in ne_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a111582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing tweets...\n",
      "\n",
      "Removing unfrequent words...\n",
      "\n",
      "(Removing words that are not among 486 most frequent ones.)\n",
      "\n",
      "Removing named entities...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rohingya[\"text_frame_identification\"] = preprocessing(df_rohingya[\"frame_identification_corpus\"],\n",
    "                                                  tokenization,\n",
    "                                                  remove_unfrequent_words,\n",
    "                                                  remove_named_entities)\n",
    "\n",
    "most_frequent_words = get_most_frequent_words(df_rohingya[\"frame_identification_corpus\"].apply(lambda x: word_tokenize(x)))\n",
    "words_to_cluster_rohingya = [word for word in most_frequent_words if word not in ne_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7b3553fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source', 'text', 'lang', 'id', 'created_at', 'author_id',\n",
       "       'retweet_count', 'reply_count', 'like_count', 'quote_count',\n",
       "       'withheld.scope', 'hashtags', 'mentions', 'annotations', 'text_clean',\n",
       "       'year', 'calendar_week', 'year_month', 'year_calendar_week', 'refugee',\n",
       "       'migrant', 'immigrant', 'asylum_seeker', 'other', 'date',\n",
       "       'text_coherent', 'retweet_count_sum', 'count', 'text_alphanum',\n",
       "       'text_stm', 'text_frame_identification', 'tok',\n",
       "       'text_frame_identification_0025', 'text_frame_identification_001',\n",
       "       'text_frame_identification_005', 'text_frame_identification_01'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_greece.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2347b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_greece_test = df_greece[[\"text\",\"parsing_corpus\",\"text_frame_identification\",\"date\",'text_frame_identification_0025', 'text_frame_identification_001',\n",
    "       'text_frame_identification_005', 'text_frame_identification_01']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "25825e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_greece_test.to_csv('C:\\\\Users\\\\jawo19ad\\\\Documents\\\\GitHub\\\\refugee_project\\\\Code/../../../..//Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_greece_fi_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "53f1b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_greece_frame = df_greece[[\"text\",\"parsing_corpus\",\"text_frame_identification\",\"date\"]]\n",
    "df_tigray_frame = df_tigray[[\"text\",\"parsing_corpus\",\"text_frame_identification\",\"date\"]]\n",
    "df_rohingya_frame = df_rohingya[[\"text\",\"parsing_corpus\",\"text_frame_identification\",\"date\"]]\n",
    "df_channel_frame = df_channel[[\"text\",\"parsing_corpus\",\"text_frame_identification\",\"date\"]]\n",
    "\n",
    "#df_greece_frame.to_csv(greece_url_fi)\n",
    "#df_tigray_frame.to_csv(tigray_url_fi)\n",
    "#df_rohingya_frame.to_csv(rohingya_url_fi)\n",
    "#df_channel_frame.to_csv(channel_url_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2f8ab",
   "metadata": {},
   "source": [
    "## Train BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1023618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def get_most_frequent_words(df_col):\n",
    "    \n",
    "    def clean_nes(tweet_words):\n",
    "        return \" \".join([word for word in tweet_words.split() if word not in ne_list])\n",
    "    #tokenized_cols = df_col.apply(lambda x: word_tokenize(x) if len(x)>0 else x)\n",
    "    df_col = df_col[~df_col.isnull()] \n",
    "    #df_col = df_col.apply(lambda x: print(x.split()) )\n",
    "    #df_col = df_col.apply(clean_nes)\n",
    "    #print(df_col)\n",
    "    counter = Counter(\" \".join(df_col).split())\n",
    "    #print(counter.most_common(200))\n",
    "    words = [pair[0] for pair in counter.most_common(250)]\n",
    "\n",
    "    words = [word for word in words if word not in removed_words]\n",
    "\n",
    "    return words # list(set(words).difference(set(removed_words)))\n",
    "\n",
    "print('aegean' not in removed_words)\n",
    "def embed_words(words):\n",
    "    from time import time\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    #sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    sbert_model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "    #bert_corpus = list(words['entity'])\n",
    "\n",
    "    print(f'there are {len(words)} entities to be encoded')\n",
    "    t0 = time()\n",
    "    word_embeddings = sbert_model.encode(words)\n",
    "    print(f'Training embeddings took {time()-t0} seconds')\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4d569dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_list1 = str.split(\"abiy addis aegean afewerki afeworki afghan afghanistan africa african ahval aleppo  america \\\n",
    "                     amnesty ahmed american amhara andrew ankara antony arab arabia asean asia assad athens aung \\\n",
    "                     austria axum bangladesh bangladeshi biden blinken boris borisjohnson borrell brexit brit britain \\\n",
    "                     british brussels bulgaria bulgarian burma burmese calais canada \\\n",
    "                     channel bengal china chios commission commissioner corona coronavirus council coup covid cox \\\n",
    "                     cyprus delhi dover dublin edirne egypt england english erdoan erdogan erdogans  eritrea eritrean \\\n",
    "                     ethiopia ethiopian euro europe european evros farage filippo finland fontelles france french \\\n",
    "                     freya_cole frontex garneau geneva german germany grandi greece greek guterres haavisto hamdayet \\\n",
    "                     harris hindu hitsats hlaing houthi houthis hungary idlib idp india indian iran iranian iraq \\\n",
    "                     iraqi ireland irish isaias isayas isi israel istanbul italy italian jammu japan jazeera jerry \\\n",
    "                     johnson jordan josep junta justin kachin kadra karen kayin kamala kashmir kastanies kenneth kent \\\n",
    "                     kenya kurd kurdish labour leyen linda lindat_g lebanon lesbos lesvos libya london macron mahlakon \\\n",
    "                     maikadra malaysia manipur marc matthew mediterranean merkel michelle mitsotakis mizoram modi moria \\\n",
    "                     muslim mutraw myanmar nato nazi nigel november obama oromia oromo pakistan pakistani palestine \\\n",
    "                     palestinian patel pazarkule poland president priti putin qaeda rakhine recep reuters rohingya \\\n",
    "                     rohingyas roth rterdogan russia russian samri saudi scotland secretary shimelba shire somalia \\\n",
    "                     soros spain state sudan sudanese sweden syria syrian tayyip tegaru tigrai tigray tigrayan \\\n",
    "                     tigrayans thai thailand tory tostevin tplf trudeau trump turk turkey turkish twitter unhcr unicef \\\n",
    "                     union unsc us yangon yemen york youtube\",\" \")\n",
    "ne_list2 = str.split(\"attempt  blocked born bring brother build call called calling came cant case caught cause \\\n",
    "                     centre change christian claim claiming clear clearly close come coming comment commited \\\n",
    "                     commiting committed committing completely condition confirmed  continue continues could \\\n",
    "                     country course cover created credible  currently daily day deal dear decade decision demand \\\n",
    "                     department despite didnt difference different dire displaced doesnt done dont dy east eastern \\\n",
    "                     easy either effort else endf enough especially ethnic even ever every everyone everything \\\n",
    "                      evil exactly expect extremely face facility facing fact failed fake false federal \\\n",
    "                     feel find first found four forget free fuck fucking full genuine getting given giving great \\\n",
    "                     ground group happy imagine including issue instead isnt give going good half hand happen \\\n",
    "                     happened happening happeningln hard head heading held high horn hour however huge idea image \\\n",
    "                     immediate immediately independent information inside internal internally international interview  \\\n",
    "                     investigation islam islamic issue join journalist keep kind know known lack land landing largest \\\n",
    "                     last latest le  least left let letting level lie like likely little live living load local \\\n",
    "                     long longer look looking lost made mail major majority make making many  matter maybe \\\n",
    "                     mean meanwhile medium member middle might migration mind month morning mostly move much must \\\n",
    "                     name national near nearly need needed neighboring never news next nobody north northern nothing \\\n",
    "                     obviously office official one ongoing operation others paid part party pas past people perhaps \\\n",
    "                     person place plan please point post press prevails prime probably  process programme \\\n",
    "                     provide  push putting question quite rather reach read real realise reality really reason \\\n",
    "                     received recent record reqion remains remember report reported reporting resident response \\\n",
    "                      rest result right said satellite say saying second seek seeker seeking seem seems \\\n",
    "                     seen send sending sent series service several shame share shit show side simple simply since \\\n",
    "                     single site situation small someone something soon sorry sort south source speak  spread \\\n",
    "                     stand star start started statement street still stop stopped stopping story  sure surely \\\n",
    "                      take taken taking talk talking tell term testimony thank thanks thats there theyre thing \\\n",
    "                     think though thought three time today told took  towards town tried true truth trying turn \\\n",
    "                     tweet understand unless  used using video vice view virus visit voice wait waiting \\\n",
    "                     want wanted watch water week  well west western  within without whats whilst white \\\n",
    "                     whole wing wish wonder wont word world  wrong would year yesterday young youre\",\" \")\n",
    "part_of_ne = str.split(\" agency asylum bazar seeker united nation nations nationshumanrights \")\n",
    "ambiguous = str.split(\" access according account action  agreement  allowed allowing arriving back become \\\n",
    "                    believe best better black breaking came closed caused come coming cross deal do done enter entering \\\n",
    "                    event fear find full get getting give given go going gone good got hate hope keep large last leave leaving left love \\\n",
    "                      made make making march matter massive mean medium need open opened part point return \\\n",
    "                      returned right said say \\\n",
    "                      saying seek send show special start stop sure surely take taken taking tear torn tell thing think \\\n",
    "                      time trying turn urgent urgently withdraw\")\n",
    "temp_num = str.split(\" always april  daily first  mass  monday never night number  today \\\n",
    "                     week year \")\n",
    "non_pos = str.split(\"  able absolutely across actually almost along alone already also amid among another anti anymore anyone \\\n",
    "                    anything arent around away based cant dont enough every le majority maybe \\\n",
    "                    many much nothing \\\n",
    "                    rather really something still thats theyre without\")\n",
    "\n",
    "non_framing = str.split('agree allow ambassador answer area arrived article basic beach blame boat border camp \\\n",
    "                        center choose\\\n",
    "                        coast country city convention crisis dinghy create culture district food force \\\n",
    "                        forest hold hotel history idiot illegally immigration internet island language leader \\\n",
    "                        leaf life migration minister navy news order photo population power problem public \\\n",
    "                        pushing rapporteur region responsible responsibility river route royal save ship shore solution \\\n",
    "                        status stay stupid system township village water white worse zone')\n",
    "tigray = str.split('organization mission campaign targeted corridor missing')\n",
    "ne_list = part_of_ne + ambiguous + temp_num + non_pos + ne_list2  + non_framing + tigray#+ ne_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "12fc7c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_list = 'abiy africa ahmed amhara amnesty andrew antony asean assad axum bangladesh bangladeshi biden blinken boris borisjohnson brexit britain british bulgaria burma burmese calais channel china commission coronavirus council coup covid cox dover dublin england english erdogan eritrea eritrean ethiopia ethiopian europe european evros farage filippo france french freya_cole germany grandi greece greek guterres harris hindu hitsats hlaing idlib idp india indian iran isaias jammu johnson junta kachin kadra kamala karen kent labour lesbos maikadra manipur mizoram muslim myanmar nato nigel pakistan patel president priti putin rohingya rohingyas russia russian secretary shimelba shire state sudan sudanese syria syrian thai thailand tigrai tigray tigrayan tigrayans tory tplf trudeau turk turkey turkish unhcr union unsc yangon yemen'.split(\" \")\n",
    "\n",
    "non_framing = 'minority community villager access according across action actually agency allow already always another anyone april area around arriving asylum away back bazar believe better blame border call called came came campaign cant cant case centre change city claim close closed coast come come coming coming committed continue country country crisis cross daily daily day deal deal dear displaced district done done dont dont east enough enough enter ethnic every every face fact fake find find first first force free full full genuine getting getting give give given given going going good good great group happening happeningln hate head hope huge illegally  including internally international internet island issue issue keep keep land last last le le leader least leave leaving left left life live living local long longer look lost made made majority majority make make making making many many march mass massive maybe maybe mean mean medium medium member middle missing mission month morning move much much nation navy need need never never news news night northern nothing nothing number office official ongoing open opened order part part people photo place plan please point point population problem provide public push rapporteur rather rather reach read real really really reason region report reported return right right said said save say say saying saying seek seek seeker seeker send send sending sent service shame shore show show side situation small something something source south special stand start stay still still stop stop story sure sure system taking taking talk targeted tear tell tell thank thats thats theyre theyre thing thing think think time time today today told towards township truth trying trying turn turn united urgent urgently used using vice video village virus voice wait want watch water water week week west western whats white white without without world wrong year year yesterday zone'.split(\" \")\n",
    "\n",
    "search_terms = str.split(\"refugee refugees migrant migrants immigrant immigrants asylum seeker seekers displaced stateless people person persons\",\" \")\n",
    "\n",
    "added_today = str.split(\"fire conflict foreign condition camp boat dinghy wave flood citizen\",\" \")\n",
    "\n",
    "channel_words = str.split(\"feel agreement agree desperate route anti dangerous little someone young seeking word party beach next anything ever instead youre become allowed convention street apply matter taken thought understand true accept might\", \" \")\n",
    "\n",
    "tigray_words = str.split(\"axum organization unity independent lie happened hand based high others tigray evidence town witness investigation\", \" \")\n",
    "\n",
    "rohingya_words = str.split(\"blaze leaf forest ground staff coup yangon supreme federal modi broke affected blackout caused held event pray praying needed strike hlaing near several within large mahlakon largest\",\" \")\n",
    "\n",
    "removed_words = list(set(ne_list1 + non_framing + search_terms + added_today + channel_words + tigray_words + rohingya_words))\n",
    "\n",
    "len(removed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f96e7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 137462 tweets!\n",
      "loaded 173758 tweets!\n",
      "loaded 42853 tweets!\n",
      "loaded 29432 tweets!\n"
     ]
    }
   ],
   "source": [
    "df_greece = read_event_df('greece')\n",
    "df_channel = read_event_df('channel')\n",
    "df_tigray = read_event_df('tigray')\n",
    "df_rohingya = read_event_df('rohingya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "a863529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words_greece = get_most_frequent_words(df_greece[\"frame_identification_corpus\"])\n",
    "most_frequent_words_channel = get_most_frequent_words(df_channel[\"frame_identification_corpus\"])\n",
    "most_frequent_words_tigray = get_most_frequent_words(df_tigray[\"frame_identification_corpus\"])\n",
    "most_frequent_words_rohingya = get_most_frequent_words(df_rohingya[\"frame_identification_corpus\"])\n",
    "most_frequent_words = list(set(most_frequent_words_greece+most_frequent_words_channel+most_frequent_words_tigray+most_frequent_words_rohingya))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "6def634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frames(embedding, preference, most_frequent_words):\n",
    "    cluster = AffinityPropagation(preference = preference,  random_state=42).fit(embedding)\n",
    "    cluster_labels = cluster.labels_\n",
    "    \n",
    "    labeled_tweet = pd.DataFrame({'word': most_frequent_words,'label':cluster_labels})\n",
    "    \n",
    "    # Create documents per label\n",
    "    docs_per_class = labeled_tweet.groupby(['label'], as_index=False).agg({'word': ' '.join})\n",
    "\n",
    "    words_per_class = dict()\n",
    "    for label,word in zip(docs_per_class['label'],docs_per_class['word']):\n",
    "        words_per_class[label] = word.split(' ')\n",
    "    return (pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in words_per_class.items() ])).fillna('.').head(50),words_per_class)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b6c71",
   "metadata": {},
   "source": [
    "### Greece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "714f2892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_frequent_words_greece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "266ee2b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 65 entities to be encoded\n",
      "Training embeddings took 0.5062229633331299 seconds\n"
     ]
    }
   ],
   "source": [
    "greece_embeddings = embed_words(most_frequent_words_greece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "da87b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.668034315109253 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "reduced_embedding_greece = umap.UMAP(random_state=42,\n",
    "                                     n_components=3,\n",
    "                                     min_dist=0.1,\n",
    "                                     n_neighbors = 5).fit_transform(greece_embeddings)\n",
    "print(f'Duration: {time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "a9b9151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "greece_frame_df, greece_frame_dict = create_frames(reduced_embedding_greece,-4,most_frequent_words_greece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d01e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "humanitarian\n",
    "reception\n",
    "migration\n",
    "security\n",
    "violence\n",
    "political\n",
    "economic\n",
    "military\n",
    "criminal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "93a81767",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file(\"greece_frame_devices\",greece_frame_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0319cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "securitisation\n",
    "criminality\n",
    "threat/violence\n",
    "political\n",
    "economic\n",
    "humanitarian\n",
    "militarisation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b688550",
   "metadata": {},
   "source": [
    "### Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "35ee4546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_frequent_words_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "e8e47003",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 51 entities to be encoded\n",
      "Training embeddings took 0.4698755741119385 seconds\n"
     ]
    }
   ],
   "source": [
    "channel_embeddings = embed_words(most_frequent_words_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "f501ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.7096350193023682 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "reduced_embedding_channel = umap.UMAP(random_state=42,\n",
    "                                     n_components=3,\n",
    "                                     min_dist=0.1,\n",
    "                                     n_neighbors = 10).fit_transform(channel_embeddings)\n",
    "print(f'Duration: {time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "22ef80b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>home</td>\n",
       "      <td>economic</td>\n",
       "      <td>illegal</td>\n",
       "      <td>safe</td>\n",
       "      <td>crossing</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legal</td>\n",
       "      <td>hotel</td>\n",
       "      <td>money</td>\n",
       "      <td>racist</td>\n",
       "      <td>help</td>\n",
       "      <td>immigration</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>voted</td>\n",
       "      <td>house</td>\n",
       "      <td>million</td>\n",
       "      <td>criminal</td>\n",
       "      <td>benefit</td>\n",
       "      <td>fleeing</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>policy</td>\n",
       "      <td>welcome</td>\n",
       "      <td>thousand</td>\n",
       "      <td>police</td>\n",
       "      <td>support</td>\n",
       "      <td>invasion</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rule</td>\n",
       "      <td>housing</td>\n",
       "      <td>poor</td>\n",
       "      <td>illegals</td>\n",
       "      <td>control</td>\n",
       "      <td>migration</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>govt</td>\n",
       "      <td>homeless</td>\n",
       "      <td>paying</td>\n",
       "      <td>crime</td>\n",
       "      <td>care</td>\n",
       "      <td>entering</td>\n",
       "      <td>working</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vote</td>\n",
       "      <td>.</td>\n",
       "      <td>cost</td>\n",
       "      <td>.</td>\n",
       "      <td>death</td>\n",
       "      <td>arrive</td>\n",
       "      <td>job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>law</td>\n",
       "      <td>.</td>\n",
       "      <td>paid</td>\n",
       "      <td>.</td>\n",
       "      <td>risk</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>taxpayer</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3        4            5        6\n",
       "0  government      home  economic   illegal     safe     crossing    child\n",
       "1       legal     hotel     money    racist     help  immigration     work\n",
       "2       voted     house   million  criminal  benefit      fleeing   family\n",
       "3      policy   welcome  thousand    police  support     invasion    human\n",
       "4        rule   housing      poor  illegals  control    migration    woman\n",
       "5        govt  homeless    paying     crime     care     entering  working\n",
       "6        vote         .      cost         .    death       arrive      job\n",
       "7         law         .      paid         .     risk            .        .\n",
       "8    taxpayer         .         .         .        .            .        ."
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_frames(reduced_embedding_channel,-2,most_frequent_words_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "60e899db",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_frame_df, channel_frame_dict = create_frames(reduced_embedding_channel,-2,most_frequent_words_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "b45763da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file(\"channel_frame_devices\",channel_frame_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c310809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "political\n",
    "accomodation\n",
    "economisation\n",
    "criminality\n",
    "support\n",
    "migration\n",
    "integration\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c7908",
   "metadata": {},
   "source": [
    "### Tigray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "5a612040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_frequent_words_tigray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "c35ff1af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 76 entities to be encoded\n",
      "Training embeddings took 0.8544445037841797 seconds\n"
     ]
    }
   ],
   "source": [
    "tigray_embeddings = embed_words(most_frequent_words_tigray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "634b02ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.4146029949188232 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "reduced_embedding_tigray = umap.UMAP(random_state=42,\n",
    "                                     n_components=3,\n",
    "                                     min_dist=0.1,\n",
    "                                     n_neighbors = 5).fit_transform(tigray_embeddings)\n",
    "print(f'Duration: {time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "eaf6c47f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>million</td>\n",
       "      <td>fled</td>\n",
       "      <td>food</td>\n",
       "      <td>civilian</td>\n",
       "      <td>crime</td>\n",
       "      <td>genocide</td>\n",
       "      <td>innocent</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thousand</td>\n",
       "      <td>fleeing</td>\n",
       "      <td>starvation</td>\n",
       "      <td>militia</td>\n",
       "      <td>rape</td>\n",
       "      <td>killed</td>\n",
       "      <td>protect</td>\n",
       "      <td>humanitarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hundred</td>\n",
       "      <td>flee</td>\n",
       "      <td>starving</td>\n",
       "      <td>government</td>\n",
       "      <td>violence</td>\n",
       "      <td>killing</td>\n",
       "      <td>care</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>famine</td>\n",
       "      <td>troop</td>\n",
       "      <td>raped</td>\n",
       "      <td>massacre</td>\n",
       "      <td>peace</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>hunger</td>\n",
       "      <td>soldier</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>safe</td>\n",
       "      <td>child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>army</td>\n",
       "      <td>criminal</td>\n",
       "      <td>looting</td>\n",
       "      <td>safety</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>military</td>\n",
       "      <td>forced</td>\n",
       "      <td>death</td>\n",
       "      <td>health</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>govt</td>\n",
       "      <td>atrocity</td>\n",
       "      <td>massacred</td>\n",
       "      <td>protection</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>dictator</td>\n",
       "      <td>attack</td>\n",
       "      <td>looted</td>\n",
       "      <td>security</td>\n",
       "      <td>assistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>victim</td>\n",
       "      <td>destruction</td>\n",
       "      <td>.</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>fighting</td>\n",
       "      <td>kill</td>\n",
       "      <td>.</td>\n",
       "      <td>humanity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>justice</td>\n",
       "      <td>burned</td>\n",
       "      <td>.</td>\n",
       "      <td>girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>suffering</td>\n",
       "      <td>cleansing</td>\n",
       "      <td>.</td>\n",
       "      <td>youth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>violation</td>\n",
       "      <td>dying</td>\n",
       "      <td>.</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>raping</td>\n",
       "      <td>dead</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>sexual</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>abuse</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>attacked</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0        1           2           3          4            5  \\\n",
       "0    million     fled        food    civilian      crime     genocide   \n",
       "1   thousand  fleeing  starvation     militia       rape       killed   \n",
       "2    hundred     flee    starving  government   violence      killing   \n",
       "3          .        .      famine       troop      raped     massacre   \n",
       "4          .        .      hunger     soldier  terrorist    destroyed   \n",
       "5          .        .           .        army   criminal      looting   \n",
       "6          .        .           .    military     forced        death   \n",
       "7          .        .           .        govt   atrocity    massacred   \n",
       "8          .        .           .    dictator     attack       looted   \n",
       "9          .        .           .           .     victim  destruction   \n",
       "10         .        .           .           .   fighting         kill   \n",
       "11         .        .           .           .    justice       burned   \n",
       "12         .        .           .           .  suffering    cleansing   \n",
       "13         .        .           .           .  violation        dying   \n",
       "14         .        .           .           .     raping         dead   \n",
       "15         .        .           .           .     sexual            .   \n",
       "16         .        .           .           .      abuse            .   \n",
       "17         .        .           .           .   attacked            .   \n",
       "\n",
       "             6             7  \n",
       "0     innocent         human  \n",
       "1      protect  humanitarian  \n",
       "2         care         woman  \n",
       "3        peace          help  \n",
       "4         safe         child  \n",
       "5       safety          home  \n",
       "6       health       support  \n",
       "7   protection        family  \n",
       "8     security    assistance  \n",
       "9            .          work  \n",
       "10           .      humanity  \n",
       "11           .          girl  \n",
       "12           .         youth  \n",
       "13           .         house  \n",
       "14           .             .  \n",
       "15           .             .  \n",
       "16           .             .  \n",
       "17           .             .  "
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_frames(reduced_embedding_tigray,-10,most_frequent_words_tigray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "b9e2ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tigray_frame_df, tigray_frame_dict = create_frames(reduced_embedding_tigray,-10,most_frequent_words_tigray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "4f0505fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file(\"tigray_frame_devices\",tigray_frame_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "misc\n",
    "misc\n",
    "vulnerabilty\n",
    "political/military\n",
    "criminality\n",
    "violence\n",
    "support/protection (maybe even outreach)\n",
    "humanitarian\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4f71a",
   "metadata": {},
   "source": [
    "### Rohingya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "5531d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_frequent_words_rohingya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "3d070162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 79 entities to be encoded\n",
      "Training embeddings took 0.8724038600921631 seconds\n"
     ]
    }
   ],
   "source": [
    "rohingya_embeddings = embed_words(most_frequent_words_rohingya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "60d0c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.4543967247009277 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "reduced_embedding_rohingya = umap.UMAP(random_state=42,\n",
    "                                     n_components=3,\n",
    "                                     min_dist=0.1,\n",
    "                                     n_neighbors = 10).fit_transform(rohingya_embeddings)\n",
    "print(f'Duration: {time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "b435033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>military</td>\n",
       "      <td>child</td>\n",
       "      <td>killed</td>\n",
       "      <td>government</td>\n",
       "      <td>fleeing</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>thousand</td>\n",
       "      <td>shelter</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>army</td>\n",
       "      <td>human</td>\n",
       "      <td>killing</td>\n",
       "      <td>illegal</td>\n",
       "      <td>fled</td>\n",
       "      <td>airstrikes</td>\n",
       "      <td>hundred</td>\n",
       "      <td>food</td>\n",
       "      <td>humanitarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>civilian</td>\n",
       "      <td>home</td>\n",
       "      <td>dead</td>\n",
       "      <td>authority</td>\n",
       "      <td>flee</td>\n",
       "      <td>attack</td>\n",
       "      <td>million</td>\n",
       "      <td>safe</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soldier</td>\n",
       "      <td>family</td>\n",
       "      <td>massacre</td>\n",
       "      <td>democracy</td>\n",
       "      <td>deportation</td>\n",
       "      <td>police</td>\n",
       "      <td>.</td>\n",
       "      <td>safety</td>\n",
       "      <td>supply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brigade</td>\n",
       "      <td>house</td>\n",
       "      <td>genocide</td>\n",
       "      <td>forced</td>\n",
       "      <td>.</td>\n",
       "      <td>violence</td>\n",
       "      <td>.</td>\n",
       "      <td>injured</td>\n",
       "      <td>donation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fighter</td>\n",
       "      <td>worker</td>\n",
       "      <td>death</td>\n",
       "      <td>govt</td>\n",
       "      <td>.</td>\n",
       "      <td>crackdown</td>\n",
       "      <td>.</td>\n",
       "      <td>suppression</td>\n",
       "      <td>supporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>armed</td>\n",
       "      <td>woman</td>\n",
       "      <td>devastating</td>\n",
       "      <td>court</td>\n",
       "      <td>.</td>\n",
       "      <td>crime</td>\n",
       "      <td>.</td>\n",
       "      <td>security</td>\n",
       "      <td>emergency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jet</td>\n",
       "      <td>humanity</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>detained</td>\n",
       "      <td>.</td>\n",
       "      <td>attacked</td>\n",
       "      <td>.</td>\n",
       "      <td>protect</td>\n",
       "      <td>assistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>homeless</td>\n",
       "      <td>burnt</td>\n",
       "      <td>forcing</td>\n",
       "      <td>.</td>\n",
       "      <td>besieging</td>\n",
       "      <td>.</td>\n",
       "      <td>health</td>\n",
       "      <td>donate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>work</td>\n",
       "      <td>died</td>\n",
       "      <td>envoy</td>\n",
       "      <td>.</td>\n",
       "      <td>bombing</td>\n",
       "      <td>.</td>\n",
       "      <td>innocent</td>\n",
       "      <td>aid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>kill</td>\n",
       "      <td>national</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>burned</td>\n",
       "      <td>civil</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1            2           3            4           5  \\\n",
       "0   military     child       killed  government      fleeing   terrorist   \n",
       "1       army     human      killing     illegal         fled  airstrikes   \n",
       "2   civilian      home         dead   authority         flee      attack   \n",
       "3    soldier    family     massacre   democracy  deportation      police   \n",
       "4    brigade     house     genocide      forced            .    violence   \n",
       "5    fighter    worker        death        govt            .   crackdown   \n",
       "6      armed     woman  devastating       court            .       crime   \n",
       "7        jet  humanity    destroyed    detained            .    attacked   \n",
       "8          .  homeless        burnt     forcing            .   besieging   \n",
       "9          .      work         died       envoy            .     bombing   \n",
       "10         .         .         kill    national            .           .   \n",
       "11         .         .       burned       civil            .           .   \n",
       "\n",
       "           6            7             8  \n",
       "0   thousand      shelter          help  \n",
       "1    hundred         food  humanitarian  \n",
       "2    million         safe       support  \n",
       "3          .       safety        supply  \n",
       "4          .      injured      donation  \n",
       "5          .  suppression    supporting  \n",
       "6          .     security     emergency  \n",
       "7          .      protect    assistance  \n",
       "8          .       health        donate  \n",
       "9          .     innocent           aid  \n",
       "10         .            .             .  \n",
       "11         .            .             .  "
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_frames(reduced_embedding_rohingya,-4,most_frequent_words_rohingya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "cc4cff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rohingya_frame_df, rohingya_frame_dict = create_frames(reduced_embedding_rohingya,-4,most_frequent_words_rohingya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "2dc7b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file(\"rohingya_frame_devices\",rohingya_frame_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f096bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "military\n",
    "protection\n",
    "violence\n",
    "political\n",
    "misc\n",
    "violence (merge with other)\n",
    "misc\n",
    "protection (merge with other)\n",
    "humanitarian\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cac09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
