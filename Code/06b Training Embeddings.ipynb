{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings training\n",
    "\n",
    "**Required files:**\n",
    " - event_df_clean = event specific dataframe with preprocessed text, use column 'text_coherent' for training\n",
    " - event_cands_merged = dataframe of candidates that are merged after 1st step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages for the following parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "#import gensim\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# self written modules\n",
    "#import preprocessing\n",
    "\n",
    "# storing python objects in the desired locations using pickle\n",
    "import pickle\n",
    "\n",
    "def pickle_file(file_name, file_to_dump):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path +  fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, 'wb') as fp:\n",
    "        pickle.dump(file_to_dump, fp)\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    directory_path = os.getcwd() + \"/../../../../\"\n",
    "    folder_name = file_name.split('_')[0]\n",
    "    file_path = directory_path + fr\"Dropbox (CBS)/Master thesis data/Candidate Data/{folder_name}/{file_name}\"\n",
    "    with open(file_path, \"rb\") as input_file:\n",
    "        return pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tigray_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_tigray_clean.csv\" # location of Tigray dataset\n",
    "greece_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_greece_clean.csv\" # location of Greece dataset\n",
    "rohingya_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_rohingya_clean.csv\" # location of Rohingya dataset\n",
    "channel_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/Clean/df_channel_clean.csv\" # location of Channel dataset\n",
    "\n",
    "def read_event_df(data_url):\n",
    "    directory_path = os.getcwd() + \"/../../../../\" + data_url \n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "#event_df1 = read_event_df(tigray_url)\n",
    "#event_df2 = read_event_df(rohingya_url)\n",
    "#event_df3 = read_event_df(channel_url)\n",
    "#event_df4 = read_event_df(greece_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets at the start: 42853\n",
      "Tweets after 100% duplicates removed: 42164\n",
      "calculating similarities across documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "955it [00:00, 9455.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity calculation completed in 149.39532446861267 seconds\n",
      "removing fuzzy duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112872it [00:04, 26938.80it/s]\n",
      "C:\\Users\\nikodemicek\\Documents\\GitHub\\refugee_project\\Code\\preprocessing.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dupl_removed['is_dup'][i] = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32831 tweets left after 70.0% similar tweets (by cosine similarity) removed\n",
      "Tweets at the start: 29432\n",
      "Tweets after 100% duplicates removed: 28820\n",
      "calculating similarities across documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5806it [00:00, 29353.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity calculation completed in 59.32112240791321 seconds\n",
      "removing fuzzy duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90075it [00:02, 34711.69it/s]\n",
      "C:\\Users\\nikodemicek\\Documents\\GitHub\\refugee_project\\Code\\preprocessing.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dupl_removed['is_dup'][i] = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21154 tweets left after 70.0% similar tweets (by cosine similarity) removed\n",
      "Tweets at the start: 173758\n",
      "Tweets after 100% duplicates removed: 173339\n",
      "calculating similarities across documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2947it [00:00, 29283.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity calculation completed in 2959.0516135692596 seconds\n",
      "removing fuzzy duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240693it [00:06, 37092.65it/s]\n",
      "C:\\Users\\nikodemicek\\Documents\\GitHub\\refugee_project\\Code\\preprocessing.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dupl_removed['is_dup'][i] = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162413 tweets left after 70.0% similar tweets (by cosine similarity) removed\n",
      "Tweets at the start: 137462\n",
      "Tweets after 100% duplicates removed: 135891\n",
      "calculating similarities across documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2959it [00:00, 29297.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity calculation completed in 1541.3745939731598 seconds\n",
      "removing fuzzy duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "248357it [00:08, 29633.15it/s]\n",
      "C:\\Users\\nikodemicek\\Documents\\GitHub\\refugee_project\\Code\\preprocessing.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dupl_removed['is_dup'][i] = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116533 tweets left after 70.0% similar tweets (by cosine similarity) removed\n"
     ]
    }
   ],
   "source": [
    "unique_tweets_df1 = preprocessing.fuzzy_duplicate_removal(event_df1)\n",
    "unique_tweets_df2= preprocessing.fuzzy_duplicate_removal(event_df2)\n",
    "unique_tweets_df3 = preprocessing.fuzzy_duplicate_removal(event_df3)\n",
    "unique_tweets_df4 = preprocessing.fuzzy_duplicate_removal(event_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_embeddings(event_cands):\n",
    "    from time import time\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    #sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    sbert_model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "    bert_corpus = list(event_cands['entity'])\n",
    "\n",
    "    print(f'there are {len(bert_corpus)} entities to be encoded')\n",
    "    t0 = time()\n",
    "    cands_embeddings = sbert_model.encode(bert_corpus)\n",
    "    print(f'Training embeddings took {time()-t0} seconds')\n",
    "    return cands_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2076 sentences to be encoded\n",
      "Training embeddings took 46.14393377304077 seconds\n"
     ]
    }
   ],
   "source": [
    "#event_df = read_event_df(greece_url)\n",
    "#unique_tweets_df = preprocessing.fuzzy_duplicate_removal(event_df)\n",
    "#tweet_sentences = [sent for tweet in event_df['text_alphanum'] for sent in sent_tokenize(tweet)]\n",
    "event_cands = load_pickle('greece_ents')\n",
    "\n",
    "document_embeddings = train_embeddings(event_cands)\n",
    "\n",
    "pickle_file('greece_embeddings_ents', document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 939 sentences to be encoded\n",
      "Training embeddings took 27.10666036605835 seconds\n"
     ]
    }
   ],
   "source": [
    "#event_df = read_event_df(rohingya_url)\n",
    "#unique_tweets_df = preprocessing.fuzzy_duplicate_removal(event_df)\n",
    "#tweet_sentences = [sent for tweet in event_df['text_alphanum'] for sent in sent_tokenize(tweet)]\n",
    "event_cands = load_pickle('rohingya_ents')\n",
    "\n",
    "document_embeddings = train_embeddings( event_cands)\n",
    "\n",
    "pickle_file('rohingya_embeddings_ents', document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1125 sentences to be encoded\n",
      "Training embeddings took 36.669280767440796 seconds\n"
     ]
    }
   ],
   "source": [
    "#event_df = read_event_df(tigray_url)\n",
    "#unique_tweets_df = preprocessing.fuzzy_duplicate_removal(event_df)\n",
    "#tweet_sentences = [sent for tweet in event_df['text_alphanum'] for sent in sent_tokenize(tweet)]\n",
    "event_cands = load_pickle('tigray_ents')\n",
    "\n",
    "document_embeddings = train_embeddings(event_cands)\n",
    "\n",
    "pickle_file('tigray_embeddings_ents', document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2679 entities to be encoded\n",
      "Training embeddings took 45.38644218444824 seconds\n"
     ]
    }
   ],
   "source": [
    "#event_df = read_event_df(channel_url)\n",
    "#unique_tweets_df = preprocessing.fuzzy_duplicate_removal(event_df)\n",
    "#tweet_sentences = [sent for tweet in event_df['text_alphanum'] for sent in sent_tokenize(tweet)]\n",
    "event_cands = load_pickle('channel_ents')\n",
    "\n",
    "document_embeddings = train_embeddings(event_cands)\n",
    "\n",
    "pickle_file('channel_embeddings_ents', document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe with similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def create_sim_df(cand_df,cand_embeddings,sim_threshold=0.6):\n",
    "    \n",
    "    rows_list = []\n",
    "    sims = cosine_similarity(cand_embeddings)\n",
    "    \n",
    "    for up_cand_id in tqdm(range(len(cand_df['entity']))):\n",
    "        for low_cand_id in range(up_cand_id+1,len(cand_df)):\n",
    "            dict1 = {}\n",
    "            # get input row in dictionary format\n",
    "            # key = col_name\n",
    "            if sims[up_cand_id][low_cand_id]>sim_threshold:\n",
    "                dict1.update({'text': cand_df['entity'][up_cand_id], \n",
    "                              'text_to_compare':cand_df['entity'][low_cand_id], \n",
    "                              'sim':sims[up_cand_id][low_cand_id]}) \n",
    "                rows_list.append(dict1)\n",
    "\n",
    "    sim_df = pd.DataFrame(rows_list)\n",
    "    return sim_df\n",
    "\n",
    "def create_sim_df(event_name, entity_type = 'anns', sim_threshold=0.6):\n",
    "    event_entities = load_pickle(f'{event_name}_{entity_type}')\n",
    "    cand_embeddings = load_pickle(f'{event_name}_embeddings_{entity_type}')\n",
    "    rows_list = []\n",
    "    sims = cosine_similarity(cand_embeddings)\n",
    "    \n",
    "    for up_cand_id in tqdm(range(len(event_entities['entity']))):\n",
    "        for low_cand_id in range(up_cand_id+1,len(event_entities)):\n",
    "            dict1 = {}\n",
    "            # get input row in dictionary format\n",
    "            # key = col_name\n",
    "            if sims[up_cand_id][low_cand_id]>sim_threshold:\n",
    "                dict1.update({'text': event_entities['entity'][up_cand_id], \n",
    "                              'text_to_compare':event_entities['entity'][low_cand_id], \n",
    "                              'sim':sims[up_cand_id][low_cand_id]}) \n",
    "                rows_list.append(dict1)\n",
    "\n",
    "    sim_df = pd.DataFrame(rows_list)\n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5120/5120 [01:51<00:00, 45.76it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_df = create_sim_df('rohingya')\n",
    "pickle_file('rohingya_sim_df_anns',sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9470/9470 [05:26<00:00, 29.02it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_df = create_sim_df('rohingya',entity_type='ents')\n",
    "pickle_file('rohingya_sim_df_ents',sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = load_pickle('greece_sim_df')\n",
    "event_cands = load_pickle('greece_cands_after1')\n",
    "\n",
    "sim_df_anns = load_pickle('greece_sim_df_anns')\n",
    "event_cands_anns = load_pickle('greece_anns')\n",
    "\n",
    "sim_df_ents = load_pickle('greece_sim_df_ents')\n",
    "event_cands_ents = load_pickle('greece_ents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim_df_freq(sim_df, event_cands):\n",
    "    #get frequency of of text column\n",
    "    sim_df_freq_ = sim_df.merge(event_cands[['entity','freq']],left_on='text',right_on='entity')\n",
    "    sim_df_freq_ = sim_df_freq_[sim_df_freq_.freq>5]\n",
    "    #get frequency of of text_to_compare columns\n",
    "    sim_df_freq = sim_df_freq_.merge(event_cands[['entity','freq']],left_on='text_to_compare',right_on='entity')\n",
    "    sim_df_freq = sim_df_freq[sim_df_freq.freq_y>5]\n",
    "    \n",
    "    sim_df_freq.drop(['entity_x','entity_y'],axis=1,inplace=True)\n",
    "    return sim_df_freq\n",
    "\n",
    "sim_df_freq_ents = make_sim_df_freq(sim_df_ents,event_cands_ents)\n",
    "sim_df_freq_anns = make_sim_df_freq(sim_df_anns,event_cands_anns)\n",
    "\n",
    "\n",
    "def make_sim_df_freq(sim_df, event_cands):\n",
    "    #get frequency of of text column\n",
    "    sim_df_freq_ = sim_df.merge(event_cands[['cand_text','cand_freq']],left_on='text',right_on='cand_text')\n",
    "    sim_df_freq_ = sim_df_freq_[sim_df_freq_.cand_freq>5]\n",
    "    #get frequency of of text_to_compare columns\n",
    "    sim_df_freq = sim_df_freq_.merge(event_cands[['cand_text','cand_freq']],left_on='text_to_compare',right_on='cand_text')\n",
    "    sim_df_freq = sim_df_freq[sim_df_freq.cand_freq_y>5]\n",
    "    \n",
    "    sim_df_freq.drop(['cand_text_x','cand_text_y'],axis=1,inplace=True)\n",
    "    return sim_df_freq\n",
    "\n",
    "sim_df_freq = make_sim_df_freq(sim_df,event_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['turkey', 'greece', 'europe', 'eu', 'syria', 'erdogan', 'idlib',\n",
       "       'russia', 'uk', 'assad', 'germany', 'nato', 'turkish', 'us',\n",
       "       'lesbos', 'syrians', 'syrian', 'iran', 'putin', 'bulgaria'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df_freq_anns.sort_values('cand_freq_x',ascending=False)['text'].unique()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(sim_df, entity,sim=0.7):\n",
    "    merged = list(sim_df[(sim_df['sim']>sim) & (sim_df['text']==entity)].sort_values('sim',ascending=False)['text_to_compare'])\n",
    "    #text_freq = max(sim_df[(sim_df['sim']>sim) & (sim_df['text']==entity)]['cand_freq_x'])\n",
    "    #text_to_compare_freq = sum(sim_df[(sim_df['sim']>sim) & (sim_df['text']==entity)]['cand_freq_y'])\n",
    "    #print(f'{entity} mentions: {text_freq}. After merging: {text_freq+text_to_compare_freq}')\n",
    "    #print(merged)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_freq_ents['cand_freq_x'] = sim_df_freq_ents['freq_x']\n",
    "sim_df_freq_ents['cand_freq_y'] = sim_df_freq_ents['freq_y']\n",
    "sim_df_freq_anns['cand_freq_x'] = sim_df_freq_anns['freq_x']\n",
    "sim_df_freq_anns['cand_freq_y'] = sim_df_freq_anns['freq_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greece mentions: 30547. After merging: 51301\n",
      "['greece &', 'greeces', 'greek islands', 'greece border', 'greek island', 'greek coast', 'the greek islands', 'the greek island', 'greeces border', 'greek islanders', 'greek border', 'greek embassy', 'greeks', 'greek borders', 'greek government', 'greek', 'greek aegean', 'athens', 'the greek embassy', 'greek pm', 'the greek border', 'greece & europe', 'greek forces', 'greece & eu', 'hellenic', 'greek consulates', 'greek city times', 'greek navy', 'thessaloniki', 'greek army', 'the greek borders', 'greek coast guards', 'greek coastguard', 'greek - european', 'greek security forces']\n",
      "greece mentions: 31752. After merging: 33191\n",
      "['greeces', 'greece europe', 'greek islands', 'greece border', 'lesvos greece', 'greek island', 'lesbos greece', 'greek state', 'greek islanders', 'greek border', 'europe greece', 'greek embassy', 'greece news', 'greek aegean islands', 'greek government', 'greece army', 'greek', 'greek aegean', 'greek patriots', 'athens', 'hellenic', 'greek city times', 'hellenic republic', 'greek navy', 'greek island of lesbos', 'thessaloniki', 'greek army', 'greek coastguard', 'turkey greece', 'greece turkey']\n",
      "greece mentions: 28113. After merging: 41286\n",
      "['them  greece', 'greeces', 'r  greece', 'greece cz', 'istandwith greece', 'greece does', 'istandfor greece', 'the greek islands', 'standwith greece', 'ist and with greece', 'the greek island', 'greek islanders', 'the greek isles', 'greeks', 'the behaviours of greece', 'greek authorities', 'greek', 'greek intervention', 'greek sec', 'neighbouring greece', 'greek airspace', 'greecenews', 'greece records', 'athens', 'neighboring greece', 'greekborders', 'the greek embassy', 'greek soilders', 'greek pm', 'the greek pp', 'greek waters', 'greek forces', 'the greek consulate', 'the greekborder', 'greekislands', 'greek counterpart', 'the greek fleet', 'greek navy', 'greek govt', 'greekcitytimes', 'greekreporter', 'the greek consulates', 'greek pm contacts', 'thessaloniki', 'greek villagers', 'greek faschists', 'greek scums', 'the greek gvt', 'lovers of greece', 'greeknews', 'greek coastguard', 'greek government labels', 'greek coastguards', 'greek soldier', 'hellenicpolice']\n"
     ]
    }
   ],
   "source": [
    "entity = 'greece'\n",
    "sim = 0.8\n",
    "\n",
    "print_info(sim_df_freq_ents, entity,sim)\n",
    "print_info(sim_df_freq_anns, entity,sim)\n",
    "print_info(sim_df_freq,entity,sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = 'eu'\n",
    "sim = 0.85\n",
    "\n",
    "merged_entities = set()\n",
    "for merged in print_info(sim_df_freq_ents, entity,sim):\n",
    "    merged_entities.update(print_info(sim_df_freq_ents, merged, sim))\n",
    "    \n",
    "merged_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
