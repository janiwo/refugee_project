---
title: "STM"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import relevant libraries and load the data

```{r }
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(stm)
library(ggplot2)

#file_path = "/Users/Janiwo/Desktop/Clean"
file_path = "C:/Users/jawo19ad/Dropbox (CBS)/Master thesis data/Event Dataframes/Clean"
greece_path = paste(file_path,"/df_greece_fi.csv", sep = "")
channel_path = paste(file_path,"/df_channel_fi.csv", sep = "")
tigray_path = paste(file_path,"/df_tigray_fi.csv", sep = "")
rohingya_path = paste(file_path,"/df_rohingya_fi.csv", sep = "")

# --------------------------------------------------------
# ------------------- Load greece data -------------------
# --------------------------------------------------------
df_greece_raw <- read.csv(file = greece_path,
                          stringsAsFactors = FALSE)

# df_greece_raw <- df_greece_raw %>%
#   mutate(entity = case_when(
#     refugee == "True" & migrant == "False" ~ "Refugee",
#     refugee == "True" & migrant == "True" ~ "Both",
#     refugee == "False" & migrant == "True" ~ "Migrant",
#     refugee == "False" & migrant == "False" ~ "None",
#   ))

df_greece_raw$day <- cumsum(!duplicated(df_greece_raw$date))

df_greece <- df_greece_raw %>% select(text_frame_identification, text_coherent, day)



# --------------------------------------------------------
# ------------------- Load channel data -------------------
# --------------------------------------------------------
df_channel_raw <- read.csv(file = channel_path,
                           stringsAsFactors = FALSE)

# df_channel_raw <- df_channel_raw %>%
#   mutate(entity = case_when(
#     refugee == "True" & migrant == "False" ~ "Refugee",
#     refugee == "True" & migrant == "True" ~ "Both",
#     refugee == "False" & migrant == "True" ~ "Migrant",
#     refugee == "False" & migrant == "False" ~ "None",
#   ))

df_channel_raw$day <- cumsum(!duplicated(df_channel_raw$date))

df_channel <- df_channel_raw %>% select(text_frame_identification, text_coherent, day)



# --------------------------------------------------------
# ------------------- Load tigray data -------------------
# --------------------------------------------------------
df_tigray_raw <- read.csv(file = tigray_path,
                          stringsAsFactors = FALSE)

# df_tigray_raw <- df_tigray_raw %>%
#   mutate(entity = case_when(
#     refugee == "True" & migrant == "False" ~ "Refugee",
#     refugee == "True" & migrant == "True" ~ "Both",
#     refugee == "False" & migrant == "True" ~ "Migrant",
#     refugee == "False" & migrant == "False" ~ "None",
#   ))

df_tigray_raw$day <- cumsum(!duplicated(df_tigray_raw$date))

df_tigray <- df_tigray_raw %>% select(text_frame_identification, text_coherent, day)



# --------------------------------------------------------
# ------------------- Load rohingya data -------------------
# --------------------------------------------------------
df_rohingya_raw <- read.csv(file = rohingya_path,
                          stringsAsFactors = FALSE)

# df_rohingya_raw <- df_rohingya_raw %>%
#   mutate(entity = case_when(
#     refugee == "True" & migrant == "False" ~ "Refugee",
#     refugee == "True" & migrant == "True" ~ "Both",
#     refugee == "False" & migrant == "True" ~ "Migrant",
#     refugee == "False" & migrant == "False" ~ "None",
#   ))

df_rohingya_raw$day <- cumsum(!duplicated(df_rohingya_raw$date))

df_rohingya <- df_rohingya_raw %>% select(text_frame_identification, text_coherent, day)

```

# Process Text (most already done in Python) and prepare the documents

```{r}
# Process Text
stop_uninformative <- strsplit("able absolutely access according account accountable across action actually affected agency agree agreement alive allegation allow allowed allowing almost alone along already also always ambassador amid among another answer anti anymore anyone anything area arent around arrived article asylum attempt attention away back based basic become believe best better black blame blocked born breaking bring brother build call called calling came cant case caught cause centre change claim claiming clear clearly close come coming comment commited commiting completely concern concerned condemn condition confirmed  continue continues could country course cover created credible crisis currently daily day deal dear decade decision demand department despite didnt difference different dire doesnt done dont east eastern easy either effort else endf enough especially ethnic even ever every everyone everything evidence evil exactly expect extremely face facility facing fact failed fake false federal feel find first found four forget free fuck fucking full genuine getting given giving great ground group happy imagine including issue instead isnt give going good half hand happen happened happening hard head heading held high horn hour however huge idea image immediate immediately independent information inside internal internally interview investigate investigation issue join journalist keep kind know known lack land landing largest last latest leader least left let letting level lie like likely little live living load local long longer look looking lost made mail major majority make making many massive matter maybe mean medium member middle might migration mind month morning mostly move much must name national near nearly need needed neighboring never news next nobody north northern nothing obviously office official one ongoing operation others paid part party pas past people perhaps person place plan please point post press prevails prime probably problem process programme provide public push putting question quite rather reach read real realise reality really reason received recent record remains remember report reported reporting resident response responsible rest result right said satellite say saying second seek seeker seeking seem seems seen send sending sent series service several shame share shit show side simple simply since single site situation small someone something soon sorry sort south source speak special spread stand star start started statement street still stop stopped stopping story stupid sure surely system take taken taking talk talking tell term testimony thank thanks thats there theyre thing think though thought three time today told took torn towards town tried true truth trying turn tweet understand unless urgent urgently used using video vice view virus visit voice wait waiting want wanted watch water week welcome well west western withdraw within without whats whilst white whole wing wish wonder wont word world worse wrong would year yesterday young youre"," ")[[1]]

stop_ne <- strsplit("abiy addis aegean afewerki afeworki afghan afghanistan africa african america amnesty ahmed american amhara andrew ankara antony arab asean asia assad aung bachelet balukhali bangladesh bangladeshi biden blinken boris borisjohnson borrell brexit brit britain british brussels bulgaria burma burmese calais canada channel china commission commissioner corona coronavirus council covid cox dover dublin edirne england english erdoan erdogan eritrea eritrean ethiopia ethiopian euro europe european evros farage filippo fontelles france french freya_cole garneau geneva german germany grandi greece greek guterres haavisto hamdayet harris hindu hitsats houthi houthis idlib idp india indian iran iraq isaias isayas isi israel italy jammu jazeera jerry johnson josep junta justin kachin kadra karen kayin kamala kent kenya kurd labour linda lindat_g lebanon lesbos lesvos libya london maikadra manipur marc merkel michelle mizoram moria muslim mutraw myanmar nation nations nationshumanrights nato nazi nigel november oromia oromo pakistan patel president priti putin reuters rohingya rohingyas russia russian samri secretary shimelba shire somalia spain state sudan sudanese syria syrian tegaru tigrai tigrayan tigrayans thai thailand tory tplf trudeau trump turk turkey turkish unhcr unicef union united unsc yemen youtube"," ")[[1]]

stop_model_opt <- strsplit("accept allow blame border center closed create cross desparate eastern enter immigration international fake fascist home hundred illegal illegally island journalist leave legal legally life live love million open opened opened photo political poor propaganda racist safe save stay tear thousand western"," ")[[1]]

stop_cust <- c(stop_uninformative,stop_ne,stop_model_opt)


# --------------------------------------------------------
# ------------------- Process greece data ----------------
# --------------------------------------------------------
?textProcessor

processed_greece = textProcessor(df_greece$text_frame_identification, metadata = df_greece,
                                 lowercase = FALSE,
                                 removestopwords = FALSE,
                                 removenumbers = FALSE,
                                 removepunctuation = FALSE,
                                 ucp = FALSE,
                                 stem = FALSE,
                                 customstopwords = FALSE)

# Prepare Docs
?plotRemoved
rows <- nrow(df_greece)
plotRemoved(processed_greece$documents,
            lower.thresh = seq(as.integer(0.001*rows),
                               as.integer(0.05*rows)))

?prepDocuments
out_greece <- prepDocuments(processed_greece$documents,
                            processed_greece$vocab,
                            processed_greece$meta)


# --------------------------------------------------------
# ------------------- Process channel data ---------------
# --------------------------------------------------------
processed_channel = textProcessor(df_channel$text_frame_identification, metadata = df_channel,
                                  lowercase = FALSE,
                                  removestopwords = FALSE,
                                  removenumbers = FALSE,
                                  removepunctuation = FALSE,
                                  ucp = FALSE,
                                  stem = FALSE,
                                  customstopwords = FALSE)

# Prepare Docs
rows <- nrow(df_channel)
plotRemoved(processed_channel$documents,
            lower.thresh = seq(as.integer(0.001*rows),
                               as.integer(0.05*rows)))

out_channel <- prepDocuments(processed_channel$documents,
                             processed_channel$vocab,
                             processed_channel$meta)


# --------------------------------------------------------
# ------------------- Process tigray data ----------------
# --------------------------------------------------------
?textProcessor
processed_tigray = textProcessor(df_tigray$text_frame_identification, metadata = df_tigray,
                                 lowercase = FALSE,
                                 removestopwords = FALSE,
                                 removenumbers = FALSE,
                                 removepunctuation = FALSE,
                                 ucp = FALSE,
                                 stem = FALSE,
                                 customstopwords = FALSE)

# Prepare Docs
?plotRemoved
rows <- nrow(df_tigray)
plotRemoved(processed_tigray$documents,
            lower.thresh = seq(as.integer(0.001*rows),
                               as.integer(0.05*rows)))

?prepDocuments
out_tigray <- prepDocuments(processed_tigray$documents,
                            processed_tigray$vocab,
                            processed_tigray$meta)


# --------------------------------------------------------
# ------------------- Process rohingya data --------------
# --------------------------------------------------------
?textProcessor
processed_rohingya = textProcessor(df_rohingya$text_frame_identification, metadata = df_rohingya,
                                  lowercase = FALSE,
                                  removestopwords = FALSE,
                                  removenumbers = FALSE,
                                  removepunctuation = FALSE,
                                  ucp = FALSE,
                                  stem = FALSE,
                                  customstopwords = FALSE)

# Prepare Docs
?plotRemoved
rows <- nrow(df_rohingya)
plotRemoved(processed_rohingya$documents,
            lower.thresh = seq(as.integer(0.001*rows),
                               as.integer(0.05*rows)))

?prepDocuments
out_rohingya <- prepDocuments(processed_rohingya$documents,
                              processed_rohingya$vocab,
                              processed_rohingya$meta)




##### ARCHIEVE
# processed_rohingya = textProcessor(df_rohingya$text_stm, metadata = df_rohingya,
#                                   lowercase = FALSE,
#                                   removestopwords = FALSE,
#                                   removenumbers = FALSE,
#                                   removepunctuation = FALSE,
#                                   ucp = FALSE,
#                                   stem = FALSE,
#                                   customstopwords = stop_cust)
# 
# # Prepare Docs
# ?plotRemoved
# rows <- nrow(df_rohingya)
# plotRemoved(processed_rohingya$documents,
#             lower.thresh = seq(as.integer(0.001*rows),
#                                as.integer(0.05*rows)))
# 
# ?prepDocuments
# out_rohingya <- prepDocuments(processed_rohingya$documents,
#                               processed_rohingya$vocab,
#                               processed_rohingya$meta,
#                               lower.thresh = 0.005*rows,
#                               upper.thresh = 0.5*rows)
```

# Find optimal number of topics
```{r}

# --------------------------------------------------------
# ------------------- Find k for greece ------------------
# --------------------------------------------------------

# Find best K (use this code to train stm for different K's)
?searchK
# ngroups argument to speed up computation (at the cost of increased memory)
storage_greece <- searchK(documents = out_greece$documents,
                          vocab = out_greece$vocab,
                          K = c(4,5,6,7,8,9,10,11,12),
                          #K = c(4,6,8,10,12,15,20),
                          #prevalence =~ entity + s(day),
                          data = out_greece$meta,
                          max.em.its = 15,
                          seed = 42)
plot(storage_greece) # -> 6/8 seems to be good
#exclusivity(model_fit, M = 10)


# --------------------------------------------------------
# ------------------- Find k for channel ------------------
# --------------------------------------------------------
# Find best K (use this code to train stm for different K's)
storage_channel <- searchK(documents = out_channel$documents,
                          vocab = out_channel$vocab,
                          K = c(4,5,6,7,8,9,10,11,12),
                          #K = c(4,6,8,10,12,15,20),
                          #prevalence =~ entity + s(day),
                          data = out_channel$meta,
                          max.em.its = 15,
                          seed = 42)
plot(storage_channel) # -> 5 probably
#exclusivity(model_fit, M = 10)


# --------------------------------------------------------
# ------------------- Find k for tigray ------------------
# --------------------------------------------------------
storage_tigray <- searchK(documents = out_tigray$documents,
                          vocab = out_tigray$vocab,
                          K = c(4,5,6,7,8,9,10,11,12),
                          #prevalence =~ entity + s(day),
                          data = out_tigray$meta,
                          max.em.its = 15,
                          seed = 42)
plot(storage_tigray) #6 looks best, possibly 7


# --------------------------------------------------------
# ------------------- Find k for rohingya ----------------
# --------------------------------------------------------
storage_rohingya <- searchK(documents = out_rohingya$documents,
                            vocab = out_rohingya$vocab,
                            K = c(4,5,6,7,8,9,10,11,12),
                            #prevalence =~ entity + s(day),
                            data = out_rohingya$meta,
                            max.em.its = 25,
                            seed = 42)
plot(storage_rohingya) #6 looks best, could also be 7


# exclusivity -> 
# semantic coherence -> co-occurance of words that are most probable under a topic in the same document (should be high)
# held-out likelihood -> hold out fraction of words in set of documents, train the model and use the document-level latent variables to evaulate propability of heldout portion (eval.heldout, should be high)
# resilduals -> should be around 1, if it's above, topic number is probabaly too low


#https://www.r-bloggers.com/2018/09/training-evaluating-and-interpreting-topic-models/
# storage_greece$results %>%
#   select(K, exclusivity, semantic_coherence) %>%
#   filter(K %in% c(4,6,8)) %>%
#   unnest() %>%
#   mutate(K = as.factor(K)) %>%
#   ggplot(aes(semantic_coherence, exclusivity, color = K)) +
#   geom_point(size = 2, alpha = 0.7) +
#   labs(x = "Semantic coherence",
#        y = "Exclusivity",
#        title = "Comparing exclusivity and semantic coherence",
#        subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")



```

# --------------------------------------------------------
# ------------------- GREECE -----------------------------
# --------------------------------------------------------

# Model Creation
```{r}
# --------------------------------------------------------
# ------------------- Build model for greece ------------- 6,7,8
# --------------------------------------------------------
# Build candidate models
?stm

model_greece_4 <- stm(documents = out_greece$documents,
                      vocab = out_greece$vocab,
                      K = 4,
                      #prevalence =~ entity + s(day),
                      data = out_greece$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)

model_greece_5 <- stm(documents = out_greece$documents,
                      vocab = out_greece$vocab,
                      K = 5,
                      #prevalence =~ entity + s(day),
                      data = out_greece$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)

model_greece_6 <- stm(documents = out_greece$documents,
                      vocab = out_greece$vocab,
                      K = 6,
                      #prevalence =~ entity + s(day),
                      data = out_greece$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)

model_greece_7 <- stm(documents = out_greece$documents,
                      vocab = out_greece$vocab,
                      K = 7,
                      #prevalence =~ entity + s(day),
                      data = out_greece$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)

model_greece_8 <- stm(documents = out_greece$documents,
                      vocab = out_greece$vocab,
                      K = 8,
                      #prevalence =~ entity + s(day),
                      data = out_greece$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)

model_greece_40 <- stm(documents = out_greece$documents,
                       vocab = out_greece$vocab,
                       K = 40,
                       prevalence =~ entity + s(day),
                       data = out_greece$meta,
                       init.type = "Spectral",
                       seed = 42)

# model_greece <- stm(documents = out_greece$documents,
#                     vocab = out_greece$vocab,
#                     K = 7,
#                     prevalence =~ entity + s(day),
#                     content =~ entity,
#                     data = out_greece$meta,
#                     init.type = "Spectral",
#                     seed = 42)


1) immigration
2) hosting
3) misc
4) securitisation
5) humanitarian
6) vulnerability
7) invasion




# prep2_greece <- estimateEffect(c(7) ~ entity * s(day), model_greece, metadata = out_greece$meta, uncertainty = "None")
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Refugee", linecol = "red", ylim = c(0,0.25), printlegend = FALSE)
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Migrant", linecol = "blue",add = TRUE, printlegend = FALSE)
# legend(0,0.2, c("Refugee","Migrant"), lwd = 2, col = c("red","blue"))
```

# Model Exploration
```{r}
# Choose the model to investiate
model_greece <- model_greece_7

# Plot the convergence
plot(model_greece$convergence$bound,
     type = "l",
     ylab = "Approximate Convergence",
     main = "Convergence")

# Label Topics
?labelTopics
x = labelTopics(model = model_greece,
            n = 10)

# Plot frequency of topics
plot(model_greece, type="summary", label="frex")

# Plot topic correlations
mod.out.corr <-topicCorr(model_greece)
plot(mod.out.corr)

?toLDAvis
toLDAvis(model_greece, docs = out_greece$documents)

# # Plot the model
# ?plot.STM
# plot.STM(x = model_greece,
#          type = "labels",
#          n = 10)

# # Find thougths
# ?findThoughts # shows documents that are highly associated with a topic
# findThoughts(model = model_greece,
#              texts = out_greece$meta$text_coherent)


# # Plot expected distribution of topic proportions
# plot(model_greece, type="hist")
# 
# # Plot wordcloud of a topic
# cloud(model_greece, topic = 1)
# 
# plot(prep_greece, "date", method="continuous", topics = 5, model = model_greece)



```



# Effect of Covariates
```{r}
## Estimating metadata/topic relationship
?estimateEffect

out_greece$meta$entity <- as.factor(out_greece$meta$entity)
prep_greece <- estimateEffect(1:6 ~ entity + s(day), model_greece, meta = out_greece$meta, uncertainty = "None") # consider using "Global" instead of "None"
summary(prep_greece)
plot(prep_greece, covariate = "entity", topics = c(1,2,3,4,5,6),
     model = model_greece, method = "difference",
     cov.value1 = "Refugee", cov.value2 = "Migrant",
     xlab = "Refugee ... Migrant",
     main = "Effect of tweets containing refugee vs migrants",
     xlim = c(-0.1, 0.1),
     labeltype = "custom",
     custom.labels = c("Topic1","Topic2","Topic3","Topic4","Topic5","Topic6"))

# Compare content covariates
plot(model_greece, type = "perspectives", topics = 7, covarlevels = c("Refugee","Migrant"))

# Compare Frames
plot(model_greece, type = "perspectives", topics = c(1,6))

# Plot topics over time
plot(prep_greece, "day", method = "continuous", topics = c(1,2,3,4,5,6,7), model = model_greece,
     printlegend = FALSE, xaxt ="n", xlab = "Calendar Week")
weekseq <- seq(from = as.Date("2020-02-11"),
               to = as.Date("2020-03-23"),
               by = "week")
weeknames <- strftime(weekseq, format = "%V")
axis(1,at = as.numeric(weekseq) - min(as.numeric(weekseq)), labels = weeknames)
```


# --------------------------------------------------------
# ------------------- CHANNEL -----------------------------
# --------------------------------------------------------

# Model Creation
```{r}
# --------------------------------------------------------
# ------------------- Build model for channel ------------- 5,6,7,8
# --------------------------------------------------------
# Build candidate models
model_channel_5 <- stm(documents = out_channel$documents,
                       vocab = out_channel$vocab,
                       K = 5,
                       #prevalence =~ entity + s(day),
                       data = out_channel$meta,
                       init.type = "Spectral",
                       max.em.its = 15,
                       seed = 42)

model_channel_6 <- stm(documents = out_channel$documents,
                       vocab = out_channel$vocab,
                       K = 6,
                       #prevalence =~ entity + s(day),
                       data = out_channel$meta,
                       init.type = "Spectral",
                       max.em.its = 15,
                       seed = 42)

model_channel_7 <- stm(documents = out_channel$documents,
                       vocab = out_channel$vocab,
                       K = 7,
                       #prevalence =~ entity + s(day),
                       data = out_channel$meta,
                       init.type = "Spectral",
                       max.em.its = 15,
                       seed = 42)

model_channel_8 <- stm(documents = out_channel$documents,
                       vocab = out_channel$vocab,
                       K = 8,
                       #prevalence =~ entity + s(day),
                       data = out_channel$meta,
                       init.type = "Spectral",
                       max.em.its = 15,
                       seed = 42)



```

# Model Exploration
```{r}
# Choose the model to investiate

model_channel <- model_channel_5

# Plot the convergence
plot(model_channel$convergence$bound,
     type = "l",
     ylab = "Approximate Convergence",
     main = "Convergence")

# Label Topics
?labelTopics
x = labelTopics(model = model_channel,
            n = 10)

# Plot frequency of topics
plot(model_channel, type="summary")

# Plot topic correlations
mod.out.corr <-topicCorr(model_channel)
plot(mod.out.corr)

toLDAvis(model_channel, docs = out_channel$documents)

# # Plot the model
# ?plot.STM
# plot.STM(x = model_channel,
#          type = "labels",
#          n = 10)

# # Find thougths
# ?findThoughts # shows documents that are highly associated with a topic
# findThoughts(model = model_channel,
#              texts = out_channel$meta$text_coherent)


# # Plot expected distribution of topic proportions
# plot(model_channel, type="hist")
# 
# # Plot wordcloud of a topic
# cloud(model_channel, topic = 1)
# 
# plot(prep_channel, "date", method="continuous", topics = 5, model = model_channel)



```



# Effect of Covariates
```{r}
## Estimating metadata/topic relationship
?estimateEffect

out_channel$meta$entity <- as.factor(out_channel$meta$entity)
prep_tigray <- estimateEffect(1:6 ~ entity + s(day), model_channel, meta = out_channel$meta, uncertainty = "None") # consider using "Global" instead of "None"
summary(prep_channel)
plot(prep_channel, covariate = "entity", topics = c(1,2,3,4,5,6),
     model = model_channel, method = "difference",
     cov.value1 = "Refugee", cov.value2 = "Migrant",
     xlab = "Refugee ... Migrant",
     main = "Effect of tweets containing refugee vs migrants",
     xlim = c(-0.1, 0.1),
     labeltype = "custom",
     custom.labels = c("Topic1","Topic2","Topic3","Topic4","Topic5","Topic6"))

# Compare content covariates
plot(model_channel, type = "perspectives", topics = 7, covarlevels = c("Refugee","Migrant"))

# Compare Frames
plot(model_channel, type = "perspectives", topics = c(1,6))

# Plot topics over time
plot(prep_channel, "day", method = "continuous", topics = c(1,2,3,4,5,6,7), model = model_channel,
     printlegend = FALSE, xaxt ="n", xlab = "Calendar Week")
weekseq <- seq(from = as.Date("2020-02-11"),
               to = as.Date("2020-03-23"),
               by = "week")
weeknames <- strftime(weekseq, format = "%V")
axis(1,at = as.numeric(weekseq) - min(as.numeric(weekseq)), labels = weeknames)
```





# --------------------------------------------------------
# ------------------- TIGRAY -----------------------------
# --------------------------------------------------------

# Model Creation
```{r}
# --------------------------------------------------------
# ------------------- Build model for tigray -------------
# --------------------------------------------------------
# Build candidate models
?stm
model_tigray_6 <- stm(documents = out_tigray$documents,
                      vocab = out_tigray$vocab,
                      K = 6,
                      #prevalence =~ entity + s(day),
                      data = out_tigray$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)

model_tigray_7 <- stm(documents = out_tigray$documents,
                      vocab = out_tigray$vocab,
                      K = 7,
                      #prevalence =~ entity + s(day),
                      data = out_tigray$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)

model_tigray_8 <- stm(documents = out_tigray$documents,
                      vocab = out_tigray$vocab,
                      K = 8,
                      #prevalence =~ entity + s(day),
                      data = out_tigray$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)



# prep2_greece <- estimateEffect(c(7) ~ entity * s(day), model_greece, metadata = out_greece$meta, uncertainty = "None")
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Refugee", linecol = "red", ylim = c(0,0.25), printlegend = FALSE)
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Migrant", linecol = "blue",add = TRUE, printlegend = FALSE)
# legend(0,0.2, c("Refugee","Migrant"), lwd = 2, col = c("red","blue"))
```

# Model Exploration
```{r}
# Choose the model to investiate

model_tigray <- model_tigray_6

# Plot the convergence
plot(model_tigray$convergence$bound,
     type = "l",
     ylab = "Approximate Convergence",
     main = "Convergence")


# Label Topics
?labelTopics
x = labelTopics(model = model_tigray,
            n = 10)

# Plot frequency of topics
plot(model_tigray, type="summary")

# Plot topic correlations
mod.out.corr <-topicCorr(model_tigray)
plot(mod.out.corr)

?toLDAvis
toLDAvis(model_tigray, docs = out_tigray$documents)

# # Plot the model
# ?plot.STM
# plot.STM(x = model_greece,
#          type = "labels",
#          n = 10)

# # Find thougths
# ?findThoughts # shows documents that are highly associated with a topic
# findThoughts(model = model_greece,
#              texts = out_greece$meta$text_coherent)


# # Plot expected distribution of topic proportions
# plot(model_greece, type="hist")
# 
# # Plot wordcloud of a topic
# cloud(model_greece, topic = 1)
# 
# plot(prep_greece, "date", method="continuous", topics = 5, model = model_greece)



```



# Effect of Covariates
```{r}
## Estimating metadata/topic relationship
?estimateEffect

out_tigray$meta$entity <- as.factor(out_tigray$meta$entity)
prep_tigray <- estimateEffect(1:6 ~ entity + s(day), model_tigray, meta = out_tigray$meta, uncertainty = "None") # consider using "Global" instead of "None"
summary(prep_tigray)
plot(prep_tigray, covariate = "entity", topics = c(1,2,3,4,5,6),
     model = model_tigray, method = "difference",
     cov.value1 = "Refugee", cov.value2 = "Migrant",
     xlab = "Refugee ... Migrant",
     main = "Effect of tweets containing refugee vs migrants",
     xlim = c(-0.1, 0.1),
     labeltype = "custom",
     custom.labels = c("Topic1","Topic2","Topic3","Topic4","Topic5","Topic6"))

# Compare content covariates
plot(model_tigray, type = "perspectives", topics = 7, covarlevels = c("Refugee","Migrant"))

# Compare Frames
plot(model_tigray, type = "perspectives", topics = c(1,6))

# Plot topics over time
plot(prep_tigray, "day", method = "continuous", topics = c(1,2,3,4,5,6,7), model = model_tigray,
     printlegend = FALSE, xaxt ="n", xlab = "Calendar Week")
weekseq <- seq(from = as.Date("2020-02-11"),
               to = as.Date("2020-03-23"),
               by = "week")
weeknames <- strftime(weekseq, format = "%V")
axis(1,at = as.numeric(weekseq) - min(as.numeric(weekseq)), labels = weeknames)
```


# --------------------------------------------------------
# ------------------- Rohingya ---------------------------
# --------------------------------------------------------

# Model Creation
```{r}
# --------------------------------------------------------
# ------------------- Build model for greece -------------
# --------------------------------------------------------
# Build candidate models
model_rohingya_6 <- stm(documents = out_rohingya$documents,
                      vocab = out_rohingya$vocab,
                      K = 6,
                      #prevalence =~ entity + s(day),
                      data = out_rohingya$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)

model_rohingya_7 <- stm(documents = out_rohingya$documents,
                      vocab = out_rohingya$vocab,
                      K = 7,
                      #prevalence =~ entity + s(day),
                      data = out_rohingya$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)

model_rohingya_8 <- stm(documents = out_rohingya$documents,
                      vocab = out_rohingya$vocab,
                      K = 8,
                      #prevalence =~ entity + s(day),
                      data = out_rohingya$meta,
                      init.type = "Spectral",
                      max.em.its = 15,
                      seed = 42)



# prep2_greece <- estimateEffect(c(7) ~ entity * s(day), model_greece, metadata = out_greece$meta, uncertainty = "None")
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Refugee", linecol = "red", ylim = c(0,0.25), printlegend = FALSE)
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Migrant", linecol = "blue",add = TRUE, printlegend = FALSE)
# legend(0,0.2, c("Refugee","Migrant"), lwd = 2, col = c("red","blue"))
```

# Model Exploration
```{r}
# Choose the model to investiate

model_rohingya <- model_rohingya_6

# Plot the convergence
plot(model_rohingya$convergence$bound,
     type = "l",
     ylab = "Approximate Convergence",
     main = "Convergence")


# Label Topics
?labelTopics
x = labelTopics(model = model_rohingya,
            n = 10)

# Plot frequency of topics
plot(model_rohingya, type="summary")

# Plot topic correlations
mod.out.corr <-topicCorr(model_rohingya)
plot(mod.out.corr)

?toLDAvis
toLDAvis(model_rohingya, docs = out_rohingya$documents)

# # Plot the model
# ?plot.STM
# plot.STM(x = model_greece,
#          type = "labels",
#          n = 10)

# # Find thougths
# ?findThoughts # shows documents that are highly associated with a topic
# findThoughts(model = model_greece,
#              texts = out_greece$meta$text_coherent)


# # Plot expected distribution of topic proportions
# plot(model_greece, type="hist")
# 
# # Plot wordcloud of a topic
# cloud(model_greece, topic = 1)
# 
# plot(prep_greece, "date", method="continuous", topics = 5, model = model_greece)



```



# Effect of Covariates
```{r}
## Estimating metadata/topic relationship
?estimateEffect

out_rohingya$meta$entity <- as.factor(out_rohingya$meta$entity)
prep_rohingya <- estimateEffect(1:6 ~ entity + s(day), model_rohingya, meta = out_rohingya$meta, uncertainty = "None") # consider using "Global" instead of "None"
summary(prep_rohingya)
plot(prep_rohingya, covariate = "entity", topics = c(1,2,3,4,5,6),
     model = model_rohingya, method = "difference",
     cov.value1 = "Refugee", cov.value2 = "Migrant",
     xlab = "Refugee ... Migrant",
     main = "Effect of tweets containing refugee vs migrants",
     xlim = c(-0.2, 0.2),
     labeltype = "custom",
     custom.labels = c("Topic1","Topic2","Topic3","Topic4","Topic5","Topic6"))

# Compare content covariates
plot(model_rohingya, type = "perspectives", topics = 7, covarlevels = c("Refugee","Migrant"))

# Compare Frames
plot(model_rohingya, type = "perspectives", topics = c(1,6))

# Plot topics over time
plot(prep_rohingya, "day", method = "continuous", topics = c(1,2,3,4,5,6,7), model = model_rohingya,
     printlegend = FALSE, xaxt ="n", xlab = "Calendar Week")
weekseq <- seq(from = as.Date("2020-02-11"),
               to = as.Date("2020-03-23"),
               by = "week")
weeknames <- strftime(weekseq, format = "%V")
axis(1,at = as.numeric(weekseq) - min(as.numeric(weekseq)), labels = weeknames)
```













