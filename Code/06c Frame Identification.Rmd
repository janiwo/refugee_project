---
title: "STM"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import relevant libraries and load the data

```{r }
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(stm)
library(ggplot2)

#file_path = "/Users/Janiwo/Desktop/Clean"
file_path = "C:/Users/jawo19ad/Dropbox (CBS)/Master thesis data/Event Dataframes/Clean"
greece_path = paste(file_path,"/df_greece_clean.csv", sep = "")
channel_path = paste(file_path,"/df_channel_clean.csv", sep = "")
tigray_path = paste(file_path,"/df_tigray_clean.csv", sep = "")
rohingya_path = paste(file_path,"/df_rohingya_clean.csv", sep = "")

# --------------------------------------------------------
# ------------------- Load greece data -------------------
# --------------------------------------------------------
df_greece_raw <- read.csv(file = greece_path,
                          stringsAsFactors = FALSE)

df_greece_raw <- df_greece_raw %>%
  mutate(entity = case_when(
    refugee == "True" & migrant == "False" ~ "Refugee",
    refugee == "True" & migrant == "True" ~ "Both",
    refugee == "False" & migrant == "True" ~ "Migrant",
    refugee == "False" & migrant == "False" ~ "None",
  ))

df_greece_raw$day <- cumsum(!duplicated(df_greece_raw$date))

df_greece <- df_greece_raw %>% select(text_stm, text_coherent, day, entity)



# --------------------------------------------------------
# ------------------- Load channel data -------------------
# --------------------------------------------------------
df_channel_raw <- read.csv(file = channel_path,
                           stringsAsFactors = FALSE)

df_channel_raw <- df_channel_raw %>%
  mutate(entity = case_when(
    refugee == "True" & migrant == "False" ~ "Refugee",
    refugee == "True" & migrant == "True" ~ "Both",
    refugee == "False" & migrant == "True" ~ "Migrant",
    refugee == "False" & migrant == "False" ~ "None",
  ))

df_channel_raw$day <- cumsum(!duplicated(df_channel_raw$date))

df_channel <- df_channel_raw %>% select(text_stm, text_coherent, day, entity)



# --------------------------------------------------------
# ------------------- Load tigray data -------------------
# --------------------------------------------------------
df_tigray_raw <- read.csv(file = tigray_path,
                          stringsAsFactors = FALSE)

df_tigray_raw <- df_tigray_raw %>%
  mutate(entity = case_when(
    refugee == "True" & migrant == "False" ~ "Refugee",
    refugee == "True" & migrant == "True" ~ "Both",
    refugee == "False" & migrant == "True" ~ "Migrant",
    refugee == "False" & migrant == "False" ~ "None",
  ))

df_tigray_raw$day <- cumsum(!duplicated(df_tigray_raw$date))

df_tigray <- df_tigray_raw %>% select(text_stm, text_coherent, day, entity)



# --------------------------------------------------------
# ------------------- Load rohingya data -------------------
# --------------------------------------------------------
df_rohingya_raw <- read.csv(file = rohingya_path,
                          stringsAsFactors = FALSE)

df_rohingya_raw <- df_rohingya_raw %>%
  mutate(entity = case_when(
    refugee == "True" & migrant == "False" ~ "Refugee",
    refugee == "True" & migrant == "True" ~ "Both",
    refugee == "False" & migrant == "True" ~ "Migrant",
    refugee == "False" & migrant == "False" ~ "None",
  ))

df_rohingya_raw$day <- cumsum(!duplicated(df_rohingya_raw$date))

df_rohingya <- df_rohingya_raw %>% select(text_stm, text_coherent, day, entity)

```

# Process Text (most already done in Python) and prepare the documents

```{r}
# Process Text
stop_uninformative <- strsplit("able absolutely according across actually agree agreement almost along already also always amid among another answer anti anymore anyone anything area arent around asylum attempt away back become believe best better born breaking bring call called calling came cant case caught cause centre change claim claiming clear clearly close come coming comment condition continue continues could country course created crisis currently daily day deal decision didnt difference different doesnt done dont east either else enough especially even ever every everyone everything exactly face facing fact failed feel find first found forget free fuck fucking full genuine getting given giving great including instead isnt give going good half hand happen happening hard head heading held high however huge idea keep know largest last least left let like little live living long longer look looking lost made mail majority make making many matter maybe mean medium member middle might migration month move much must name near need never news next north nothing office one others part party people perhaps place plan please point post probably problem process push putting question quite rather reach read real really reason record remains remember report said say saying seek seeker seeking seem seems seen send sending sent series several shit show side since situation small someone something soon sorry sort south speak stand star start still stop stopped stopping story sure surely system take taken taking talk talking tell thats there theyre thing think though thought time today told took torn towards true trying turn tweet understand unless urgent urgently used using video vice view virus want wanted watch water week well west without whats whilst white whole wonder wont word world worse wrong would year yesterday young youre"," ")[[1]]

stop_ne <- strsplit("abiy aegean afghan afghanistan africa african america ahmed amhara andrew ankara antony arab asean asia assad aung balukhali bangladesh bangladeshi biden blinken boris borisjohnson borrell brexit brit britain british brussels bulgaria burma burmese calais channel china corona coronavirus covid cox dover dublin edirne england english erdoan erdogan eritrea eritrean ethiopia ethiopian euro europe european evros farage filippo fontelles france french freya_cole geneva german germany grandi greece greek guterres harris hindu hitsats idlib india indian iran iraq isaias isi italy jammu jerry johnson josep junta justin kachin karen kayin kamala kent labour lindat_g lebanon lesbos lesvos libya london manipur merkel mizoram moria muslim mutraw myanmar nation nations nato nigel pakistan patel president priti putin rohingya rohingyas russia russian secretary shimelba spain state sudan sudanese syria syrian tigrai tigrayan tigrayans thai thailand tory tplf trudeau trump turk turkey unhcr unicef union united unsc"," ")[[1]]

stop_cust <- c(stop_uninformative,stop_ne)


# --------------------------------------------------------
# ------------------- Process greece data ----------------
# --------------------------------------------------------
?textProcessor

processed_greece = textProcessor(df_greece$text_stm, metadata = df_greece,
                                 lowercase = FALSE,
                                 removestopwords = FALSE,
                                 removenumbers = FALSE,
                                 removepunctuation = FALSE,
                                 ucp = FALSE,
                                 stem = FALSE,
                                 customstopwords = stop_cust)

# Prepare Docs
?plotRemoved
rows <- nrow(df_greece)
plotRemoved(processed_greece$documents,
            lower.thresh = seq(as.integer(0.001*rows),
                               as.integer(0.05*rows)))

?prepDocuments
out_greece <- prepDocuments(processed_greece$documents,
                            processed_greece$vocab,
                            processed_greece$meta,
                            lower.thresh = 0.005*rows,
                            upper.thresh = 0.5*rows)


# --------------------------------------------------------
# ------------------- Process channel data ----------------
# --------------------------------------------------------
processed_channel = textProcessor(df_channel$text_stm, metadata = df_channel,
                                  lowercase = FALSE,
                                  removestopwords = FALSE,
                                  removenumbers = FALSE,
                                  removepunctuation = FALSE,
                                  ucp = FALSE,
                                  stem = FALSE,
                                  customstopwords = stop_cust)

# Prepare Docs
rows <- nrow(df_channel)
plotRemoved(processed_channel$documents,
            lower.thresh = seq(as.integer(0.001*rows),
                               as.integer(0.05*rows)))

out_channel <- prepDocuments(processed_channel$documents,
                             processed_channel$vocab,
                             processed_channel$meta,
                             lower.thresh = 0.005*rows,
                             upper.thresh = 0.5*rows)


# --------------------------------------------------------
# ------------------- Process tigray data ----------------
# --------------------------------------------------------
?textProcessor
processed_tigray = textProcessor(df_tigray$text_stm, metadata = df_tigray,
                                 lowercase = FALSE,
                                 removestopwords = FALSE,
                                 removenumbers = FALSE,
                                 removepunctuation = FALSE,
                                 ucp = FALSE,
                                 stem = FALSE,
                                 customstopwords = stop_cust)

# Prepare Docs
?plotRemoved
rows <- nrow(df_tigray)
plotRemoved(processed_tigray$documents,
            lower.thresh = seq(as.integer(0.001*rows),
                               as.integer(0.05*rows)))

?prepDocuments
out_tigray <- prepDocuments(processed_tigray$documents,
                            processed_tigray$vocab,
                            processed_tigray$meta,
                            lower.thresh = 0.005*rows,
                            upper.thresh = 0.5*rows)


# --------------------------------------------------------
# ------------------- Process rohingya data --------------
# --------------------------------------------------------
?textProcessor
processed_rohingya = textProcessor(df_rohingya$text_stm, metadata = df_rohingya,
                                  lowercase = FALSE,
                                  removestopwords = FALSE,
                                  removenumbers = FALSE,
                                  removepunctuation = FALSE,
                                  ucp = FALSE,
                                  stem = FALSE,
                                  customstopwords = stop_cust)

# Prepare Docs
?plotRemoved
rows <- nrow(df_rohingya)
plotRemoved(processed_rohingya$documents,
            lower.thresh = seq(as.integer(0.001*rows),
                               as.integer(0.05*rows)))

?prepDocuments
out_rohingya <- prepDocuments(processed_rohingya$documents,
                              processed_rohingya$vocab,
                              processed_rohingya$meta,
                              lower.thresh = 0.005*rows,
                              upper.thresh = 0.5*rows)
```

# Find optimal number of topics
```{r}

# --------------------------------------------------------
# ------------------- Find k for greece ------------------
# --------------------------------------------------------

# Find best K (use this code to train stm for different K's)
?searchK
# ngroups argument to speed up computation (at the cost of increased memory)
storage_greece <- searchK(documents = out_greece$documents,
                          vocab = out_greece$vocab,
                          K = c(4,6,8,10,12,15,20,25,30,40,50),
                          prevalence =~ entity + s(day),
                          data = out_greece$meta,
                          max.em.its = 50,
                          seed = 42)
plot(storage_greece) # -> 7/8 seems to be good
#exclusivity(model_fit, M = 10)


# --------------------------------------------------------
# ------------------- Find k for channel ------------------
# --------------------------------------------------------
# Find best K (use this code to train stm for different K's)
storage_channel <- searchK(documents = out_channel$documents,
                          vocab = out_channel$vocab,
                          K = c(4,6,8,10,12,15,20,25,30,40,50),
                          prevalence =~ entity + s(day),
                          data = out_channel$meta,
                          max.em.its = 50,
                          seed = 42)
plot(storage_channel) # -> 7/8 seems to be good
#exclusivity(model_fit, M = 10)


# --------------------------------------------------------
# ------------------- Find k for tigray ------------------
# --------------------------------------------------------
storage_tigray <- searchK(documents = out_tigray$documents,
                          vocab = out_tigray$vocab,
                          K = c(4,6,8,10,12,15,20,25,30,40,50),
                          prevalence =~ entity + s(day),
                          data = out_tigray$meta,
                          max.em.its = 50,
                          seed = 42)
plot(storage_tigray)


# --------------------------------------------------------
# ------------------- Find k for rohingya ----------------
# --------------------------------------------------------
storage_rohingya <- searchK(documents = out_rohingya$documents,
                            vocab = out_rohingya$vocab,
                            K = c(4,6,8,10,12,15,20,25,30,40,50),
                            prevalence =~ entity + s(day),
                            data = out_rohingya$meta,
                            max.em.its = 50,
                            seed = 42)
plot(storage_rohingya)


# exclusivity -> 
# semantic coherence -> co-occurance of words that are most probabale under a topic in the same document (should be high)
# held-out likelihood -> hold out fraction of words in set of documents, train the model and use the document-level latent variables to evaulate propability of heldout portion (eval.heldout, should be high)
# resilduals -> should be around 1, if it's above, topic number is probabaly too low


#https://www.r-bloggers.com/2018/09/training-evaluating-and-interpreting-topic-models/
# storage_greece$results %>%
#   select(K, exclusivity, semantic_coherence) %>%
#   filter(K %in% c(4,6,8)) %>%
#   unnest() %>%
#   mutate(K = as.factor(K)) %>%
#   ggplot(aes(semantic_coherence, exclusivity, color = K)) +
#   geom_point(size = 2, alpha = 0.7) +
#   labs(x = "Semantic coherence",
#        y = "Exclusivity",
#        title = "Comparing exclusivity and semantic coherence",
#        subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")



```

# --------------------------------------------------------
# ------------------- GREECE -----------------------------
# --------------------------------------------------------

# Model Creation
```{r}
# --------------------------------------------------------
# ------------------- Build model for greece -------------
# --------------------------------------------------------
# Build candidate models
?stm

model_greece_5 <- stm(documents = out_greece$documents,
                      vocab = out_greece$vocab,
                      K = 5,
                      prevalence =~ entity + s(day),
                      data = out_greece$meta,
                      init.type = "Spectral",
                      seed = 42)

model_greece_6 <- stm(documents = out_greece$documents,
                      vocab = out_greece$vocab,
                      K = 6,
                      prevalence =~ entity + s(day),
                      data = out_greece$meta,
                      init.type = "Spectral",
                      seed = 42)

model_greece_7 <- stm(documents = out_greece$documents,
                      vocab = out_greece$vocab,
                      K = 7,
                      prevalence =~ entity + s(day),
                      data = out_greece$meta,
                      init.type = "Spectral",
                      seed = 42)

model_greece_8 <- stm(documents = out_greece$documents,
                      vocab = out_greece$vocab,
                      K = 8,
                      prevalence =~ entity + s(day),
                      data = out_greece$meta,
                      init.type = "Spectral",
                      seed = 42)

model_greece_40 <- stm(documents = out_greece$documents,
                       vocab = out_greece$vocab,
                       K = 40,
                       prevalence =~ entity + s(day),
                       data = out_greece$meta,
                       init.type = "Spectral",
                       seed = 42)

# model_greece <- stm(documents = out_greece$documents,
#                     vocab = out_greece$vocab,
#                     K = 7,
#                     prevalence =~ entity + s(day),
#                     content =~ entity,
#                     data = out_greece$meta,
#                     init.type = "Spectral",
#                     seed = 42)


# Choose the model to investiate

model_greece <- model_greece_6

# Plot the convergence
plot(model_greece$convergence$bound,
     type = "l",
     ylab = "Approximate Convergence",
     main = "Convergence")


# prep2_greece <- estimateEffect(c(7) ~ entity * s(day), model_greece, metadata = out_greece$meta, uncertainty = "None")
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Refugee", linecol = "red", ylim = c(0,0.25), printlegend = FALSE)
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Migrant", linecol = "blue",add = TRUE, printlegend = FALSE)
# legend(0,0.2, c("Refugee","Migrant"), lwd = 2, col = c("red","blue"))
```

# Model Exploration
```{r}
# Label Topics
?labelTopics
labelTopics(model = model_greece,
            n = 10)

# Plot frequency of topics
plot(model_greece, type="summary")

# Plot topic correlations
mod.out.corr <-topicCorr(model_greece)
plot(mod.out.corr)

?toLDAvis
toLDAvis(model_greece, docs = out_greece$documents)

# # Plot the model
# ?plot.STM
# plot.STM(x = model_greece,
#          type = "labels",
#          n = 10)

# # Find thougths
# ?findThoughts # shows documents that are highly associated with a topic
# findThoughts(model = model_greece,
#              texts = out_greece$meta$text_coherent)


# # Plot expected distribution of topic proportions
# plot(model_greece, type="hist")
# 
# # Plot wordcloud of a topic
# cloud(model_greece, topic = 1)
# 
# plot(prep_greece, "date", method="continuous", topics = 5, model = model_greece)



```



# Effect of Covariates
```{r}
## Estimating metadata/topic relationship
?estimateEffect

out_greece$meta$entity <- as.factor(out_greece$meta$entity)
prep_greece <- estimateEffect(1:6 ~ entity + s(day), model_greece, meta = out_greece$meta, uncertainty = "None") # consider using "Global" instead of "None"
summary(prep_greece)
plot(prep_greece, covariate = "entity", topics = c(1,2,3,4,5,6),
     model = model_greece, method = "difference",
     cov.value1 = "Refugee", cov.value2 = "Migrant",
     xlab = "Refugee ... Migrant",
     main = "Effect of tweets containing refugee vs migrants",
     xlim = c(-0.1, 0.1),
     labeltype = "custom",
     custom.labels = c("Topic1","Topic2","Topic3","Topic4","Topic5","Topic6"))

# Compare content covariates
plot(model_greece, type = "perspectives", topics = 7, covarlevels = c("Refugee","Migrant"))

# Compare Frames
plot(model_greece, type = "perspectives", topics = c(1,6))

# Plot topics over time
plot(prep_greece, "day", method = "continuous", topics = c(1,2,3,4,5,6,7), model = model_greece,
     printlegend = FALSE, xaxt ="n", xlab = "Calendar Week")
weekseq <- seq(from = as.Date("2020-02-11"),
               to = as.Date("2020-03-23"),
               by = "week")
weeknames <- strftime(weekseq, format = "%V")
axis(1,at = as.numeric(weekseq) - min(as.numeric(weekseq)), labels = weeknames)
```


# --------------------------------------------------------
# ------------------- CHANNEL -----------------------------
# --------------------------------------------------------

# Model Creation
```{r}
# --------------------------------------------------------
# ------------------- Build model for channel -------------
# --------------------------------------------------------
# Build candidate models
model_channel_6 <- stm(documents = out_channel$documents,
                       vocab = out_channel$vocab,
                       K = 6,
                       prevalence =~ entity + s(day),
                       data = out_channel$meta,
                       init.type = "Spectral",
                       seed = 42)

model_channel_7 <- stm(documents = out_channel$documents,
                       vocab = out_channel$vocab,
                       K = 7,
                       prevalence =~ entity + s(day),
                       data = out_channel$meta,
                       init.type = "Spectral",
                       seed = 42)

model_channel_8 <- stm(documents = out_channel$documents,
                       vocab = out_channel$vocab,
                       K = 8,
                       prevalence =~ entity + s(day),
                       data = out_channel$meta,
                       init.type = "Spectral",
                       seed = 42)


# Choose the model to investiate

model_channel <- model_channel_6

# Plot the convergence
plot(model_channel$convergence$bound,
     type = "l",
     ylab = "Approximate Convergence",
     main = "Convergence")
```

# Model Exploration
```{r}
# Label Topics
?labelTopics
labelTopics(model = model_channel,
            n = 10)

# Plot frequency of topics
plot(model_channel, type="summary")

# Plot topic correlations
mod.out.corr <-topicCorr(model_channel)
plot(mod.out.corr)

toLDAvis(model_channel, docs = out_channel$documents)

# # Plot the model
# ?plot.STM
# plot.STM(x = model_channel,
#          type = "labels",
#          n = 10)

# # Find thougths
# ?findThoughts # shows documents that are highly associated with a topic
# findThoughts(model = model_channel,
#              texts = out_channel$meta$text_coherent)


# # Plot expected distribution of topic proportions
# plot(model_channel, type="hist")
# 
# # Plot wordcloud of a topic
# cloud(model_channel, topic = 1)
# 
# plot(prep_channel, "date", method="continuous", topics = 5, model = model_channel)



```



# Effect of Covariates
```{r}
## Estimating metadata/topic relationship
?estimateEffect

out_channel$meta$entity <- as.factor(out_channel$meta$entity)
prep_tigray <- estimateEffect(1:6 ~ entity + s(day), model_channel, meta = out_channel$meta, uncertainty = "None") # consider using "Global" instead of "None"
summary(prep_channel)
plot(prep_channel, covariate = "entity", topics = c(1,2,3,4,5,6),
     model = model_channel, method = "difference",
     cov.value1 = "Refugee", cov.value2 = "Migrant",
     xlab = "Refugee ... Migrant",
     main = "Effect of tweets containing refugee vs migrants",
     xlim = c(-0.1, 0.1),
     labeltype = "custom",
     custom.labels = c("Topic1","Topic2","Topic3","Topic4","Topic5","Topic6"))

# Compare content covariates
plot(model_channel, type = "perspectives", topics = 7, covarlevels = c("Refugee","Migrant"))

# Compare Frames
plot(model_channel, type = "perspectives", topics = c(1,6))

# Plot topics over time
plot(prep_channel, "day", method = "continuous", topics = c(1,2,3,4,5,6,7), model = model_channel,
     printlegend = FALSE, xaxt ="n", xlab = "Calendar Week")
weekseq <- seq(from = as.Date("2020-02-11"),
               to = as.Date("2020-03-23"),
               by = "week")
weeknames <- strftime(weekseq, format = "%V")
axis(1,at = as.numeric(weekseq) - min(as.numeric(weekseq)), labels = weeknames)
```





# --------------------------------------------------------
# ------------------- TIGRAY -----------------------------
# --------------------------------------------------------

# Model Creation
```{r}
# --------------------------------------------------------
# ------------------- Build model for greece -------------
# --------------------------------------------------------
# Build candidate models
?stm
model_tigray_6 <- stm(documents = out_tigray$documents,
                      vocab = out_tigray$vocab,
                      K = 6,
                      prevalence =~ entity + s(day),
                      data = out_tigray$meta,
                      init.type = "Spectral",
                      seed = 42)

model_tigray_7 <- stm(documents = out_tigray$documents,
                      vocab = out_tigray$vocab,
                      K = 7,
                      prevalence =~ entity + s(day),
                      data = out_tigray$meta,
                      init.type = "Spectral",
                      seed = 42)

model_tigray_8 <- stm(documents = out_tigray$documents,
                      vocab = out_tigray$vocab,
                      K = 8,
                      prevalence =~ entity + s(day),
                      data = out_tigray$meta,
                      init.type = "Spectral",
                      seed = 42)


# Choose the model to investiate

model_tigray <- model_tigray_8

# Plot the convergence
plot(model_tigray$convergence$bound,
     type = "l",
     ylab = "Approximate Convergence",
     main = "Convergence")


# prep2_greece <- estimateEffect(c(7) ~ entity * s(day), model_greece, metadata = out_greece$meta, uncertainty = "None")
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Refugee", linecol = "red", ylim = c(0,0.25), printlegend = FALSE)
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Migrant", linecol = "blue",add = TRUE, printlegend = FALSE)
# legend(0,0.2, c("Refugee","Migrant"), lwd = 2, col = c("red","blue"))
```

# Model Exploration
```{r}
# Label Topics
?labelTopics
labelTopics(model = model_tigray,
            n = 10)

# Plot frequency of topics
plot(model_tigray, type="summary")

# Plot topic correlations
mod.out.corr <-topicCorr(model_tigray)
plot(mod.out.corr)

?toLDAvis
toLDAvis(model_tigray, docs = out_tigray$documents)

# # Plot the model
# ?plot.STM
# plot.STM(x = model_greece,
#          type = "labels",
#          n = 10)

# # Find thougths
# ?findThoughts # shows documents that are highly associated with a topic
# findThoughts(model = model_greece,
#              texts = out_greece$meta$text_coherent)


# # Plot expected distribution of topic proportions
# plot(model_greece, type="hist")
# 
# # Plot wordcloud of a topic
# cloud(model_greece, topic = 1)
# 
# plot(prep_greece, "date", method="continuous", topics = 5, model = model_greece)



```



# Effect of Covariates
```{r}
## Estimating metadata/topic relationship
?estimateEffect

out_tigray$meta$entity <- as.factor(out_tigray$meta$entity)
prep_tigray <- estimateEffect(1:6 ~ entity + s(day), model_tigray, meta = out_tigray$meta, uncertainty = "None") # consider using "Global" instead of "None"
summary(prep_tigray)
plot(prep_tigray, covariate = "entity", topics = c(1,2,3,4,5,6),
     model = model_tigray, method = "difference",
     cov.value1 = "Refugee", cov.value2 = "Migrant",
     xlab = "Refugee ... Migrant",
     main = "Effect of tweets containing refugee vs migrants",
     xlim = c(-0.1, 0.1),
     labeltype = "custom",
     custom.labels = c("Topic1","Topic2","Topic3","Topic4","Topic5","Topic6"))

# Compare content covariates
plot(model_tigray, type = "perspectives", topics = 7, covarlevels = c("Refugee","Migrant"))

# Compare Frames
plot(model_tigray, type = "perspectives", topics = c(1,6))

# Plot topics over time
plot(prep_tigray, "day", method = "continuous", topics = c(1,2,3,4,5,6,7), model = model_tigray,
     printlegend = FALSE, xaxt ="n", xlab = "Calendar Week")
weekseq <- seq(from = as.Date("2020-02-11"),
               to = as.Date("2020-03-23"),
               by = "week")
weeknames <- strftime(weekseq, format = "%V")
axis(1,at = as.numeric(weekseq) - min(as.numeric(weekseq)), labels = weeknames)
```


# --------------------------------------------------------
# ------------------- Rohingya ---------------------------
# --------------------------------------------------------

# Model Creation
```{r}
# --------------------------------------------------------
# ------------------- Build model for greece -------------
# --------------------------------------------------------
# Build candidate models
?stm
model_rohingya_6 <- stm(documents = out_rohingya$documents,
                      vocab = out_rohingya$vocab,
                      K = 6,
                      prevalence =~ entity + s(day),
                      data = out_rohingya$meta,
                      init.type = "Spectral",
                      seed = 42)

model_rohingya_8 <- stm(documents = out_rohingya$documents,
                      vocab = out_rohingya$vocab,
                      K = 8,
                      prevalence =~ entity + s(day),
                      data = out_rohingya$meta,
                      init.type = "Spectral",
                      seed = 42)

model_rohingya_9 <- stm(documents = out_rohingya$documents,
                      vocab = out_rohingya$vocab,
                      K = 9,
                      prevalence =~ entity + s(day),
                      data = out_rohingya$meta,
                      init.type = "Spectral",
                      seed = 42)


# Choose the model to investiate

model_rohingya <- model_rohingya_6

# Plot the convergence
plot(model_rohingya$convergence$bound,
     type = "l",
     ylab = "Approximate Convergence",
     main = "Convergence")


# prep2_greece <- estimateEffect(c(7) ~ entity * s(day), model_greece, metadata = out_greece$meta, uncertainty = "None")
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Refugee", linecol = "red", ylim = c(0,0.25), printlegend = FALSE)
# plot(prep2_greece, covariate = "day", model = model_greece, method = "continuous", xlab = "Days", moderator = "entity", moderator.value = "Migrant", linecol = "blue",add = TRUE, printlegend = FALSE)
# legend(0,0.2, c("Refugee","Migrant"), lwd = 2, col = c("red","blue"))
```

# Model Exploration
```{r}
# Label Topics
?labelTopics
labelTopics(model = model_rohingya,
            n = 6)

# Plot frequency of topics
plot(model_rohingya, type="summary")

# Plot topic correlations
mod.out.corr <-topicCorr(model_rohingya)
plot(mod.out.corr)

?toLDAvis
toLDAvis(model_rohingya, docs = out_rohingya$documents)

# # Plot the model
# ?plot.STM
# plot.STM(x = model_greece,
#          type = "labels",
#          n = 10)

# # Find thougths
# ?findThoughts # shows documents that are highly associated with a topic
# findThoughts(model = model_greece,
#              texts = out_greece$meta$text_coherent)


# # Plot expected distribution of topic proportions
# plot(model_greece, type="hist")
# 
# # Plot wordcloud of a topic
# cloud(model_greece, topic = 1)
# 
# plot(prep_greece, "date", method="continuous", topics = 5, model = model_greece)



```



# Effect of Covariates
```{r}
## Estimating metadata/topic relationship
?estimateEffect

out_rohingya$meta$entity <- as.factor(out_rohingya$meta$entity)
prep_rohingya <- estimateEffect(1:6 ~ entity + s(day), model_rohingya, meta = out_rohingya$meta, uncertainty = "None") # consider using "Global" instead of "None"
summary(prep_rohingya)
plot(prep_rohingya, covariate = "entity", topics = c(1,2,3,4,5,6),
     model = model_rohingya, method = "difference",
     cov.value1 = "Refugee", cov.value2 = "Migrant",
     xlab = "Refugee ... Migrant",
     main = "Effect of tweets containing refugee vs migrants",
     xlim = c(-0.2, 0.2),
     labeltype = "custom",
     custom.labels = c("Topic1","Topic2","Topic3","Topic4","Topic5","Topic6"))

# Compare content covariates
plot(model_rohingya, type = "perspectives", topics = 7, covarlevels = c("Refugee","Migrant"))

# Compare Frames
plot(model_rohingya, type = "perspectives", topics = c(1,6))

# Plot topics over time
plot(prep_rohingya, "day", method = "continuous", topics = c(1,2,3,4,5,6,7), model = model_rohingya,
     printlegend = FALSE, xaxt ="n", xlab = "Calendar Week")
weekseq <- seq(from = as.Date("2020-02-11"),
               to = as.Date("2020-03-23"),
               by = "week")
weeknames <- strftime(weekseq, format = "%V")
axis(1,at = as.numeric(weekseq) - min(as.numeric(weekseq)), labels = weeknames)
```













