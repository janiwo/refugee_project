{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4ec2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import candidate_extraction as cand_ex\n",
    "\n",
    "import preprocessor\n",
    "from ekphrasis.classes.tokenizer import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer \n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "#from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "\n",
    "#more advanced tokenizer gives freedom to adjust the way tokens are split\n",
    "social_pipeline = [\"TAG\", \"EMAIL\", \"USER\", \"HASHTAG\", \"CASHTAG\", \"PHONE\", \"PERCENT\", \"NUMBER\",\"WORD\"]\n",
    "tokenizer = Tokenizer(pipeline = social_pipeline, lowercase=False).tokenize\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "#spell_cor = SpellCorrector(corpus=\"english\") # spell correction not used \n",
    "seg_eng = Segmenter(corpus=\"english\") \n",
    "\n",
    "# preprocessor should remove emojis and urls in the tweets\n",
    "preprocessor.set_options(preprocessor.OPT.URL, preprocessor.OPT.EMOJI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a625c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 175900 tweets!\n"
     ]
    }
   ],
   "source": [
    "tigray_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_tigray.csv\" # location of Tigray dataset\n",
    "greece_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_greece.csv\" # location of Greece dataset\n",
    "rohingya_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_rohingya.csv\" # location of Rohingya dataset\n",
    "moria_url = r\"Dropbox (CBS)/Master thesis data/Event Dataframes/df_moria.csv\" # location of Moria dataset (for testing)\n",
    "\n",
    "def read_event_df(data_url):\n",
    "    # easy dataframe load\n",
    "    directory_path = os.getcwd() + \"/../../../\" + data_url \n",
    "    event_df = pd.read_csv(directory_path, index_col=0)\n",
    "    event_df.reset_index(drop=True, inplace=True)\n",
    "    print(f'loaded {event_df.shape[0]} tweets!')\n",
    "    return event_df\n",
    "\n",
    "# pick the df \n",
    "event_df = read_event_df(greece_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c7cd7",
   "metadata": {},
   "source": [
    "## 1. Coherent sentences: Removing non-syntactic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a08a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 175900/175900 [02:03<00:00, 1425.88it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    The goal of the function is to yield coherent sentences from raw tweets (without hashtags, URLs, emojis) \n",
    "    '''\n",
    "\n",
    "    #remove emojis, links \n",
    "    tweet = preprocessor.clean(tweet)\n",
    "    \n",
    "    \n",
    "    # we are using social tokenizer from ekphrasis due to potentially improper text structure\n",
    "    tweet = tokenizer(tweet)\n",
    "    \n",
    "    #removing the irrelevant hashtags and mention using the heuristic that mentions in the beginning of the tweet \n",
    "    # and at least 2 consecutive hashtags at the end of the tweet carry no valuable information\n",
    "    try:\n",
    "        while tweet[0].startswith('@'):\n",
    "            tweet.remove(tweet[0])\n",
    "\n",
    "        if tweet[-1].startswith('@') and tweet[-2].startswith('@'):\n",
    "            while tweet[-1].startswith('@'):\n",
    "                tweet.remove(tweet[-1])\n",
    "\n",
    "        if tweet[-1].startswith('#') and tweet[-2].startswith('#'):\n",
    "            while tweet[-1].startswith('#'):\n",
    "                tweet.remove(tweet[-1])\n",
    "                \n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    #for hashtags that may carry information, we remove the # and split the word into more if applicable\n",
    "    for word in range(len(tweet)):\n",
    "        if tweet[word].startswith('#'):\n",
    "            tweet[word] = tweet[word].replace('#','')\n",
    "            tweet[word] = seg_eng.segment(tweet[word])\n",
    "\n",
    "        # potentially correct spelling - but it is not working very well - corrects numbers to weird words\n",
    "        #tweet[word] = spell_cor.correct(tweet[word])\n",
    "\n",
    "    # instead of .join we use detokenizer in order to reconstruct the cleaned sentence in a better way\n",
    "    #sample_df[twt] =  \" \".join(tweet) \n",
    "    tweet = detokenizer.detokenize(tweet)\n",
    "    \n",
    "    \n",
    "    #  tweets that end up being empty after preprocessing will cause problems when batching, replace empty tweet with 'no_tweet_text' which we can ignore later\n",
    "    tweet = 'no_tweet_text' if len(tweet)==0 else tweet\n",
    "    return tweet\n",
    "\n",
    "\n",
    "tqdm.pandas() # allowing progress bar on .apply method (== .progress_apply)\n",
    "event_df['text_coherent'] = event_df['text'].progress_apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b825f2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading users dataframe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 175900/175900 [00:02<00:00, 87252.14it/s]\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = os.getcwd() + \"/../../../Dropbox (CBS)/Master thesis data\"\n",
    "USERS_PATH = FILE_PATH + \"/df_users.csv\"\n",
    "# Read the users csv\n",
    "\n",
    "print(\"loading users dataframe...\")\n",
    "df_users = pd.read_csv(USERS_PATH)\n",
    "\n",
    "# Drop unnecessary index column\n",
    "df_users.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "df_users.head()\n",
    "\n",
    "# Create dict that maps usernames to actual names\n",
    "mapping = dict(df_users[[\"username\",\"name\"]].values)\n",
    "mapping = {f'@{key}': value for key, value in mapping.items()}\n",
    "\n",
    "\n",
    "def resolve_username_to_name(text):\n",
    "    new_text = text\n",
    "    for word in text.split(\" \"):\n",
    "        if word in mapping:\n",
    "            new_text = new_text.replace(word,mapping[word])\n",
    "    return new_text\n",
    "\n",
    "#tqdm.pandas()\n",
    "event_df['text_coherent'] = event_df['text_coherent'].progress_apply(resolve_username_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe2fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-26 21:17:08 INFO: Writing properties to tmp file: corenlp_server-c3904f7ba6a1489a.props\n",
      "2021-05-26 21:17:08 INFO: Starting server with command: java -Xmx8G -cp C:\\Users\\nikodemicek\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 600000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-c3904f7ba6a1489a.props -annotators tokenize,ssplit,pos,parse,coref -preload -outputFormat serialized\n",
      "  0%|                                                                             | 42/175900 [00:00<07:02, 415.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing coreference chains...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 175900/175900 [02:06<00:00, 1394.58it/s]\n",
      "  0%|                                                                                       | 0/175900 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolving coreference chains...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                        | 398/175900 [08:10<173:13:07,  3.55s/it]"
     ]
    }
   ],
   "source": [
    "# this code runs for around 14h per 100k tweets\n",
    "event_df['event_corefs_resolved'] = cand_ex.replace_corefs(event_df['text_coherent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1225770",
   "metadata": {},
   "source": [
    "## 2. Alphanumeric text: Remove remaining special characters (apart from punctuation) and lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tqdm.pandas()\n",
    "event_df['text_alphanum'] = event_df['text_clean'].progress_apply(lambda tweet:re.sub(r'[^A-Za-z0-9.!? ]+', '', tweet.lower()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45732d",
   "metadata": {},
   "source": [
    "## 3. Keywords text - ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408c3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3738e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f649234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48a8f9e8",
   "metadata": {},
   "source": [
    "## Save the dataframe with clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bdf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df[['id','date','text','text_coherent','text_alphanum']].to_csv('greece_df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32a230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
